#+STARTUP: folded indent inlineimages latexpreview
#+PROPERTY: header-args:R :session *R:lm* :results output :width 560 :height 420 :colnames yes

* ライブラリの読み込み
  
#+begin_src R :results silent
library(DescTools)
library(Metrics)
library(broom)
library(glmmML)
library(glue)
library(tidyverse)
library(kernlab)
#+end_src

* stats::lm()
** Usage

#+begin_src R :results silent
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
#+end_src

** formula class

- モデルの記述方法
  - ~y ~ x1 + x2~ 
  - ~y ~ .~ (. は y 以外の全ての変数)
  - ~y ~ x1:x2~ (x1 と x2 の交互作用項)
  - ~y ~ x1*x2~ (y ~ x1 + x2 + x1:x2 の省略形)
  - ~y ~ (x1+x2)^2~ (上記と同じ)
  - ~y ~ x + 0~  (切片なし, 0 or -1)
  - ~y ~ 1~ (切片のみ)

#+begin_src R
formula <- as.formula(substitute(Sepal.Length ~ Sepal.Width))
lm(formula, iris)
#+end_src

#+RESULTS:
: 
: Call:
: lm(formula = formula, data = iris)
: 
: Coefficients:
: (Intercept)  Sepal.Width  
:      6.5262      -0.2234

** iris

- t value (t 値)
  - 係数=0 という H0 に対する t 検定量

- p.value の意味
  - H0(帰無仮説) は 係数 = 0 (つまり変数の影響がゼロ)
  - p.value は H0 の確率

- Std. Error = 標準誤差(SE)
  - 係数の推定量の標準誤差
  - SE = SD * sqrt(N)

#+begin_src R
fit <- lm(Sepal.Width ~ ., data = iris)
class(fit)
summary(fit)
#+end_src

#+RESULTS:
#+begin_example

[1] "lm"

Call:
lm(formula = Sepal.Width ~ ., data = iris)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.00102 -0.14786  0.00441  0.18544  0.69719 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.65716    0.25595   6.475 1.40e-09 ***
Sepal.Length       0.37777    0.06557   5.761 4.87e-08 ***
Petal.Length      -0.18757    0.08349  -2.246   0.0262 *  
Petal.Width        0.62571    0.12338   5.072 1.20e-06 ***
Speciesversicolor -1.16029    0.19329  -6.003 1.50e-08 ***
Speciesvirginica  -1.39825    0.27715  -5.045 1.34e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2678 on 144 degrees of freedom
Multiple R-squared:  0.6352,	Adjusted R-squared:  0.6225 
F-statistic: 50.14 on 5 and 144 DF,  p-value: < 2.2e-16
#+end_example

** iris (no intercept)

#+begin_src R
fit2 <- lm(Sepal.Length ~ Sepal.Width + 0, data = iris)
summary(fit2)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = Sepal.Length ~ Sepal.Width
0, data = iris)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.5236 -1.0362  0.4823  0.9897  2.8406 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
Sepal.Width  1.86901    0.03265   57.25   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.235 on 149 degrees of freedom
Multiple R-squared:  0.9565,	Adjusted R-squared:  0.9562 
F-statistic:  3277 on 1 and 149 DF,  p-value: < 2.2e-16
#+end_example

* stats::glm() function
** Usage

#+begin_src R
glm(formula,
    family = gaussian,
    data,
    weights,
    subset,
    na.action,
    start = NULL,
    etastart,
    mustart,
    offset,
    control = list(...),
    model = TRUE,
    method = "glm.fit",
    x = FALSE,
    y = TRUE,
    singular.ok = TRUE,
    contrasts = NULL,
    ...)
#+end_src

** 一般化線形モデル

- 構成要素
1. 確率分布   (probability distribution)
2. 線形予測子 (linear predictor)
3. リンク関数 (link function) 線形予測子と目的関数の関係を表すもの

- 対応モデル
  https://www.marketechlabo.com/r-glm-libraries/

- binomial(link = "logit")
# - 目的変数が 2 値変数
# - 対応しているリンク関数は
#   - logit   ロジスティック回帰／ロジットモデル
#   - probit  プロビットモデル
#   - cauchit (= Cauchy)
#   - log
#   - cloglog complementary log-log
#   - リンク関数の違い: http://www.karlin.mff.cuni.cz/~kulich/vyuka/pokreg/R/glm_binary_links.html

## poisson(link = "log")
# - 目的変数が 0 以上の離散変数、分散がそこまで大きくない
# - 対応しているリンク関数は
#   - log       対数線形モデル
#   - identity
#   - sqrt

## gaussian(link = "identity")
# - 目的変数が正規分布に従う
# - 対応しているリンク関数は
#   - identity  線形予測子＝推定値
#   - log       対数正規
#   - inverse

## Gamma(link = "inverse")
# - 目的変数が 0 以上の連続変量
# - 対応しているリンク関数は
#   - inverse
#   - identity
#   - log

## inverse.gaussian(link = "1/mu^2")
# - 対応しているリンク関数は
#   - 1/mu^2
#   - inverse
#   - identity
#   - log

## quasi(link = "identity", variance = "constant")
## quasibinomial(link = "logit")
## quasipoisson(link = "log")

## glmmML pacakge (GLM with Clustering) ----
# Cluster = 場所差・個体差

glmmML(formula, family = binomial, data, cluster, weights,
       cluster.weights, subset, na.action,
       offset, contrasts = NULL, prior = c("gaussian", "logistic", "cauchy"),
       start.coef = NULL, start.sigma = NULL, fix.sigma = FALSE, x = FALSE,
       control = list(epsilon = 1e-08, maxit = 200, trace = FALSE),
       method = c("Laplace", "ghq"), n.points = 8, boot = 0)

** ロジスティック回帰
*** 概要

- 目的変数が (0, 1) データで利用する
- 発生確率(data=1)が知りたいときに利用

- 確率分布に二項分布、リンク関数にロジットリンク関数 (logit link fun)
- 二項分布の発生確率が、説明変数によって変動する統計モデル
  - 他にも、probit link, complementary log-log link fun などが使われる
  - 二項分布の生起確率、0 <= p <= 1 という制約をうまく扱うために、ロジットリンク関数が使われる

*** ロジスティック関数 

$p = logistic(z) = \frac{exp(z)}{1 + exp(z)} = \frac{1}{1+exp(-z)}$
$z = \alpha + \beta x + \dots$

- 二項分布の確率 p は、線形予測子のロジスティック関数ということ
- z = 線形予測子)
- exp(-n) は必ず、0 ~ 1(確率) になる
- ロジット関数は、ロジスティックス関数の逆関数
  - そのため、ロジスティックス関数のことをインバースロジットとも呼ぶ

#+begin_src R :results output graphics file :file (my/get-babel-file)
logistic <- function(z) 1 / (1 + exp(-z))
z <- seq(-6, 6, 0.1)
plot(z, logistic(z), type = "l")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-wnfICl.png]]

*** ロジット関数

- ロジスティック関数を変形. ロジット関数 = ロジスティック関数の逆関数
  - 逆関数 = ある関数に対して「もとにもどす」関数を逆関数と呼ぶ
  - y = f(x)を x について解き、 x = g(y) となったとき, y = g(x) を f(x) の逆関数と呼ぶ
- 対数オッズによる曲線

$logit(p) = log(\frac{p}{1 - p}) = \alpha + \beta x + \dots$

以下でオッズを表現している (オッズ=発生確率/発生しない確率)
$Odds = \frac{p}{1-p}$

#+begin_src R
logit <- function(p) log(p / 1 - p)
p <- seq(0, 1, 0.1)
logit(p)
#+end_src

#+RESULTS:
: 
:  [1] -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf -Inf

*** [[http://www.ner.takushoku-u.ac.jp/masano/class_material/waseda/keiryo/15_logit.html][ロジスティック回帰分析]] の例
**** データ

- 選挙の当落データ
  - wlsmd 当落(1=当選)
  - previous 当選回数
  - expm 選挙費用
#+begin_src R :results value
url <- "http://www.ner.takushoku-u.ac.jp/masano/class_material/waseda/keiryo/logit.csv"
data <- read_csv(url)
head(data)
#+end_src

#+RESULTS:
| wlsmd | previous | expm |
|-------+----------+------|
|     1 |        0 |   10 |
|     1 |        1 |   10 |
|     1 |        3 |  8.9 |
|     1 |        5 |  7.7 |
|     1 |        7 |  5.4 |
|     1 |        4 |    3 |

**** プロット

- 当選回数 (previous) と当落 (wlsmd)の散布図
#+begin_src R :results output graphics file :file (my/get-babel-file)
ggplot(data, aes(previous, wlsmd)) + geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_jitter(width = 0.05, height = 0.05) #jitterで重複したデータを散らす
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-4OPQNF.png]]

**** ロジスティック回帰

#+begin_src R
model_1 <- glm(wlsmd ~ previous + expm, data = data,
               family = binomial(link = "logit"))
summary(model_1)
#+end_src

#+RESULTS:
#+begin_example

Call:
glm(formula = wlsmd ~ previous
expm, family = binomial(link = "logit"), 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5741  -0.3781   0.2013   0.3943   1.4948  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)  -6.3811     3.5147  -1.816   0.0694 .
previous      0.8085     0.5851   1.382   0.1670  
expm          0.8088     0.4000   2.022   0.0431 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 20.728  on 14  degrees of freedom
Residual deviance: 10.384  on 12  degrees of freedom
AIC: 16.384

Number of Fisher Scoring iterations: 6
#+end_example

**** p の予測値

- 各データの当選確率(p)の予測値を出力
#+begin_src R
predict(model_1, type="response")
#+end_src

#+RESULTS:
:           1           2           3           4           5           6 
: 0.846455457 0.925228189 0.962419494 0.979953464 0.974574115 0.327276800 
:           7           8           9          10          11          12 
: 0.327213998 0.925168964 0.009934946 0.051995861 0.710296138 0.088057029 
:          13          14          15 
: 0.522058216 0.022027721 0.327339608

** ポワソン回帰

#? データ(Y)の分布にポワソン分布を想定し、説明変数(x)によって、lambda (平均) が変化するモデル
#? lambda = exp(a + b * x)  or  lambda = exp(a + b * log(x)) (指数なので、各要因の効果が積で表される)
#? => log(lambda) = a + bx  or  log(lambda) = a + b * log(x)
#  左辺 = リンク関数 (たいてい対数リンク関数が使われる, lambda >= 0 のため非負が保証される)
#  右辺 = 線形予測子 (a = 切片, b = 係数, x = 説明変数)

# 緑本のデータ
data3 <- read.csv(glue("{repos}/Workspace/R/data/green_book/data3a.csv",
                       repos = Sys.getenv()["REPOS"]), stringsAsFactors = TRUE)

fit1 <- glm(y ~ x,     family = poisson(link = "log"), data = data3)
fit2 <- glm(y ~ f,     family = poisson, data = data3)
fit3 <- glm(y ~ x + f, family = poisson, data = data3)
fit4 <- glm(y ~ 1,     family = poisson, data = data3) # 切片のみのモデル
summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)

## 結果の見方
# Estimate:   パラメータの最尤推定値
# Std. Error: 標準誤差 (SE) の推定値(= Estimate のばらつきを標準偏差で表したもの, ばらつきに正規分布を仮定)
# z value:    z 値 = Estimate / SE. Wald 統計量と呼ばれる.
# Pr(>|z|):   平均が z 値 の絶対値であり、標準偏差 1 の正規分布における、-Inf ~ 0 までの値を取る確率の 2 倍
#?            この確率が大きいほど、z値がゼロ近くになり、推定値がゼロに近いことを表現 (信頼区間が近似的に算出されたと考える)

## 最大対数尤度
logLik(fit1)
logLik(fit2)
logLik(fit3) # 最も大きい
logLik(fit4) # 最も小さい

## 逸脱度 (Deviance)
D <- -2 * logLik(fit1) # カイ 2 乗分布との対応関係をよくするため、-2 をかける

# 最小逸脱度 (フルモデル = データ数と同じ数のパラメーターを使ったモデル)
loglik <- sum(log(dpois(data3$y, lambda = data3$y))) # データ = lambda なので、尤度は最大になる
min_deviance <- -2 * loglik # 385.7795 <= 最小逸脱度

# 残差逸脱度 (Residual Deviance) = 相対的な当てはまりの悪さ
D - min_deviance

# Null Deviance (= 残差逸脱度の最大値)
# => Null model = 切片だけのモデル
-2 * logLik(fit4) - min_deviance

## AIC
# AIC = -2{(最大対数尤度) - (最尤推定したパラメタ数)} = -2(LogL* - k) = D + 2k
D + 2 * 2 # AIC = 474.77 (k = 2)

* stats::predict()
** 概要

- 第 1 引数に取る Model オブジェクトによって、挙動が異なる

- 信頼区間 confidence
  - 回帰直線の収まる区間。母平均の予測値。
  - 母平均のバラツキはサンプルが増えると小さくなるので、
    データ量が増えると、信頼区間は狭くなる
  - SE をバラツキの指標にした考え方

- 予測区間 prediction
  - 個々のデータ(=標本)の予測。次の観測値が収まる区間。
  - サンプルサイズが増えても区間が狭まるわけではない
  - 残差のバラツキも考慮した区間
  - SD をバラツキの指標にした考え方

- predict.lm
#+begin_src R
predict(object, newdata, se.fit = FALSE, scale = NULL, df = Inf,
        interval = c("none", "confidence", "prediction"),
        level = 0.95, type = c("response", "terms"),
        terms = NULL, na.action = na.pass,
        pred.var = res.var / weights, weights = 1, ...)
#+end_src

#+begin_src R
conf_95 <- predict(lm_fit, new_data, interval="confidence", level=0.95)
pred_95 <- predict(lm_fit, new_data, interval="prediction", level=0.95)
#+end_src

** 予測区間の求め方

- R による予測区間
  https://qiita.com/ksksk/items/75ba95337ccdb32e7cb1

#+begin_src R :results graphics :file (get-babel-file)
data(mtcars)
## 燃費と重さ
fit <- lm(log(mpg) ~ log(wt), data = mtcars)

new.wt <- seq(0.2, 2, 0.1)

predicted <- predict(fit, newdata = list(wt = exp(new.wt)), interval = "predict")
predicted <- cbind(as.data.frame(predicted), wt = new.wt)

ggplot(predicted, aes(x= wt)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "lightblue", alpha = 0.5) +
  geom_line(aes(y = fit), color = "blue", linetype = 2) +
  geom_point(data = mtcars, aes(x = log(wt), y = log(mpg))) +
  xlab(expression(log(wt))) + ylab(expression(log(mpg)))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-HlZRrA.png]]

* ロバスト回帰
** Reference

- R で学ぶロバスト推定
  https://www.slideshare.net/sfchaos/r-7773031

** Overview

- 外れ値に強い回帰分析
- 非説明変数に外れ値を想定 = M 推定
- 説明変数にも外れ値を想定 = MM 推定
- 外れ値に対する重みの付け方 Tukey's biweight / Huber weight

** MASS::rlm()

#+begin_src R

#+end_src

** robustbase::lmrob()

- こちらの方が最新の分析手法を掲載している

* 線形混合モデル (Liner Mixed-Effects)
* サポートベクターマシン (SVM, Support Vector Machine)
** 概要

- [[https://logics-of-blue.com/svm-concept/][サポートベクトルマシンの考え方@Logics of Blue]]

- 分類問題・回帰問題どちらも OK
- サポートベクター = 外れ値を除いた本当に予測に役立つデータ
- *マージン最大化* によって、サポートベクターを計算する
  - 分類の境界線とデータとの距離を最大化する
  - つまり分類の境界に余裕をもたせる、ということ
  - _境界線に最も近いデータをサポートベクトルという_ (=分類する上で重要なデータ)

- ハードマージンとソフトマージン
  - ソフトマージン = 誤判別を許容することで、過学習を抑制
  - 誤判定の許容度 C = ハイパーパラメタ

** データ

#+begin_src R :results value
bird <- data.frame(
  wing = c(12, 10, 13, 10, 13, 12),
  body = c(15, 20, 23, 30, 36, 39),
  type = c("A","A", "A", "B", "B", "B")
)
bird
#+end_src

#+RESULTS:
| wing | body | type |
|------+------+------|
|   12 |   15 | A    |
|   10 |   20 | A    |
|   13 |   23 | A    |
|   10 |   30 | B    |
|   13 |   36 | B    |
|   12 |   39 | B    |

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(wing ~ body, data=bird, type="n", main="鳥の羽と体の大きさ")
text(wing ~ body, data=bird, rownames(bird), col=c(1,2)[bird$type], cex=2)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-rzey1e.png]]

** 当てはめ

- *type* (目的変数が 因子型か否かで、分類・回帰を判別)
  - =C-svc= C classification
  - =nu-svc= nu classification
  - =C-bsvc= bound-constraint svm classification
  - =spoc-svc= Crammer, Singer native multi-class
  - =kbb-svc= Weston, Watkins native multi-class
  - =one-svc= novelty detection
  - =eps-svr= epsilon *regression*
  - =nu-svr= nu *regression*
  - =eps-bsvr= bound-constraint svm *regression*
 
- *kernel*
  - =rbfdot= Radial Basis kernel "Gaussian" (= =parsnip::svm_rbf()=)
  - =polydot= Polynomial kernel (= =parsnip::svm_poly()=)
  - =vanilladot= Linear kernel 線形データに使う
  - =tanhdot= Hyperbolic tangent kernel
  - =laplacedot= Laplacian kernel
  - =besseldot= Bessel kernel
  - =anovadot= ANOVA RBF kernel
  - =splinedot= Spline kernel
  - =stringdot= String kernel

#+begin_src R
## 線形のSV分類
svm_bird <- ksvm(type ~ wing + body, data=bird, type="C-svc", kernel="vanilladot")
svm_bird
#+end_src

#+RESULTS:
#+begin_example

 Setting default kernel parameters

Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 
 parameter : cost C = 1 

Linear (vanilla) kernel function. 

Number of Support Vectors : 4 

Objective Function Value : -1.5318 
Training error : 0
#+end_example

* 参考

- Blog
  - [[https://logics-of-blue.com/svm-concept/][サポートベクトルマシンの考え方@Logics of Blue]]
