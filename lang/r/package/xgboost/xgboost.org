#+STARTUP: folded indent inlineimages latexpreview
#+PROPERTY: header-args:R :results output :session *R:xgb* :width 640 :height 480 :colnames yes

* ={xgboost}=
* [[file:../../stats/tree_models.org][List of Tree Models]]
* ライブラリの読み込み

#+begin_src R :results silent
library(tidyverse)
library(Matrix) # for sparse matrix
library(data.table)
library(xgboost)
#+end_src

* Reference

- =parsnip::xgb_train()= は =xgboost::xgb.train()= のラッパー
- =...= で引数を渡すことができる
- [[https://xgboost.readthedocs.io/en/latest/parameter.html][XGBoost Parameters (Complete list)]]

#+begin_src R :results silent
xgboost::xgb.train(
  params = list(), # booster 毎のパラメタ 
  data,            # 訓練データ。Dmatrix 形式。モデル式もここで指定。
  nrounds,
  watchlist = list(),
  obj = NULL,
  feval = NULL,
  verbose = 1,
  print_every_n = 1L,
  early_stopping_rounds = NULL, # 指定したラウンド数で改善が見られなければ学習を停止
  maximize = NULL,
  save_period = NULL,
  save_name = "xgboost.model",
  xgb_model = NULL,
  callbacks = list(),
  ...)

xgb.cv(
  params = list(),
  data,
  nrounds,
  nfold, # Fold 数を指定
  label = NULL,
  missing = NA,
  prediction = FALSE,
  showsd = TRUE,
  metrics = list(),
  obj = NULL,
  feval = NULL,
  stratified = TRUE,
  folds = NULL,
  verbose = TRUE,
  print_every_n = 1L,
  early_stopping_rounds = NULL,
  maximize = NULL,
  callbacks = list(),
  ...)

## Dmatrix 作成関数
## dgCMatrix は {Matrix} パッケージで作成できる
xgboost::xgb.DMatrix(
  data, # matrix or dgCMatrix
  info = list(),
  missing = NA,
  silent = FALSE,
  ...   # info 引数から list で渡さない場合は、直接ここで指定
        # 通常 label=train$label で目的変数を指定する
)

## 変数重要度 (Importance の data.table を返す)
xgb.importance(
  feature_names = NULL, # 特徴量の名前をベクトルで指定. NULL の場合モデルから抽出
  model = NULL, # xgb.Booster オブジェクト
  trees = NULL,
  data = NULL, # deprecated
  label = NULL, # deprecated
  target = NULL) # deprecated

xgb.ggplot.importance(
  importance_matrix = NULL, # xgb.importance() の戻り値
  top_n = NULL,
  measure = NULL, # "Gain" for trees, "Weight" for linear
  rel_to_first = FALSE,
  n_clusters = c(1:10),
  ...)
#+end_src

* [[https://www.marketechlabo.com/r-xgboost-tuning/][Rを使ったXGBoostの高度なパラメータチューニングと細かいノウハウ]] の例
** データ

- House Sales in King County, USA データ・セット
- ={xgboost}= で扱えるように =xgb.DMatrix= クラスに変換する
#+begin_src R
data.dt <- fread("kc_house_data.csv")
data.dt[,id:=NULL]
data.dt[,date:=as.Date(date, format='%Y%m%d')]
data.dt[,zipcode:=as.factor(zipcode)]
data.dt[,lat:=NULL]
data.dt[,long:=NULL]

nr <- nrow(data.dt)
train <- sample(nr, nr*0.8)
train.dt <- data.dt[train] # 学習データ (80%)
test.dt <- data.dt[-train] # 検証データ (20%)

## 欠損値がある場合
previous_na_action <- options()$na.action
options(na.action='na.pass')

## 学習データ
train_dmat <- xgb.DMatrix(
  ## {Matrix} で疎行列を作成
  sparse.model.matrix(price ~ ., data = train.dt),
  ## 正解データ(目的変数)を指定
  label = train.dt[,price]
)

## 検証データ
test_dmat <- xgb.DMatrix(
  sparse.model.matrix(price ~ ., data = test.dt),
  label = test.dt[,price]
)
## NA に対する扱いを元に戻しておく
options(na.action=previous_na_action)

train_dmat
#+end_src

#+RESULTS:
: 
: xgb.DMatrix  dim: 17290 x 86  info: label  colnames: yes

** パラメタ

#+begin_src R
l_params = list(
  booster = 'gbtree',
  objective = 'reg:linear',
  eval_metric = 'mae',
  eta = 0.1,
  max_depth = 3,
  min_child_weight = 2,
  colsample_bytree = 0.8
)
l_params
#+end_src

#+RESULTS:
#+begin_example

$booster
[1] "gbtree"

$objective
[1] "reg:linear"

$eval_metric
[1] "mae"

$eta
[1] 0.1

$max_depth
[1] 3

$min_child_weight
[1] 2

$colsample_bytree
[1] 0.8
#+end_example

** 学習 1 (最も基本的な形) by =xgb.train()=

#+begin_src R
xgb_model <- xgb.train(
  data = train_dmat,
  nrounds = 1000,
  params = l_params)
xgb_model
#+end_src

#+RESULTS:
#+begin_example

##### xgb.Booster
raw: 664.3 Kb 
call:
  xgb.train(params = l_params, data = train_dmat, nrounds = 1000)
params (as set within xgb.train):
  booster = "gbtree", objective = "reg:linear", eval_metric = "mae", eta = "0.1", max_depth = "3", min_child_weight = "2", colsample_bytree = "0.8", silent = "1"
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
# of features: 86 
niter: 1000
nfeatures : 86
#+end_example

- 変数重要度
#+begin_src R :results output graphics file :file (my/get-babel-file)
importance <- xgboost::xgb.importance(NULL, xgb_model)
xgboost::xgb.ggplot.importance(importance, top_n = 10)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-EJp4py.png]]

** 学習 2 (CV + Early Stopping で最適なブースティング数を決める) by =xgb.cv()=

#+begin_src R
xgb_cv <- xgb.cv(
  data = train_dmat,
  ## 最大の繰り返し回数を指定。十分大きな値を指定する
  nrounds = 50000,
  ## クロスバリデーションの分割数を指定
  nfold = 5,
  params = l_params,
  ## ある回数を基準としてそこから 100 回以内に評価関数の値が改善しなければ計算をストップ
  early_stopping_rounds = 100,
  verbose=0
)
xgb_cv$best_iteration
#+end_src

#+RESULTS:
: 
: [1] 2343

** 学習 3 (Watchlist + Early Stopping で最適なブースティング数を決める) by =xgb.train()=

- Watchlist にテストデータを指定し、それに対する評価関数の値で自動アーリーストップする
#+begin_src R
xgb_model <- xgb.train(
  data = train_dmat,
  nrounds = 50000,
  params = l_params,
  ## このデータに対する評価関数の値をモニタリングする
  watchlist = list(train = train_dmat, eval = test_dmat),
  early_stopping_rounds = 100,
  verbose = 0
)
xgb_model$best_iteration
#+end_src

#+RESULTS:
: 
: [1] 1840

* 参考

- [[https://cran.r-project.org/web/packages/xgboost/index.html][CRAN]]
- [[https://github.com/dmlc/xgboost][Github repo]]
- [[https://cran.r-project.org/web/packages/xgboost/xgboost.pdf][Reference Manual]]
- Vignette
  - [[https://cran.r-project.org/web/packages/xgboost/vignettes/xgboostPresentation.html][Xgboost presentation]]
  - [[https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html][Understand your dataset with Xgboost]]
  - [[https://cran.r-project.org/web/packages/xgboost/vignettes/xgboostfromJSON.html][XGBoost from JSON]]
  - [[https://cran.r-project.org/web/packages/xgboost/vignettes/xgboost.pdf][xgboost: eXtreme Gradient Boosting (PDF)]]

- Blog
  - [[https://www.marketechlabo.com/r-xgboost-tuning/][Rを使ったXGBoostの高度なパラメータチューニングと細かいノウハウ]]

- Parameters
  - [[https://sites.google.com/view/lauraepp/parameters][Laurae++: xgboost / LightGBM]]
  - [[https://xgboost.readthedocs.io/en/latest/parameter.html][XGBoost Parameters (Complete list)]]
    
