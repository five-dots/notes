#+STARTUP: folded indent inlineimages latexpreview
#+PROPERTY: header-args:R :results output :session *R:tree* :width 640 :height 480 :colnames yes

* ライブラリの読み込み

#+begin_src R :results silent
library(tidyverse)
library(tidymodels)

library(rpart) # Decision Tree
library(ranger) # Random Forest
library(kernlab)
library(RGF)
library(extraTrees)

library(partykit) # Plot Decision Tree
library(pdp) # モデル解釈 (Partial Dependence Plot)
library(vip) # モデル解釈 (Variable Importance Plot)
#+end_src

* 決定木 (けっていぎ, Decision Tree)
** 概要

- 木構造のアルゴリズム
- 変数を木に見立て、木の分岐で変数毎の閾値を設定し分類していく

- 分類問題・回帰問題の両対応
- アルゴリズム
  - CART ={rpart}= (Classification and Regression Tree)
  - CHAID
  - ID3 (Iterative Dichotomiser 3)
  - C4.5 / C5.0

- 過学習対策
  - 最小データ数の設定を大きくする
  - 木の深さを浅くする等

** ={rpart}= package

- =method= （通常は目的変数の型によって自動で最適なものが選択される）
  - "class"で分類木（目的変数が factor 型）
  - "poisson"で生起（目的変数が 2 カラムの生起データ）
  - "exp" で生存   （目的変数が survical オブジェクト）
  - "anova"で回帰木（目的変数が上記のいずれでもない）

- =parms=:
  - method = "class" の場合、以下の指標に基づいて分割
    - =parms = list(split = "gini")= でジニ係数を使う（デフォルト）
    - =parms = list(split = "information")= でエントロピーを使う
  - method = 'anova'の場合は指定しない

- =control=:
  - *cp*             = 0.01 (小さいほど細かく分岐する, Cost/Complexity) (=dials::cost_complexity()=)
  - *maxdepth*       = 30 (木の深さ) (=dials::tree_depth()=)
  - *minsplit*       = 20 (1 ノードのサイズの下限) (=dials::min_n()=)
  - minbucket      = 7
  - maxcompete     = 4
  - maxsurrogate   = 5
  - usesurrogate   = 2
  - surrogatestyle = 0
  - xval           = 10

#+begin_src R :results silent
rpart(
  formula,
  data,                 # data.frame
  weights,              # case weights (行単位の重み付け)
  subset,               # 行のサンプリング
  na.action = na.rpart, # Default: y が欠損は削除. xが欠損は保持する.
  method,               # 上述
  model = FALSE,        # model.frame のコピーを保持するか
  x = FALSE,            # 特徴量のコピーを保持するか
  y = TRUE,             # ラベルのコピーを保持するか
  parms,                # Splitting 関数への追加のパラメタ
                        # - "anova" 追加なし
                        # - "class" prior, loss, split
  control,              # rpart.control() の内容 (list), ... で渡しても良い
  cost,                 # 特徴量毎の非負の値。デフォルトですべて1。
  ...)
#+end_src

#+begin_src R
rpart.control()
#+end_src

#+RESULTS:
#+begin_example
$minsplit
[1] 20

$minbucket
[1] 7

$cp
[1] 0.01

$maxcompete
[1] 4

$maxsurrogate
[1] 5

$usesurrogate
[1] 2

$surrogatestyle
[1] 0

$maxdepth
[1] 30

$xval
[1] 10
#+end_example

** kyphosis (脊柱後弯症) データの例
*** データ

- [[https://toukeier.hatenablog.com/entry/2018/09/03/080713][統計ソフトRで決定木分析を行うには？@統計ER]]

_Kyphosis データ_
- Kyphosis 変形が見られるか
- Age      年齢(月)
- Number
- Start

#+begin_src R :results value
data(kyphosis)
head(kyphosis)
#+end_src

#+RESULTS:
| Kyphosis | Age | Number | Start |
|----------+-----+--------+-------|
| absent   |  71 |      3 |     5 |
| absent   | 158 |      3 |    14 |
| present  | 128 |      4 |     5 |
| absent   |   2 |      5 |     1 |
| absent   |   1 |      4 |    15 |
| absent   |   1 |      2 |    16 |

*** 当てはめ

#+begin_src R
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
fit
#+end_src

#+RESULTS:
#+begin_example

n= 81 

node), split, n, loss, yval, (yprob)
      ,* denotes terminal node

 1) root 81 17 absent (0.79012346 0.20987654)  
   2) Start>=8.5 62  6 absent (0.90322581 0.09677419)  
     4) Start>=14.5 29  0 absent (1.00000000 0.00000000) *
     5) Start< 14.5 33  6 absent (0.81818182 0.18181818)  
      10) Age< 55 12  0 absent (1.00000000 0.00000000) *
      11) Age>=55 21  6 absent (0.71428571 0.28571429)  
        22) Age>=111 14  2 absent (0.85714286 0.14285714) *
        23) Age< 111 7  3 present (0.42857143 0.57142857) *
   3) Start< 8.5 19  8 present (0.42105263 0.57894737) *
#+end_example

*** プロット by ={partykit}=

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(as.party(fit))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-iruXdW.png]]

*** CP を利用した剪定

- CP (Complex Parameter) を確認して、剪定 (Pruning) を行うかを検討する
- エラーが収束した分岐を CP として採用する
- CP=0.019608 を採用してみる

#+begin_src R
printcp(fit)
#+end_src

#+RESULTS:
#+begin_example

Classification tree:
rpart(formula = Kyphosis ~ Age + Number + Start, data = kyphosis)

Variables actually used in tree construction:
[1] Age   Start

Root node error: 17/81 = 0.20988

n= 81 

        CP nsplit rel error xerror    xstd
1 0.176471      0   1.00000      1 0.21559
2 0.019608      1   0.82353      1 0.21559
3 0.010000      4   0.76471      1 0.21559
#+end_example

- よりシンプルな木構造になる
- Start が 8.5 かどうかだけでかなり分類できるようになる
#+begin_src R :results output graphics file :file (my/get-babel-file)
fit2 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis, cp=0.019608)
plot(as.party(fit2))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Z7ugog.png]]

* ランダムフォレスト
** 概要

*アルゴリズム*
1. 訓練データからブートストラップ法で多数のデータを生成
   - ブートストラップ法
   - リサンプリング (再標本化) の手法. 復元抽出。
   - データ (サンプル) から重複ありでサブサンプルを取り出すことを繰り返す
2. _複数の決定木を並列で構築 (= Bootstrap Aggregating (Bagging) と呼ばれる手法)_
   - バギング + アンサンブル
3. テストデータに対して、決定木で予測
   - 回帰 = 二乗誤差、分類 = ジニ不純度を減少させるように分岐を作る
   - _ジニ不純度 ≠ logloss の最小化ではない_ (歪んでいる)
4. 複数の決定木の多数決で予測を作成

*パラメタ*
- 決定木の本数 (={parsnip}= = =trees=, ={ranger}= = =num.trees=)
  - [500 ~ 2000] ?
  - GBDT とは違い本数が増えすぎて精度が悪くなることはない (時間とのトレードオフ)
- 特徴量のサンプリング (={parsnip}= = =mtry=, ={ranger}= = =mtry=)
  - デフォルト sqrt(features) ={ranger}=
  - 回帰では、features/3 に設定されているパッケージもある
  - [1 ~ num features]
- 末端の葉のノード数 (={parsnip}= = =min_n=, ={ranger}= = =min.node.size=)
  - d=1or2 [1or2 ~ 40]
  - 分類=1, 回帰=5, Survival=3, Probability=10 がデフォルト (ranger)
  - 大きくすれば過学習を抑制
- 木の深さ (=max_depth=)
  - 末端のノード数で代替できるので ={ranger}= などでは存在しないパラメタ
  - [[https://stackoverflow.com/questions/44291685/what-is-equivalent-of-max-depth-in-the-r-package-ranger][What is equivalent of “max depth” in the 'R' package “ranger”?@Stackoverflow]]
- =replace= & =sample.fraction= 

*特徴*
- OOB (Out-of-Bag: リサンプリングで選ばれなかったデータ) を使って評価することで CV を代用できる
- 説明力の低いノイズ変数が多いと精度が下がってしまう

*パッケージ*
- [[https://koalaverse.github.io/machine-learning-in-R/random-forest.html#random-forest-software-in-r][Random Forest Software in R]]
- ={randomForest}= Fortran
- ={ranger}= C++, 高速
- ={h2o}= Java, Early stopping
- ={Rborist}=
- ={party}= =cforest()= Conditional Random Forest

** ={ranger}=
*** データ

- spam データ
- type = spam or nonspam
- 57 の変数
#+begin_src R
data(spam)
dim(spam)
#+end_src

#+RESULTS:
: 
: [1] 4601   58

*** =ranger::ranger()= 関数

#+begin_src R :results silent
ranger(
  formula = NULL,
  data = NULL,     # data.frame, matrix, dgCMatrix or gwaa.data
  num.trees = 500, # 決定木の数. 正の数. c(500L, 1000L, 2000L) など.
  mtry = NULL,     # サンプリングする特徴量数 (default = sqrt(num of vars))
  importance = "none", # "none", "impurity", "impurity_corrected", "permutation"
  write.forest = TRUE, # 各 Bagging を保存するか. 保存しないと predict ができない
  probability = FALSE,
  min.node.size = NULL, # 末端の枝が最低限もつサンプル数.
                        # Default (分類=1, 回帰=5, Survival=3, Probability=10)
                        # 0以上の数 (0 含む)
  replace = TRUE, # Sample with replacement
  sample.fraction = ifelse(replace, 1, 0.632),
  case.weights = NULL,
  class.weights = NULL,
  splitrule = NULL, # class = gini/extratrees
                    # regress = variance/extratrees/maxstat(=maximally selected rank statistics)
                    # survival = logrank/extratrees/C/maxstat
  num.random.splits = 1, # 1以上の数
  alpha = 0.5, # 0 以上~ 1 以下 の数
  minprop = 0.1, # 0 以上 ~ 0.5 以下の数
  split.select.weights = NULL,
  always.split.variables = NULL,
  respect.unordered.factors = NULL,
  scale.permutation.importance = FALSE,
  keep.inbag = FALSE,
  holdout = FALSE,
  quantreg = FALSE,
  num.threads = NULL,
  save.memory = FALSE,
  verbose = TRUE,
  seed = NULL,
  dependent.variable.name = NULL,
  status.variable.name = NULL,
  classification = NULL
)
#+end_src

*** 当てはめ

#+begin_src R
fit <- ranger(type ~ ., data = spam, num.trees = 1000, seed = 123)
fit
#+end_src

#+RESULTS:
#+begin_example

Ranger result

Call:
 ranger(type ~ ., data = spam, num.trees = 1000, seed = 123) 

Type:                             Classification 
Number of trees:                  1000 
Sample size:                      4601 
Number of independent variables:  57 
Mtry:                             7 
Target node size:                 1 
Variable importance mode:         none 
Splitrule:                        gini 
OOB prediction error:             4.54 %
#+end_example

** ={party}=, =cforest()=

- [[https://cran.r-project.org/web/packages/party/index.html][CRAN - Package party]]
- [[http://party.r-forge.r-project.org/][party]]

* ERT (Extremely Randomize Trees)
** 概要

- ランダムフォレストと似たモデル
- ただし Bagging (リサンプリング) はしない?
- ランダムフォレストよりも過学習を押さえたモデル
  - 特に学習データが少ないときに過学習を起こしにくい
- *分岐の閾値をランダムに決める*
  - 決定木間の相関が低くなる
  - そのため RF よりも若干精度が下がることがあるが、多様性には寄与する
- パッケージ
  - ={extraTrees}=
    - ={rJava}= に依存
  - ={ranger}= (=splitrule = "extratrees"=)
  - ={h2o}= =h2o.randomForest(histogram_type = "Random")=

*ハイパーパラメタ*
- 
|            | ={extraTrees}=  | ={ranger}=         | default                      | range                |
|------------+---------------+------------------+------------------------------+----------------------|
| 木の数     | =ntree=         | =num.trees=        | 500                          | [500L, 1000L, 2000L] |
| 特徴量の数 | =mtry=          | =mtry=             | 回帰=ncol/3, 分類=sqrt(ncol) | []                   |
| 葉の数     | =nodesize=      | =min.node.size=    | 回帰=5, 分類=1               | []                   |
| 分岐の数   | =numRandomCuts= | =num.randm.splits= | 1                            |                      |

** ={extraTrees}=

#+begin_src R :results silent
extraTrees(
  x, # matrix
  y, # regression=num, classification=factor
  ntree = 500,
  ## 回帰 ncol/3, 分類 sqrt(ncol)
  mtry = if (!is.null(y) && !is.factor(y))
         max(floor(ncol(x)/3), 1) else floor(sqrt(ncol(x))),
  ## 木1つ辺りの葉の数  (回帰5,分類1 = ranger と同じ)
  nodesize = if (!is.null(y) && !is.factor(y)) 5 else 1,
  numRandomCuts = 1,
  evenCuts = FALSE,
  numThreads = 1, # CPU スレッド数
  quantile = F,
  weights = NULL,
  subsetSizes = NULL,
  subsetGroups = NULL,
  tasks = NULL, # マルチタスクのときに指定。1から始まる整数 ラベル。
  probOfTaskCuts = mtry / ncol(x),
  numRandomTaskCuts = 1,
  na.action = "stop",
  ...
)

predict(
  object,
  newdata,
  quantile=NULL,
  allValues=F,
  probability=F,
  newtasks=NULL,
  ...
)
#+end_src

** [[https://daviddalpiaz.github.io/stat432sp18/lab/enslab/enslab.html][Extremely Randomized Trees, Ranger, XGBoost]] の例
*** データ

#+begin_src R :results silent
library(MASS)
library(randomForest)
library(caret)

## {extraTrees} は Java を利用しているが、利用のためにメモリ拡張が必要
options(java.parameters = "-Xmx4g")
library(extraTrees)

## 評価関数 RMSE
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

## Boston データを使う
set.seed(42)
boston_idx = sample(1:nrow(Boston), nrow(Boston) / 2)
boston_trn = Boston[boston_idx,]
boston_tst = Boston[-boston_idx,]
#+end_src

*** Random Forest (比較用)

#+begin_src R
cv_5 = trainControl(method = "cv", number = 5)
rf_grid =  expand.grid(mtry = 1:13)

set.seed(42)
## {caret} を使う
rf_fit = train(medv ~ ., data = boston_trn,
               method = "rf",
               trControl = cv_5,
               tuneGrid = rf_grid)

rf_fit$bestTune
rmse(predict(rf_fit, boston_tst), boston_tst$medv)
#+end_src

#+RESULTS:
: 
:   mtry
: 5    5
: null device 
:           1
: 
:   mtry
: 5    5

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(rf_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-qDQVIF.png]]

*** ERT by ={caret}= + ={extraTrees}=

#+begin_src R
et_grid =  expand.grid(mtry = 4:7, numRandomCuts = 1:10)
set.seed(42)
et_fit = train(medv ~ ., data = boston_trn,
               method = "extraTrees",
               trControl = cv_5,
               tuneGrid = et_grid,
               numThreads = 4)
et_fit$bestTune
rmse(predict(et_fit, boston_tst), boston_tst$medv)
#+end_src

#+RESULTS:
: 
:   mtry numRandomCuts
: 9    4             9
: 
: [1] 2.931548

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(et_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-SabDsQ.png]]

* RGF (Regularized Greedy Forest)
** 概要

- Boosting の手法の一つ
- 各イテレーションの終わりに追加された決定木の重みのみを調整するのではない
- *今まで構築されていたすべての決定木の重みを動かして、学習用データに対し fitting を行う*
  - これを Greedy(貪欲)な方法と表現
- Greedy な fitting は容易に過学習が起きてしまうので、正則化(Regularize)を行う
  - RGF では L2 正則化を利用

- ={RGF}= は ={reticulate}= を使った Python パッケージのラッパー
  
** [[file:~/Dropbox/repos/github/five-dots/notes/lang/r/package/RGF.org][ ={RGF}= ]]
* GBDT (Gradient Boosting, 勾配ブースティング木 )
** 概要

- *アルゴリズム*
  - 大量の決定木を直列的につなぎ、結果を集計して予測
  - _Gradient Boosting(重み付きアンサンブル学習) + ランダムフォレスト_
  - 決定木を逐次的に増やし、生成済みの決定木が誤ったケースを更新し、新たな決定木を生成
    - 以前の木の予測値の誤差に対して、新たな学習をする
  - 損失関数の最小化に *勾配降下法* を用いる

- *ライブラリ*
  - XGBoost. R では ={xgboost}=. (eXtreme Gradient Boosting)
  - LightGBM by Microsoft. より新しい。Rでは ={lightgbm}=.
  - catboost. R では ={catboost}=.

- *ブースティング (Boosting)*
  - 複数の弱学習器を 1 つずつ順番に構築して、予測モデルを生成する手法
  - Booster を選択できる
    - Tree (Default)
    - Linear (あまり使われない)
    - DART (NN のドロップアウトを GBDT に適応)

- *目的関数 (=Learning Objective)*
  - タスクに応じて選択する
    - 回帰: reg:squarederror (RMSE)
    - 分類: binary:logistic (logloss)
    - マルチクラス分類: multi:softmax (multi-class logloss)

- ={xgboost}= vs. ={lightGBM}=
- *Level-wise vs. Leaf-wise*
  - Level-wise: 木の層全体を深くしていくことで学習 (={xgboost}=)
  - Leaf-wise: 木の葉の枝分かれを増やしていくことで学習 (={lightGBM}=)
    - 訓練時間が短い
    - より複雑なモデルを表現できる (= 過学習しやすい)
- lightGBM は特徴量を階級分けしてヒストグラム化する
  - データ量を削減して大規模データ・セットでも高速化
  - xgboost でも 

** ハイパーパラメタ

_ハイパーパラメタ (上の 3 つはランダムフォレストと共通)_
- [[https://sites.google.com/view/lauraepp/parameters][Laurae++: xgboost / LightGBM]]
- [[https://github.com/tidymodels/parsnip/issues/211][Parmeter Mapping]]
- [[https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db][CatBoost vs. Light GBM vs. XGBoost]]
- [[https://nykergoto.hatenablog.jp/entry/2019/03/29/%E5%8B%BE%E9%85%8D%E3%83%96%E3%83%BC%E3%82%B9%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E5%A4%A7%E4%BA%8B%E3%81%AA%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E6%B0%97%E6%8C%81%E3%81%A1][勾配ブースティングで大事なパラメータの気持ち]]

- {parsnip} / {xgboost} / {lightgbm} / {catboost}
|----------------+-----------------------+---------------------------------+-----------------------|
| =boost_tree()=   | =xgb.train()=           | =lgb.train()=                     | =catboost.train()=      |
|----------------+-----------------------+---------------------------------+-----------------------|
| tree_depth     | max_depth (6L)        | max_depth (-1L)                 | depth (6L)            |
| min_n          | min_child_weight (1L) | -                               | -                     |
|                |                       | min_sum_hessian_in_leaf (0.001) | -                     |
| learn_rate     | eta (0.3)             | learning_rate (0.1)             | leaning_rate (0.03)   |
| loss_reduction | gamma (0)             | min_gain_to_split (0)           |                       |
|                |                       |                                 | l2_leaf_reg           |
|                |                       | min_data_in_leaf (20L)          | min_data_in_leaf (1L) |
|                |                       | min_data_in_bin (5L)            |                       |
|----------------+-----------------------+---------------------------------+-----------------------|
| mtry           | colsample_bytrees (1) | feature_fraction (1)            | rsm (1)               |
| sample_size    | subsample (1)         | bagging_fraction (1)            | subsample (1)         |
| trees          | nrounds (NULL)        | num_iterations (100)            | iterations (1000)     |
|----------------+-----------------------+---------------------------------+-----------------------|
|                | max_leaves (255)      | num_leaves (31)                 | max_leaves (31)       |
|                |                       | categorical_feature             | cat_features          |
|                |                       |                                 | one_hot_max_size      |
|                | nthread               | num_threads                     | thread_count (-1L)    |
|----------------+-----------------------+---------------------------------+-----------------------|

- *モデルの複雑さを決める重要パラメタ*
  - =tree_depth (max_depth)= [3, 12] or [3, 8]
    - 木の深さ・分岐の多さ。深くすると、相互作用項を盛り込む複雑なモデルになる。
    - 一番最初にチューニングする
    - 大きくしすぎるよりも小さい木をたくさん作る方が GBDT らしい
    - 0 (xgboost), -1(lightgbm) で inifinite tree
    - catboost では 最大 16

  - =min_n (min_child_weight, min_sum_hessian_in_leaf)= [1, 5], [1, 2, 4, 8]
    - 分岐するために最低限必要なデータ数
    - 剪定 (Pruning)
    - 大きくすると分岐が起こりづらくなる
    - ヘッセ正則化

  - =loss_reduction (gamma, min_gain_to_split)= d=0.0, r=[1e-8, 1.0]
    - 分岐を作るために最低限減らさなければならない目的関数の値。
    - 基本的にはゼロでよい by Laurae
    - 0 以上の実数
    - 大きくすると分岐が起こりづらくなる
    - 損失正則化

- *ランダム性を加えることで、過学習を抑制するパラメタ*
  - =mtry (colsample_bytree, feature_fraction)= d=0.8/0.7, r=[0.6, 0.95]
    - 木毎に特徴量をサンプルする割合
  
  - =sample_size (subsample, bagging_fraction)= d=0.8, r=[0.6, 0.95]
    - 木毎に学習データの行をサンプルする割合
    - ={catboost}= の場合、Poisson, Bernoulli, MVS の bootstrap type のときのみ有効

  - 注) =mtry= や =sample_size= を指定すると seed で結果を固定することができない (={xgboost}=)
    - [[https://github.com/dmlc/xgboost/issues/3350][R: seed parameter doesn't work, except set.seed()]]

- *ほぼ固定してよいパラメタ*
  - =trees (num_rounds)=
    - 決定木の本数
    - 1000 や 10000 などの大きな値にしておき、アーリーストッピングを使う
    - その場合 =early_stopping_rounds=50= にする (10/eta に設定するのもよい)

  - =learn_rate (eta)= [0.01, 0.05]
    - 学習率。決定木の予測にこの率を乗じて予測値に加える。
    - 一般に小さほどよい。
    - 小さくしても精度は下がらないが、計算時間がかかるようになる。
    - チューニングするというよりは、時間と精度とのトレードオフで決定する
    - 最初は 0.1 程度にしておき、徐々に 0.01 - 0.05 程度に調整する。
    - ={catboost}= では =iterations= によって自動的に設定される

- その他
  - =max_leaves= ={xgboost}=, =num_leaves= ={lightgbm}= d=31, r[15, 4095]
    - 木ごとの葉の数. 多いほど複雑なモデル.
    - Leaf-wise のアルゴリズムで利用 (lightgbm ではデフォルト)
    - そもそも 2^max_depth-1 が葉の最大数なので =as.integer(0.7 * 2^max_depth)= のようにする
      - ex: a maximum depth of 10 leads to a maximum of 1023 leaves
      - max_depth が -1 (unlimited) でない場合は指定する必要あり？
    - Typical=255

  - =min_sum_hessian_in_leaf= ={lightgbm}=
    - 過学習抑止
    - 0 以上の実数
    - デフォルト 0.001
  
  - =min_data_in_leaf {lightgbm}=
    - ={xgboost}= には存在しない. 1 にすれば xgboost と同じ挙動
    - 小データの場合は、20 では大きすぎるので小さくする
    - [[https://github.com/microsoft/LightGBM/issues/640][No further splits with positive gain, best gain: -inf #640]]

  - =min_data_in_bin {lightgbm}=
    - 小データの場合は大きすぎるので、1にする
    - [[https://github.com/Microsoft/LightGBM/issues/547][Cannot get good regression result on small trivial data #547]]
    - [[https://github.com/microsoft/LightGBM/issues/380][small dataset - poor performance #380@github]]

  - =nthread {xgboost}=, =num_threads {lightgbm}=
    - CPU コア数

** [[file:../package/tidymodels/parsnip.org][ ={parsnip}= ]]

- ネスト順
  - =parsnip::boost_tree()= -> =parsnip::xgb_train()= -> =xgboost::xgb.train()=
#+begin_src R :results silent
parsnip::boost_tree(
  mode = "unknown",
  mtry = NULL,
  trees = NULL,
  min_n = NULL,
  tree_depth = NULL,
  learn_rate = NULL,
  loss_reduction = NULL,
  sample_size = NULL
)

## 引数の名前は、xgboost::xgb.train() 側に合わせてある
parsnip::xgb_train(
  x, # 説明変数のdf
  y, # 目的変数の vector/matrix
  max_depth = 6,
  nrounds = 15,
  eta = 0.3,
  colsample_bytree = 1,
  min_child_weight = 1,
  gamma = 0,
  subsample = 1,
  ...
)
#+end_src

** [[file:../package/xgboost/xgboost.org][ ={xgboost}= ]]
** [[file:../package/lightgbm.org][ ={lightgbm}= ]]
** [[file:../package/catboost.org][ ={catboost}= ]]
** ={gbm}=

- Generalized Boosted Model
- 現在は、開発が終了しメンテナンスのみ
  - 後継は ={gbm3}=
- [[https://cran.r-project.org/web/packages/gbm/index.html][CRAN - Package gbm]]
- [[https://github.com/gbm-developers/gbm][GitHub - gbm-developers/gbm: gbm: the old gbm package]]

** ={gbm3}=
* BART (Bayesian Additive Regression Trees)

- ={bartMachine}=
  
* モデルの解釈
** 変数重要度 (Feature Importance)

- 機械学習モデルは、モデルがブラックボックスになりがち
- 変数の重要度を表す指標
- =xgboost::xgb.importance()= -> =xgboost::xgb.ggplot.importance()=
- =lightgbm::lgb.importance()= -> =lightgbm::lgb.plot.importance()=
- 算出条件
  - "gain"   特徴量の分岐時の平均的なゲイン (回帰木)
  - "cover"  (分類木)
  - "weight" 特徴量が木の中で出現する回数 (回帰木)
  - *Permutation*
    - アルゴリズムに依存しない計算方法 (={ingredients}=, ={vip}= など)
    - 特定の変数の値をシャッフルして壊したときに精度が落ちるか？を見ている

** PDP (Partial Dependence Plot)

- 重要な変数とアウトカムの関係を見る
- ブラックボックスモデルの複雑な関係を可視化する
- 興味のある変数以外の影響を周辺化して消去する

- 興味のある変数の各データポイント毎に、他の変数を使ったモデルを作成
- それぞれの結果を平均したもの = PDP
- グループ毎に平均する = Partial PDP

** ICE (Individual Conditional Expectation)

- ICE = PDP と似ているが、各ポイントを平均しない
- 交互作用項を表現できる
- 以下の名称で呼ばれることもある
  - Ceteris Paribus Profile (ケテリス・パリブス)
  - Individual Variable Profile

** ALE (Accumulated Local Effect)
** [[file:../package/DALEX/DALEX.org][ ={DALEX}= ]]
* 参考

- [[https://qiita.com/tomomoto/items/b3fd1ec7f9b68ab6dfe2][代表的な機械学習手法一覧@Qiita]]

- 決定木
  - [[https://qiita.com/3000manJPY/items/ef7495960f472ec14377][(入門)初心者の初心者による初心者のための決定木分析@Qiita]]
  - [[https://www.st-hakky-blog.com/entry/2018/08/10/080242][今更感あるけど決定木について調べたのでまとめる@St_Hakky’s blog]]

- ランダムフォレスト
  - [[https://www.slideshare.net/sfchaos/r-rangerrborist][最近のRのランダムフォレストパッケージ -ranger/Rborist-@SlideShare]]
  - [[https://qiita.com/siero5335/items/ca902f2d33d0c56b6b13][森を彷徨う@Qiita]]
  - [[http://kazoo04.hatenablog.com/entry/2013/12/04/175402][Random Forest とその派生アルゴリズム@Sideswipe]]

- 勾配ブースティング
  - [[https://qiita.com/woody_egg/items/232e982094cd3c80b3ee][Kaggle Masterが勾配ブースティングを解説する@Qiita]]
  - [[https://www.codexa.net/lightgbm-beginner/][LightGBM 徹底入門 – LightGBMの使い方や仕組み、XGBoostとの違いについて]]
  - [[https://www.kaggle.com/andrewmvd/lightgbm-in-r][LightGBM in R@Kaggle]]
  - [[https://github.com/microsoft/LightGBM/blob/master/R-package/demo/basic_walkthrough.R][basic_walkthrough.R@Github]]
  - [[https://github.com/tidymodels/parsnip/issues/211][Parmeter Mapping]]
  - [[https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db][CatBoost vs. Light GBM vs. XGBoost]]

- Extremely Randomized Trees
  - [[https://daviddalpiaz.github.io/stat432sp18/lab/enslab/enslab.html][Extremely Randomized Trees, Ranger, XGBoost]]
  - [[https://stats.stackexchange.com/questions/406666/what-exactly-is-the-extratrees-option-in-ranger][What exactly is the extratrees option in ranger?@Stackoverflow]]

- Regularized Greedy Forest
  - [[http://segafreder.hatenablog.com/entry/2016/06/12/211050][【機械学習】Regularized Greedy Forest(RGF)で多クラス分類を試してみました@verilog 書く人]]
  - [[https://cran.r-project.org/web/packages/RGF/vignettes/the_RGF_package.html][Regularized Greedy Forest in R ({RGF} Vigette)]]
  - [[http://puyokw.hatenablog.com/entry/2016/07/17/210021][Regurarized Greedy Forest - puyokwの日記]]

- モデルの解釈
  - [[https://dropout009.hatenablog.com/entry/2019/01/07/124214][変数重要度とPartial Dependence Plotでブラックボックスモデルを解釈する@Dropout]]
