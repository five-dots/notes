#+STARTUP: folded indent inlineimages latexpreview
#+PROPERTY: header-args:R :session *R:lm* :results output :width 640 :height 480 :colnames yes

* ライブラリの読み込み
  
#+begin_src R :results silent
library(glmnet)
library(glmmML)
library(kernlab)

library(tidyverse)
library(tidymodels)
library(glue)

library(DescTools)
library(Metrics)
#+end_src

* 線形回帰分析
** =stats::lm()=
*** Usage

#+begin_src R :results silent
lm(
  formula,
  data, # data.frame, list, env
  subset, # データのindex を与えてサブセットする
  weights, # 観測値毎の weight (重み付き最小二乗法)
  na.action,
  method = "qr", # 当てはめとしては、qr のみサポートされている
  model = TRUE, # 結果にmodel.frame を含む
  x = FALSE, # 結果に model.matrix を含む
  y = FALSE, # 結果に model.response を含む
  qr = TRUE, # 結果に QR decomposition を含む
  singular.ok = TRUE, # FALSE にすると singular fit がエラーになる
  contrasts = NULL, # model.matrix.default の contracts.arg
  offset, # オフセット項
  ...
)

predict.lm(
  object, # lm object
  newdata, # data.frame
  se.fit = FALSE, # 予測値の SE を計算するか
  scale = NULL,
  df = Inf,
  interval = c("none", "confidence", "prediction"),
  level = 0.95,
  type = c("response", "terms"),
  terms = NULL,
  na.action = na.pass,
  pred.var = res.var/weights,
  weights = 1,
  ...
)
#+end_src

*** [[file:formula.org][formula]]
*** Data

y = a + bx + e のシミュレーションデータ
#+begin_src R :results value
seed <- 2020
n <- 1000
set.seed(seed)

## covariate
int <- matrix(0.3, n)
x <- matrix(rnorm(n), n)
e <- matrix(rnorm(n, sd = 0.2), n)
x <- cbind(int, x, e)

## y = 0.3 + 0.5*x1 + e
coef <- c(1.0, 0.5, 1.0)
y <- x %*% coef

mat <- cbind(y, x[, 2])
colnames(mat) <- c("y", "x")
dat <- tibble::as_tibble(mat)

head(dat)
#+end_src

#+RESULTS:
|                  y |                 x |
|--------------------+-------------------|
|   0.48941614452743 | 0.376972124936433 |
|  0.205024246770516 | 0.301548373935665 |
| -0.277131180868041 |  -1.0980231706536 |
| -0.306668346596742 | -1.13040590360378 |
|  -1.28257327554322 | -2.79653431987176 |
|  0.732381596438236 | 0.720573498411587 |

#+begin_src R :results output graphics file :file (my/get-babel-file)
dat %>%
  ggplot(aes(x = y)) + geom_histogram()
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-wTvF5D.png]]

*** =lm()= で当てはめ

- シミュレーションデータ通り推定できた
#+begin_src R
set.seed(1983)
rsplit <- initial_split(dat)
train <- training(rsplit)
test <- testing(rsplit)

lm_fit <- lm(y ~ ., data = train)
summary(lm_fit)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = y ~ ., data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6970 -0.1219 -0.0103  0.1334  0.7332 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.300791   0.007204   41.75   <2e-16 ***
x           0.504614   0.006828   73.90   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.197 on 748 degrees of freedom
Multiple R-squared:  0.8795,	Adjusted R-squared:  0.8794 
F-statistic:  5462 on 1 and 748 DF,  p-value: < 2.2e-16
#+end_example

*** lm object の見方

- [[file:stats_test.org][統計的検定]]
- =t value= (t 値)
  - 係数=0 という H0 に対する t 検定量
  - 絶対値が大きいほどよい (絶対値が 2 よりも大きいかが目安にされる)
  - t = 係数 / 標準誤差

- =p.value= の意味
  - H0(帰無仮説) は 係数 = 0 (つまり変数の影響がゼロ)
  - p.value は H0 の確率
  - t value が n-k-1 の t 分布に従うことを利用して計算

- =Std. Error= = 標準誤差(SE)
  - 係数の推定量の標準誤差  (標準誤差とはそもそも「推定量」のばらつきを表す)
  - 小さいほどよい
  - 回帰係数の分散共分散行列は =vcov(lm_fit)= で取り出すことができる
  - =sqrt(diag(vcov(lm_fit)))= で全ての SE を計算
  - SE = SD * sqrt(N)

#+begin_src R
summary(lm_fit)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = y ~ ., data = train)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6970 -0.1219 -0.0103  0.1334  0.7332 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.300791   0.007204   41.75   <2e-16 ***
x           0.504614   0.006828   73.90   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.197 on 748 degrees of freedom
Multiple R-squared:  0.8795,	Adjusted R-squared:  0.8794 
F-statistic:  5462 on 1 and 748 DF,  p-value: < 2.2e-16
#+end_example

*** =predict()= で予測

- 信頼区間 "confidence"
  - *平均が収まる区間*
  - 母平均のバラツキはサンプルが増えると小さくなるので、
    データ量が増えると、信頼区間は狭くなる
  - SE をバラツキの指標にした考え方

- 予測区間 "prediction"
  - *データが収まる区間*
  - サンプルサイズが増えても区間が狭まるわけではない
  - 残差のバラツキも考慮した区間
  - SD をバラツキの指標にした考え方
#+begin_src R :results value
pred1 <- predict(lm_fit, newdata = test, interval = "confidence", level = 0.95) %>%
  as_tibble()
pred2 <- predict(lm_fit, newdata = test, interval = "prediction", level = 0.95) %>%
  as_tibble()
head(pred1)
#+end_src

#+RESULTS:
|                fit |                lwr |                upr |
|--------------------+--------------------+--------------------|
|  0.452956886216681 |  0.438035576945577 |  0.467878195487785 |
| -0.269627714054715 | -0.289780264019951 | -0.249475164089479 |
|   1.18847409263019 |   1.16032037775472 |   1.21662780750566 |
| -0.129706707065871 | -0.147400469053615 | -0.112012945078126 |
|  0.759616447864455 |  0.740446379476215 |  0.778786516252694 |
|  0.238592389768088 |  0.224443051026874 |  0.252741728509301 |

#+begin_src R :results output graphics file :file (my/get-babel-file)
test %>%
  ## bind_cols(pred1) %>% # 信頼区間
  bind_cols(pred2) %>%  # 予測区間
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = fit), color = "blue") +
  geom_ribbon(aes(ymax = upr, ymin = lwr), alpha = 0.2)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-hwL42f.png]]

* 一般化線形モデル
** 概要

- 一般化線形モデル = 正規分布以外にも対応したモデル群

- 構成要素
  1. 確率分布 (probability distribution)
  2. 線形予測子 (linear predictor)
  3. リンク関数 (link function) 線形予測子と目的関数の関係を表すもの

- 対応モデル
  https://www.marketechlabo.com/r-glm-libraries/

- 線形回帰
  - =gaussian(link = "identity")=
  - 目的変数が正規分布に従う
  - 対応しているリンク関数は
    - identity  線形予測子＝推定値
    - log       対数正規
    - inverse

- ロジスティック回帰
  - =binomial(link = "logit")=
  - 目的変数が 2 値変数
  - 対応しているリンク関数は
    - logit   ロジスティック回帰／ロジットモデル
    - probit  プロビットモデル
    - cauchit (= Cauchy)
    - log
    - cloglog complementary log-log
    - リンク関数の違い: http://www.karlin.mff.cuni.cz/~kulich/vyuka/pokreg/R/glm_binary_links.html

- ポワソン回帰
  - =poisson(link = "log")=
  - 目的変数が 0 以上の離散変数、分散がそこまで大きくない
  - 対応しているリンク関数は
    - log       対数線形モデル
    - identity
    - sqrt

- ガンマ回帰
  - =Gamma(link = "inverse")=
  - 目的変数が 0 以上の連続変量
  - 対応しているリンク関数は
    - inverse
    - identity
    - log

- =inverse.gaussian(link = "1/mu^2")=
- 対応しているリンク関数は
  - 1/mu^2
  - inverse
  - identity
  - log

- その他
  - =quasi(link = "identity", variance = "constant")=
  - =quasibinomial(link = "logit")=
  - =quasipoisson(link = "log")=

** =stats::glm()= function

#+begin_src R
glm(
  formula,
  family = gaussian,
  data,
  weights,
  subset,
  na.action,
  start = NULL,
  etastart,
  mustart,
  offset,
  control = list(...),
  model = TRUE,
  method = "glm.fit",
  x = FALSE,
  y = TRUE,
  singular.ok = TRUE,
  contrasts = NULL,
  ...
)

predict.glm(
  object,
  newdata = NULL,
  type = c("link", "response", "terms"),
  se.fit = FALSE,
  dispersion = NULL,
  terms = NULL,
  na.action = na.pass,
  ...
)
#+end_src

** ={glmmML}= pacakge (GLM with Clustering)

- Cluster = 場所差・個体差

#+begin_src R
glmmML(formula, family = binomial, data, cluster, weights,
       cluster.weights, subset, na.action,
       offset, contrasts = NULL, prior = c("gaussian", "logistic", "cauchy"),
       start.coef = NULL, start.sigma = NULL, fix.sigma = FALSE, x = FALSE,
       control = list(epsilon = 1e-08, maxit = 200, trace = FALSE),
       method = c("Laplace", "ghq"), n.points = 8, boot = 0)
#+end_src

** ={glmnet}= packge (GLM with Regularization)
*** 概要

正則化ありの回帰 ={glmnet}= を使う
- _Lasso = L1 正則化を行う回帰 (係数の絶対値に応じて罰則)_
  - Least absolute selection and shrinkage operator
  - *alpha = 1*
  - スパース推定 (いくつかの変数の係数がゼロになる) ともいう
  - Adaptive Lasso = 変数選択の一致性が保証される

- _Ridge = L2 正則化を行う回帰 (係数の二乗に応じて罰則)_
  - *alpha = 0*
  - 相関のある変数の係数を小さくする働き
  - Lasso のように変数を削減はしない
  - Neural network の世界では weight decay と呼ばれる
 
- _ElasticNet = L1 + L2 正則化を任意の割合で組み合わせたもの_
  - *alpha = 0 < alpha < 1*

- 正則化の度合いを決めるパラメタ *lambda* (Complexity Paramter) がハイパーパラメタ
  - lambda = 0 は通常の線形回帰
  - 10 ^ (1:10 * -1)

- [[file:./model_selection.org][model_selection: 正則化]]

- lambda の探索範囲例
#+begin_src R
10 ^ (1:10 * -1)
#+end_src

#+RESULTS:
:  [1] 1e-01 1e-02 1e-03 1e-04 1e-05 1e-06 1e-07 1e-08 1e-09 1e-10

*** Reference

- [[https://stats.stackexchange.com/questions/77546/how-to-interpret-glmnet/77549][How to interpret glmnet?@CrossValidated]]
- [[https://stats.stackexchange.com/questions/304440/building-final-model-in-glmnet-after-cross-validation][Building final model in glmnet after cross validation@CrossValidated]]
- [[https://stackoverflow.com/questions/23686067/default-lambda-sequence-in-glmnet-for-cross-validation][default lambda sequence in glmnet for cross-validation@CrossValidated]]
- [[https://stats.stackexchange.com/questions/243347/why-is-cv-glmnet-returning-absurd-coefficients-when-intercept-term-is-omitted][Why is cv.glmnet returning absurd coefficients when intercept term is omitted?@CrossValidated]]

- family (自動で選択してはくれない)
  - ="gaussian"= = 数値
  - ="poisson"= = 非負のカウントデータ
  - ="binomial"= =  2-level factor, 2 interger label, 2-col matrix (count or ratio)
  - ="multinomial"= = n-level factor, n integer label, n-col matrix (count or ratio)
  - ="cox"= = 2-col matrix ("time" + "status"(binary: 1=death, 0=right censored))
    - =survival::Surv()= でデータを作成できる
  - ="mgaussian"= = n-col matrix の数値

#+begin_src R
glmnet(
  ## matrix or sparse matrix from {Matrix}
  x,
  ## タスクにより異なる (binomial/multinomial の場合は factor 型)
  y,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  ## 観測値(行)毎の重み. デフォルトで全てに1
  weights,
  offset = NULL,
  ## 0 ~ 1: Lasso(1), Ridge(0) の混合割合
  alpha = 1,
  ## lambda シーケンスの数
  nlambda = 100,
  lambda.min.ratio = ifelse(nobs < nvars, 0.01, 1e-04),
  ## nlambda と lambda.min.ratio から lambda シーケンスが生成される
  ## ここに scalar 値を設定しないこと!!
  lambda = NULL,
  ## 特徴量を正規化するか
  standardize = TRUE,
  ## 切片を含むか (model.matrix での切片有無は影響しない)
  intercept = TRUE,
  ## Convergence threshold
  thresh = 1e-07,
  ## 特徴量として選択する最大数 (特徴量が多すぎる場合に上限を設定する)
  dfmax = nvars + 1,
  ## 特徴量として選択する最大数 (係数がゼロでないもの)
  pmax = min(dfmax * 2 + 20, nvars),
  ## 除外する特徴量のインデックス
  exclude,
  ## 係数毎の正則化パラメタ
  penalty.factor = rep(1, nvars),
  ## 係数の下限
  lower.limits = -Inf,
  ## 係数の上限
  upper.limits = Inf,
  maxit = 1e+05,
  type.gaussian = ifelse(nvars <500, "covariance", "naive"),
  type.logistic = c("Newton", "modified.Newton"),
  ## family="mgaussian" のときのみ利用
  standardize.response = FALSE,
  type.multinomial = c("ungrouped", "grouped"), 
  relax = FALSE,
  trace.it = 0,
  ...)
#+end_src

- クロスバリデーション
- lambda を決定するために利用
#+begin_src R
cv.glmnet(
  x,
  y,
  weights = NULL,
  offset = NULL,
  lambda = NULL,
  ## 最適化する損失関数
  type.measure = c("default", "mse", "deviance", "class", "auc", "mae", "C"), 
  nfolds = 10,
  foldid = NULL,
  alignment = c("lambda", "fraction"),
  grouped = TRUE,
  keep = FALSE,
  parallel = FALSE,
  gamma = c(0, 0.25, 0.5, 0.75, 1),
  relax = FALSE,
  trace.it = 0,
  ...)
#+end_src

*** [[https://stats.biopapyrus.jp/sparse-modeling/glmnet.html][R の glmnet パッケージを利用した LASSO 推定と Elastic Net 推定]] の例
**** Lasso 回帰 by ={glmnet}=
***** データ

- L1 正則化ありの回帰 = Lasso 回帰
- x3, x4 の推定値がゼロになることを期待
#+begin_src R :results output graphics file :file (my/get-babel-file)
x1 <- rpois(1000, 5)
x2 <- rnorm(1000, 20, 3)
x3 <- x2 + rnorm(1000, 0, 1)
x4 <- x1 + x2 + rnorm(1000, 0, 1)
x5 <- rnbinom(1000, 10, 0.5)
X <- cbind(x1, x2, x3, x4, x5)
## Y = 4*x1 - 2*x2 + x5 
Y <- 4 * x1 - 2 * x2 + x5 + rnorm(1000, 0, 1)
plot(Y)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-NaizLk.png]]

***** モデル

- CV で最適な lambda を探索
- 評価関数は、デフォルトで逸脱度 (Deviance) を利用
- alpha=1 で Lasso 回帰 (0 < alpha < 1 で ElasticNet. Ridge との混合)
- =intercept=FALSE= にするとおかしな値になる
  - [[https://stats.stackexchange.com/questions/243347/why-is-cv-glmnet-returning-absurd-coefficients-when-intercept-term-is-omitted][Why is cv.glmnet returning absurd coefficients when intercept term is omitted?@CrossValidated]]
#+begin_src R
library(glmnet)
set.seed(1983)
lasso.model.cv <- cv.glmnet(x = X, y = Y, family = "gaussian", alpha = 1)
lasso.model.cv
#+end_src

#+RESULTS:
: 
: Call:  cv.glmnet(x = X, y = Y, family = "gaussian", alpha = 1) 
: 
: Measure: Mean-Squared Error 
: 
:      Lambda Measure      SE Nonzero
: min 0.04761  0.9955 0.03452       3
: 1se 0.10999  1.0240 0.03590       3

- cv では 1set と min が選択されるが、実際には複数の lambda シーケンスが計算されている
#+begin_src R
lasso.model.cv$lambda
which(lasso.model.cv$lambda == lasso.model.cv$lambda.1se) # 48 番目
which(lasso.model.cv$lambda == lasso.model.cv$lambda.min) # 57 番目
#+end_src

#+RESULTS:
#+begin_example
 [1] 8.71682984 7.94245070 7.23686527 6.59396211 6.00817269 5.47442319
 [7] 4.98809052 4.54496230 4.14120038 3.77330755 3.43809731 3.13266622
[13] 2.85436879 2.60079453 2.36974711 2.15922531 1.96740569 1.79262679
[19] 1.63337476 1.48827025 1.35605643 1.23558811 1.12582187 1.02580696
[25] 0.93467709 0.85164296 0.77598534 0.70704894 0.64423667 0.58700446
[31] 0.53485660 0.48734141 0.44404734 0.40459940 0.36865591 0.33590553
[37] 0.30606461 0.27887467 0.25410022 0.23152665 0.21095846 0.19221749
[43] 0.17514142 0.15958234 0.14540549 0.13248806 0.12071819 0.10999392
[49] 0.10022236 0.09131888 0.08320637 0.07581454 0.06907939 0.06294256
[55] 0.05735092 0.05225602 0.04761374

[1] 48

[1] 57
#+end_example

- 最適な lambda とその時のパラメタ数 (上段) がマッピングされる
- デフォルトでは、lambda.1se が利用される
- [[https://stats.stackexchange.com/questions/138569/why-is-lambda-within-one-standard-error-from-the-minimum-is-a-recommended-valu][Why is lambda “within one standard error from the minimum” is a recommended value for lambda in an elastic net regression?@CrossValidated]]
- 1se を使ったほうがより正則化がきつい
#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(lasso.model.cv)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-M33Adp.png]]

- 対数をとったものが使われる
#+begin_src R
log(lasso.model.cv$lambda.min)
log(lasso.model.cv$lambda.1se)
#+end_src

#+RESULTS:
: [1] -3.044634
: 
: [1] -2.021263

#+begin_src R
coef(lasso.model.cv, s = "lambda.1se")
#+end_src

#+RESULTS:
: 6 x 1 sparse Matrix of class "dgCMatrix"
:                      1
: (Intercept)  0.2690160
: x1           3.9477096
: x2          -1.9900733
: x3           .        
: x4           .        
: x5           0.9778439

- 実際にやっていることは =glmnet()= と同じ
#+begin_src R
set.seed(1983)
lasso.model <- glmnet(x = X, y = Y, family = "gaussian", alpha = 1)
lasso.model
#+end_src

#+RESULTS:
#+begin_example

Call:  glmnet(x = X, y = Y, family = "gaussian", alpha = 1) 

   Df    %Dev Lambda
1   0 0.00000 8.7170
2   1 0.09305 7.9420
3   1 0.17030 7.2370
4   2 0.24820 6.5940
5   2 0.34900 6.0080
6   2 0.43270 5.4740
7   2 0.50230 4.9880
8   3 0.56860 4.5450
9   3 0.64060 4.1410
10  3 0.70040 3.7730
11  3 0.75010 3.4380
12  3 0.79130 3.1330
13  3 0.82560 2.8540
14  3 0.85400 2.6010
15  3 0.87760 2.3700
16  3 0.89710 2.1590
17  3 0.91340 1.9670
18  3 0.92690 1.7930
19  3 0.93810 1.6330
20  3 0.94740 1.4880
21  3 0.95510 1.3560
22  3 0.96160 1.2360
23  3 0.96690 1.1260
24  3 0.97130 1.0260
25  3 0.97500 0.9347
26  3 0.97800 0.8516
27  3 0.98060 0.7760
28  3 0.98270 0.7070
29  3 0.98440 0.6442
30  3 0.98580 0.5870
31  3 0.98700 0.5349
32  3 0.98800 0.4873
33  3 0.98890 0.4440
34  3 0.98960 0.4046
35  3 0.99010 0.3687
36  3 0.99060 0.3359
37  3 0.99100 0.3061
38  3 0.99130 0.2789
39  3 0.99160 0.2541
40  3 0.99180 0.2315
41  3 0.99200 0.2110
42  3 0.99220 0.1922
43  3 0.99230 0.1751
44  3 0.99240 0.1596
45  3 0.99250 0.1454
46  3 0.99260 0.1325
47  3 0.99260 0.1207
48  3 0.99270 0.1100
49  3 0.99270 0.1002
50  3 0.99270 0.0913
51  3 0.99280 0.0832
52  3 0.99280 0.0758
53  3 0.99280 0.0691
54  3 0.99280 0.0629
55  3 0.99290 0.0574
56  3 0.99290 0.0523
57  3 0.99290 0.0476
#+end_example

***** 再モデル化 by 最適 lambda

- 想定通り、x3, x4 が係数 0 と推定された
#+begin_src R
best_lambda <- lasso.model.cv$lambda.min
lasso.model <- glmnet(x = X, y = Y, family = "gaussian", lambda = best_lambda, alpha = 1, intercept = TRUE)
lasso.model$beta
#+end_src

#+RESULTS:
: 
: 5 x 1 sparse Matrix of class "dgCMatrix"
:            s0
: x1  3.9761898
: x2 -2.0085064
: x3  .        
: x4  .        
: x5  0.9911893

**** Ridge 回帰 by ={glmnet}=

- CV で最適な lambda を探索
- alpha=0 で Ridge 回帰
#+begin_src R
library(glmnet)
ridge.model.cv <- cv.glmnet(x = X, y = Y, family = "gaussian", alpha = 0)
ridge.model.cv$lambda.min
#+end_src

#+RESULTS:
: [1] 0.8493022

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(ridge.model.cv)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-w3aHtu.png]]

- Ridge 回帰 by 最適 Lambda
  - Lasso 回帰のときのように x3, x4 の係数はゼロにならない
#+begin_src R
ridge.model <- glmnet(x = X, y = Y, family = "gaussian", lambda = 0.9748698, alpha = 0)
ridge.model$beta
#+end_src

#+RESULTS:
: 5 x 1 sparse Matrix of class "dgCMatrix"
:            s0
: x1  3.5815336
: x2 -1.4168279
: x3 -0.6036017
: x4  0.1198341
: x5  0.9242469

**** Elastic Net by ={glmnet}=

- alpha (割合 0 ~ 1) を指定して、Lasso/Ridge を混合したもの
- 0 ~ 1 まで総当りで CV を実行し、最適な alpha/lamba を求める
#+begin_src R
alpha <- seq(0.01, 0.99, 0.01)
mse.df <- NULL

for (i in 1:length(alpha)) {
    m <- cv.glmnet(x = X, y = Y, family = "gaussian", alpha = alpha[i])
    mse.df <- rbind(mse.df, data.frame(alpha = alpha[i], mse = min(m$cvm)))
}

best.alpha <- mse.df$alpha[mse.df$mse == min(mse.df$mse)]
m <- cv.glmnet(x = X, y = Y, family = "gaussian", alpha = best.alpha)
best.lambda <- m$lambda.min
best.lambda
#+end_src

#+RESULTS:
: [1] 0.04594562

- サンプルデータをうまく表現できた
#+begin_src R
en.model <- glmnet(x = X, y = Y, family = "gaussian",
                   lambda = best.lambda, alpha = best.alpha)
en.model$beta
#+end_src

#+RESULTS:
: 5 x 1 sparse Matrix of class "dgCMatrix"
:            s0
: x1  3.9774437
: x2 -1.9978133
: x3  .        
: x4  .        
: x5  0.9899712

*** [[http://rpubs.com/kaz_yos/alasso][Adaptive LASSO@RPubs]]
**** 概要

- 通常の Lasso では変数選択の一致性が保証されない

流れ
1. リッジ回帰 CV で =lambda= を算出
2. リッジ回帰の係数に =lambda= の罰則を与えたものを best_ridge_coef として用意
3. ラッソ回帰 CV の =penalty.factor= に =1 / abs(best_ridge_coef)= を与える
4. ラッソ回帰の lambda で更に係数に罰則を加えたものが最終的な Adaptive Lasso の係数になる
 
**** パッケージ + データ

#+begin_src R results silent
library(tidyverse)
library(magrittr)
library(glmnet)
library(pROC)

load(system.file("data/QuickStartExample.RData", package = "glmnet"))
x_cont <- x
y_cont <- y
#+end_src

**** 初回のリッジ回帰

#+begin_src R :results output graphics file :file (my/get-babel-file)
ridge1 <- glmnet(x = x_cont, y = y_cont, alpha = 0)
plot(ridge1, xvar = "lambda")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-tmeum4.png]]

**** リッジ回帰 (CV)

#+begin_src R :results output graphics file :file (my/get-babel-file)
ridge1_cv <- cv.glmnet(x = x_cont, y = y_cont, type.measure = "mse", nfold = 10, alpha = 0)
plot(ridge1_cv)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-VinYGp.png]]

#+begin_src R
ridge1_cv$lambda.min
#+end_src

#+RESULTS:
: [1] 0.1630762

- 最適 lambda で係数にペナルティをつける
#+begin_src R
coef(ridge1_cv)
coef(ridge1_cv, s = ridge1_cv$lambda.min)
#+end_src

#+RESULTS:
#+begin_example
21 x 1 sparse Matrix of class "dgCMatrix"
                       1
(Intercept)  0.213396626
V1           1.169739470
V2           0.051728030
V3           0.639167927
V4          -0.004899885
V5          -0.787601335
V6           0.574962762
V7           0.120122487
V8           0.336713520
V9          -0.045006251
V10          0.061717501
V11          0.220721133
V12         -0.062465252
V13         -0.028896612
V14         -0.973899245
V15         -0.079116188
V16         -0.012846775
V17         -0.013402475
V18          0.066050651
V19          0.005408401
V20         -0.962573918
21 x 1 sparse Matrix of class "dgCMatrix"
                      1
(Intercept)  0.14576690
V1           1.30922436
V2           0.03496846
V3           0.72391240
V4           0.03882705
V5          -0.86710569
V6           0.60697109
V7           0.12355737
V8           0.37889309
V9          -0.03973640
V10          0.10841981
V11          0.24189927
V12         -0.06661643
V13         -0.04268166
V14         -1.09804121
V15         -0.12176667
V16         -0.03711366
V17         -0.04019624
V18          0.06146105
V19         -0.00179925
V20         -1.08563245
#+end_example

- リッジ CV での最適係数
- Intercept は除いておく
#+begin_src R
best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min))[-1]
best_ridge_coef
#+end_src

#+RESULTS:
:  [1]  1.30922436  0.03496846  0.72391240  0.03882705 -0.86710569  0.60697109
:  [7]  0.12355737  0.37889309 -0.03973640  0.10841981  0.24189927 -0.06661643
: [13] -0.04268166 -1.09804121 -0.12176667 -0.03711366 -0.04019624  0.06146105
: [19] -0.00179925 -1.08563245

**** リッジ VIF

#+begin_src R :results output graphics file :file (my/get-babel-file)
ridge_vif <- function(x, lambda) {
    ZtZ <- cor(x)
    lambdaI <- diag(rep(lambda, ncol(x)))
    diag(solve(ZtZ + lambdaI) %*% ZtZ %*% solve(ZtZ + lambdaI))
}


lapply(sort(c(seq(from = 1/10^3, to = 1, by = 1/10^3), ridge1$lambda)), function(lambda) {
    bind_cols(data_frame(lambda = lambda),
              as_data_frame(t(ridge_vif(x, lambda))))
}) %>%
    bind_rows() %>%
    gather(key = key, value = value, -lambda) %>%
    ggplot(mapping = aes(x = lambda, y = value, group = key, color = key)) +
    geom_line() +
    scale_x_log10() +
    labs(y = "VIF") +
    theme_bw() +
    theme(legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-NNazvv.png]]

**** Adaptive Lasso

- =penalty.factor= に *リッジ係数の絶対値の逆数* を設定する
#+begin_src R :results output graphics file :file (my/get-babel-file)
alasso1 <- glmnet(x = x_cont, y = y_cont, alpha = 1,
                  penalty.factor = 1 / abs(best_ridge_coef))
plot(alasso1, xvar = "lambda")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-HpPB8t.png]]

**** Adaptive Lasso (CV)

#+begin_src R :results output graphics file :file (my/get-babel-file)
alasso1_cv <- cv.glmnet(x = x_cont, y = y_cont, type.measure = "mse",
                        nfold = 10, alpha = 1,
                        penalty.factor = 1 / abs(best_ridge_coef),
                        keep = TRUE)
## Penalty vs CV MSE plot
plot(alasso1_cv)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-RismI5.png]]

#+begin_src R
alasso1_cv$lambda.min
#+end_src

#+RESULTS:
: [1] 0.6019584

#+begin_src R
best_alasso_coef1 <- coef(alasso1_cv, s = alasso1_cv$lambda.min)
best_alasso_coef1
#+end_src

#+RESULTS:
#+begin_example
21 x 1 sparse Matrix of class "dgCMatrix"
                     1
(Intercept)  0.1269282
V1           1.3864115
V2           .        
V3           0.7573605
V4           .        
V5          -0.8937925
V6           0.5717928
V7           .        
V8           0.3654423
V9           .        
V10          .        
V11          0.1823183
V12          .        
V13          .        
V14         -1.1150858
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20         -1.1268620
#+end_example

*** [[https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html][Glmnet Vignette]]

** ロジスティック回帰
*** 概要

- 目的変数が (0, 1) データで利用する
- 発生確率(data=1)が知りたいときに利用

- 確率分布に二項分布、リンク関数にロジットリンク関数 (logit link fun)
- 二項分布の発生確率が、説明変数によって変動する統計モデル
  - 他にも、probit link, complementary log-log link fun などが使われる
  - 二項分布の生起確率、0 <= p <= 1 という制約をうまく扱うために、ロジットリンク関数が使われる
- 回帰係数がオッズ比で表現される
  - オッズ比 (n 倍) で発生確率が増減するモデル

*** [[file:math.org][Math (関数)]]
*** [[http://www.ner.takushoku-u.ac.jp/masano/class_material/waseda/keiryo/15_logit.html][ロジスティック回帰分析]] の例
**** データ

- 選挙の当落データ
  - wlsmd 当落(1=当選)
  - previous 当選回数
  - expm 選挙費用
#+begin_src R :results value
url <- "http://www.ner.takushoku-u.ac.jp/masano/class_material/waseda/keiryo/logit.csv"
data <- read_csv(url)
head(data)
#+end_src

#+RESULTS:
| wlsmd | previous | expm |
|-------+----------+------|
|     1 |        0 |   10 |
|     1 |        1 |   10 |
|     1 |        3 |  8.9 |
|     1 |        5 |  7.7 |
|     1 |        7 |  5.4 |
|     1 |        4 |    3 |

**** プロット

- 当選回数 (previous) と当落 (wlsmd)の散布図
#+begin_src R :results output graphics file :file (my/get-babel-file)
ggplot(data, aes(previous, wlsmd)) + geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_jitter(width = 0.05, height = 0.05) #jitterで重複したデータを散らす
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-4OPQNF.png]]

**** ロジスティック回帰

#+begin_src R
model_1 <- glm(wlsmd ~ previous + expm, data = data,
               family = binomial(link = "logit"))
summary(model_1)
#+end_src

#+RESULTS:
#+begin_example

Call:
glm(formula = wlsmd ~ previous
expm, family = binomial(link = "logit"), 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5741  -0.3781   0.2013   0.3943   1.4948  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)  -6.3811     3.5147  -1.816   0.0694 .
previous      0.8085     0.5851   1.382   0.1670  
expm          0.8088     0.4000   2.022   0.0431 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 20.728  on 14  degrees of freedom
Residual deviance: 10.384  on 12  degrees of freedom
AIC: 16.384

Number of Fisher Scoring iterations: 6
#+end_example

**** p の予測値

- 各データの当選確率(p)の予測値を出力
#+begin_src R
predict(model_1, type="response")
#+end_src

#+RESULTS:
:           1           2           3           4           5           6 
: 0.846455457 0.925228189 0.962419494 0.979953464 0.974574115 0.327276800 
:           7           8           9          10          11          12 
: 0.327213998 0.925168964 0.009934946 0.051995861 0.710296138 0.088057029 
:          13          14          15 
: 0.522058216 0.022027721 0.327339608

** ポアソン回帰
*** 概要

- カウントデータもしくは、イベントの発生確率をモデル化する目的
- データの分布にポワソン分布を想定し、説明変数によって、lambda (平均=分散) が変化するモデル
- リンク関数として通常は対数関数が使われる (非負が保証される)

リンク関数 g とすると
$g(\lambda) = log(\lambda) = \bf{x}\beta = \begin{pmatrix} 1 & x \end{pmatrix} \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}$
- 左辺 = リンク関数
- 右辺 = 線形予測子
- 

*** [[file:distribution.org][ポアソン分布]]
*** [[https://stats.biopapyrus.jp/glm/poisson-regression.html][ポアソン回帰@biostatistics]]
**** データ

- ガラパゴス諸島で島ごとに観測された種の数
#+begin_src R
data(gala, package = 'faraway')
head(gala)
#+end_src

#+RESULTS:
: 
:              Species Endemics  Area Elevation Nearest Scruz Adjacent
: Baltra            58       23 25.09       346     0.6   0.6     1.84
: Bartolome         31       21  1.24       109     0.6  26.3   572.33
: Caldwell           3        3  0.21       114     2.8  58.7     0.78
: Champion          25        9  0.10        46     1.9  47.4     0.18
: Coamano            2        1  0.05        77     1.9   1.9   903.82
: Daphne.Major      18       11  0.34       119     8.0   8.0     1.84

- 島の面積が広いほど種が多い
#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(log10(gala$Area), gala$Species, xlab = 'log10(Area)', ylab = 'Species')
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-GwipeQ.png]]

**** モデル化

- 島の面積の影響
#+begin_src R
x <- log10(gala$Area * 1000000) # 単位をmにして、対数をとる
y <- gala$Species

m <- glm(y ~ x, family = poisson(link = "log"))
summary(m)
#+end_src

#+RESULTS:
#+begin_example

Call:
glm(formula = y ~ x, family = poisson(link = "log"))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-10.4688   -3.6073   -0.8874    2.9028   10.1517  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.39281    0.13694  -10.17   <2e-16 ***
x            0.77767    0.01647   47.21   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 3510.73  on 29  degrees of freedom
Residual deviance:  651.67  on 28  degrees of freedom
AIC: 816.5

Number of Fisher Scoring iterations: 5
#+end_example

**** モデル結果を図示

- 切片・回帰係数から lambda を推定
$E[Y] = \lambda = exp(-1.39281 + 0.77767x)$

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(x, y, xlab = "log10(Area)", ylab = "Species")
x.new <- seq(2, 10, 0.1)
lines(x.new, exp(cbind(1, x.new) %*% coef(m)))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-bwvpwl.png]]

**** モデルの信頼区間

#+begin_src R :results output graphics file :file (my/get-babel-file)
y.predicted <- predict(m, newdata = data.frame(x = x.new), type = 'link', se.fit = TRUE)

alpha <- 0.05
ci.upper <- y.predicted$fit + (qnorm(1 - alpha / 2) * y.predicted$se.fit)
ci.lower <- y.predicted$fit - (qnorm(1 - alpha / 2) * y.predicted$se.fit)

plot(x, y, xlab = "log10(Area)", ylab = "Species")
x.new <- seq(2, 10, 0.1)
lines(x.new, exp(cbind(1, x.new) %*% coef(m)))
lines(x.new, exp(ci.upper), col = 'darkgray')
lines(x.new, exp(ci.lower), col = 'darkgray')
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-hlB6PA.png]]

*** 緑本のデータ

#+begin_src R
data3 <- read.csv(glue("{repos}/Workspace/R/data/green_book/data3a.csv",
                       repos = Sys.getenv()["REPOS"]), stringsAsFactors = TRUE)

fit1 <- glm(y ~ x,     family = poisson(link = "log"), data = data3)
fit2 <- glm(y ~ f,     family = poisson, data = data3)
fit3 <- glm(y ~ x + f, family = poisson, data = data3)
fit4 <- glm(y ~ 1,     family = poisson, data = data3) # 切片のみのモデル
summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)
#+end_src

- 結果の見方
- Estimate:   パラメータの最尤推定値
- Std. Error: 標準誤差 (SE) の推定値(= Estimate のばらつきを標準偏差で表したもの, ばらつきに正規分布を仮定)
- z value:    z 値 = Estimate / SE. Wald 統計量と呼ばれる.
- Pr(>|z|):   平均が z 値 の絶対値であり、標準偏差 1 の正規分布における、-Inf ~ 0 までの値を取る確率の 2 倍
            この確率が大きいほど、z値がゼロ近くになり、推定値がゼロに近いことを表現 (信頼区間が近似的に算出されたと考える)

- 最大対数尤度
#+begin_src R
logLik(fit1)
logLik(fit2)
logLik(fit3) # 最も大きい
logLik(fit4) # 最も小さい
#+end_src

- 逸脱度 (Deviance)
#+begin_src R
D <- -2 * logLik(fit1) # カイ 2 乗分布との対応関係をよくするため、-2 をかける
#+end_src

- 最小逸脱度 (フルモデル = データ数と同じ数のパラメーターを使ったモデル)
#+begin_src R
loglik <- sum(log(dpois(data3$y, lambda = data3$y))) # データ = lambda なので、尤度は最大になる
min_deviance <- -2 * loglik # 385.7795 <= 最小逸脱度
#+end_src

- 残差逸脱度 (Residual Deviance) = 相対的な当てはまりの悪さ
#+begin_src R
D - min_deviance
#+end_src

- Null Deviance (= 残差逸脱度の最大値)
  - Null model = 切片だけのモデル
#+begin_src R
-2 * logLik(fit4) - min_deviance
#+end_src

- AIC
  - AIC = -2{(最大対数尤度) - (最尤推定したパラメタ数)} = -2(LogL* - k) = D + 2k
#+begin_src R
D + 2 * 2 # AIC = 474.77 (k = 2)
#+end_src

* ロバスト回帰
** Reference

- R で学ぶロバスト推定
  https://www.slideshare.net/sfchaos/r-7773031

** Overview

- 外れ値に強い回帰分析
- 非説明変数に外れ値を想定 = M 推定
- 説明変数にも外れ値を想定 = MM 推定
- 外れ値に対する重みの付け方 Tukey's biweight / Huber weight

** MASS::rlm()

#+begin_src R

#+end_src

** robustbase::lmrob()

- こちらの方が最新の分析手法を掲載している

* 線形混合モデル (Liner Mixed-Effects)
* サポートベクターマシン (SVM, Support Vector Machine)
** 概要

- [[https://logics-of-blue.com/svm-concept/][サポートベクトルマシンの考え方@Logics of Blue]]

- カーネル法を利用し、高次元の問題も解けるようにした線形分類器
- *線形回帰をベースにより汎化性能を高める工夫をしたもの*
  - 線形回帰のような線を引いて、分類問題を解く (= 線形識別モデル)
  - 線形問題は、逆行列さえ計算できれば問題を解くことができるので、便利

- [[https://www.cis.doshisha.ac.jp/mjin/R/31/31.html][カーネル法とは]]
  - _データをより高次元 (例えば 2 次元から 3 次元)へ変換することで、非線形の問題を線形の問題にする_
  - 主成分分析 (高次元から低次元への変換) の逆 
  - 通常は計算量が大きくなってしまうが、内積計算などのテクニックを利用
    - 計算量を押さえたものを _カーネル関数_ と呼ぶ
  - 多項式カーネル = 複雑なデータ・セットに対応する
  - RBF カーネル = 複雑さを維持しつつ、多項式の「組み合わせ爆発」を回避 (=最も一般的に用いられる)

- 分類問題・回帰問題どちらも OK
  - 外れ値の検出にも応用される

- サポートベクター = 外れ値を除いた本当に予測に役立つデータ
- *マージン最大化* によって、サポートベクターを計算する
  - 分類の境界線とデータとの距離を最大化する
  - つまり分類の境界に余裕をもたせる、ということ (=> データのブレに強い・汎化性能が高まる)
  - _境界線に最も近いデータをサポートベクトルという_ (=分類する上で重要なデータ)

- ハードマージンとソフトマージン
  - ハードマージン = サポートベクトル間のマージンを絶対に確保しようとする
  - ソフトマージン = 誤判別を許容することで、過学習を抑制
  - 一般的にソフトマージンの方が精度が高い
  - 誤判定の許容度 C = ハイパーパラメタ

- R Package
  - ={kernlab}=
    - カーネル法を用いた手法を多数そろえている
      - サポートベクターマシン
      - カーネル主成分分析
  - ={e1071}=

** [[file:../package/kernlab.org][ ={kernlab}= ]]
* Field-aware Factorization Machine (FFM)
** 概要

- R Package
  - FM: ={libFMexe}= C++ の libFM をラップしたもの
  - FFM: なし. C++ は LIBFFM. Python のラッパーは多数あり

- [[http://hhok777.hatenablog.com/entry/2016/10/19/204037][（draft）FFM (Field-aware Factorization Machines)を理解したい]]
- Factorization Machine の改良版
- FM は MF (Matrix Factorization) をより汎用にしたもの
- CTR (click-through rate), CVR (convertion rate) の予測コンペで良い成績
- 大量のスパースなデータで相互作用を盛り込むことが可能
  - <課題>
  - スパースなデータでは、相互作用のデータ量を揃えることができない
  - スパースなデータでは、テストデータの相互作用が訓練データに現れてこない
  - 組み合わせが膨大になる
- FM に Filed (=カテゴリカル変数の塊) の概念を持ち込んだもの 

- 行列分解
  - 行列分解を使って、K次元のベクトルの内積で交互作用項のウェイトを表現
  - embedding や latent vector などともよばれる
  - 商品=A, 日付=1/1 と 商品=A, 性別=男 の組み合わせでは、商品 A の K 次元ベクトルが、日付と性別という異なる対象に対して共有される

** ={libFMexe}=    
*** 全関数 

#+begin_src R
pacman::p_funs(libFMexe)
#+end_src

#+RESULTS:
: [1] "cv_libFM"          "libFM"             "libFM_groups"     
: [4] "matrix_libFM"      "model_frame_libFM" "sp_matrix_libFM"

*** =libFM()=

#+begin_src R
libFM(
  train,
  test,
  global_bias = TRUE,
  variable_bias = TRUE,
  dim = 8,
  task = c("c", "r"),
  method = c("mcmc", "sgd", "als", "sgda"),
  init_stdev = 0.1,
  regular = c(0, 0, 0),
  learn_rate = 0.1,
  validation,
  verbosity = 0,
  iter = 100,
  exe_loc,
  grouping,
  seed = NULL,
  ...
)

## S3 method for class 'data.frame'
libFM(train, test, formula, validation, grouping, ...)

## S3 method for class 'matrix'
libFM(train, test, y_train, y_test, validation, y_validation,
  grouping, ...)

## S3 method for class 'dgCMatrix'
libFM(train, test, y_train, y_test, validation,
  y_validation, grouping, ...)

#+end_src

*** =cv_libFM()=

#+begin_src R
cv_libFM(x, ...)

## S3 method for class 'data.frame'
cv_libFM(
  x, # data.frame
  formula,
  validation,
  grouping, # logical scalar or integer vector
  cv_verbosity = 0,
  ... # liFM() への引数
)

## S3 method for class 'matrix'
cv_libFM(x, y, validation, y_validation, grouping,
  cv_verbosity = 0, ...)

## S3 method for class 'dgCMatrix'
cv_libFM(x, y, validation, y_validation, grouping,
  cv_verbosity = 0, ...)

## Default S3 method:
cv_libFM(
  x,
  dims = c(0, 8),
  init_stdevs = 0.1,
  folds = 5, # cv の fold 数
  validation,
  grouping,
  task = c("c", "r"),
  loss_function,
  cv_verbosity = 0,
  ...
)
#+end_src

#+RESULTS:
#+begin_example
Error in cv_libFM(x, ...) : could not find function "cv_libFM"

Error in cv_libFM(x, formula, validation, grouping, cv_verbosity = 0,  : 
  could not find function "cv_libFM"

Error in cv_libFM(x, y, validation, y_validation, grouping, cv_verbosity = 0,  : 
  could not find function "cv_libFM"

Error in cv_libFM(x, y, validation, y_validation, grouping, cv_verbosity = 0,  : 
  could not find function "cv_libFM"

Error in cv_libFM(x, dims = c(0, 8), init_stdevs = 0.1, folds = 5, validation,  : 
  could not find function "cv_libFM"
#+end_example

*** Example

#+begin_src R
library(libFMexe)
data(movie_lens)

set.seed(1)
train_rows = sample.int(nrow(movie_lens), nrow(movie_lens) * 2 / 3)
train = movie_lens[train_rows, ]
test  = movie_lens[-train_rows, ]

predFM = libFM(train, test, Rating ~ User + Movie,
               task = "r", dim = 10, iter = 500,
               exe_loc = "/usr/local/share/libFM/bin")

mean((predFM - test$Rating)^2)
#+end_src

#+RESULTS:
: 
: [1] 0.8123433

- ={glmnet}= のリッジ回帰と比較
- =libFM(dim=0)= とほぼ同じ意味合い
#+begin_src R
suppressPackageStartupMessages(library(glmnet))

spmat = sparse.model.matrix(Rating ~ User + Movie, data = movie_lens)
trainsp = spmat[train_rows, ]
testsp = spmat[-train_rows, ]

mod = cv.glmnet(x = trainsp, y = movie_lens$Rating[train_rows], alpha = 0)
predRR = predict(mod, testsp, s = "lambda.min")

mean((predRR - test$Rating)^2)
#+end_src

#+RESULTS:
: 
: [1] 0.8864323

- cv で最適な dim を探索
#+begin_src R
mses = cv_libFM(train, Rating ~ User + Movie,
                task = "r", dims = seq(0, 20, by = 5), iter = 500,
                exe_loc = "/usr/local/share/libFM/bin")
mses
#+end_src

* ガウス過程回帰

- ガウス過程
  - ベイズに基づく手法
    - データが十分あれば、信頼のある出力 (分散が小さい)
    - データが少なければ、分散が大きくなる
  - ガウス過程 = 線形モデルの無限次元への拡張
  - 入力と出力が無限次元のガウス分布
    - 出力が無限 = 関数
  - Package
    - =kernlab::gausspr()=
    - =GPfit::GP_fit()=

- [[https://tjo.hatenablog.com/entry/2019/03/15/190000][ガウス過程回帰・分類をRで試してみた]]

#+begin_src R
GP_fit(
  X, # matrix
  Y, # vector (simulator output)
  control = c(200 * d, 80 * d, 2 * d), # vector of parameters
  nug_thres = 20, # nugget を計算するためのパラメタ
  trace = FALSE, #
  maxit = 100, # iteration 最大数
  corr = list(type = "exponential", power = 1.95), # corr_matrix() 関数へのパラメタ
  optim_start = NULL
)
#+end_src

* 参考

- ポアソン回帰
  - [[https://stats.biopapyrus.jp/glm/poisson-regression.html][ポアソン回帰@biostatistics]]
  - [[https://stats.stackexchange.com/questions/71720/error-metrics-for-cross-validating-poisson-models][Error metrics for cross-validating Poisson models@CrossValidated]]

- ={glmnet}=
  - [[https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html][Glmnet Vignette]]
  - [[https://aizine.ai/ridge-lasso-elasticnet/][超入門！リッジ回帰・Lasso回帰・Elastic Netの基本と特徴をサクッと理解！@AIZINE]]
  - [[https://stats.biopapyrus.jp/sparse-modeling/glmnet.html][R の glmnet パッケージを利用した LASSO 推定と Elastic Net 推定@biostatistics]]
  - [[https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf][Glmnet Vignette@stanford]]
  - [[https://stats.stackexchange.com/questions/77546/how-to-interpret-glmnet/77549][How to interpret glmnet?@CrossValidated]]
  - [[https://stats.stackexchange.com/questions/304440/building-final-model-in-glmnet-after-cross-validation][Building final model in glmnet after cross validation@CrossValidated]]
  - [[https://stackoverflow.com/questions/23686067/default-lambda-sequence-in-glmnet-for-cross-validation][default lambda sequence in glmnet for cross-validation@CrossValidated]]
  - [[https://stats.stackexchange.com/questions/138569/why-is-lambda-within-one-standard-error-from-the-minimum-is-a-recommended-valu][Why is lambda “within one standard error from the minimum” is a recommended value for lambda in an elastic net regression?@CrossValidated]]

- FFM
  - [[http://hhok777.hatenablog.com/entry/2016/10/19/204037][（draft）FFM (Field-aware Factorization Machines)を理解したい]]
  - [[https://qiita.com/siero5335/items/8a8eb8de8633041d865a][libFMexeを動かすまで@Qiita]]
  - [[https://github.com/srendle/libfm][libfm@github]]
  - [[https://github.com/ycjuan/libffm][libffm@github]]
  - [[https://github.com/andland/libFMexe][libFMexe@github]]

- SVM
  - [[http://ryamada22.hatenablog.jp/entry/20180109/1515282861][kernlab パッケージ@ryamadaの遺伝学・遺伝統計学メモ]]
