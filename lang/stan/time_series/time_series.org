#+STARTUP: folded indent inlineimages latexpreview
#+PROPERTY: header-args:R :session *R:time_series* :width 640 :height 480 :results output

* ライブラリの読み込みと事前準備

#+begin_src R :results silent
library(tidyverse)
library(glue)
library(lubridate)

library(forecast)
library(fGarch)

library(rstan)
library(shinystan)
library(loo)
library(brms)
#+end_src

#+begin_src R :results silent
options(mc.cores = parallel::detectCores() - 1)
rstan_options(auto_write = TRUE) # stan ファイルと同名の rds として保存される
#+end_src

* Stan による時系列モデル
** AR(1) Model
*** データ

- AR(1) のシミュレーションデータ
#+begin_src R :results silent
ar1 <- arima.sim(n = 1000, model = list(order = c(1, 0, 0), ar = 0.5))
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(ar1)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-n61REI.png]]

*** モデル
**** Basic

_AR(1) モデル_

$y_t = \alpha + \beta y_{t-1} + \epsilon, \epsilon \sim N(0, \sigma)$

こちらの書き方の方が、Stan では記述しやすい

$y_t \sim Normal(\alpha + \beta y_{t-1}, \sigma)$

#+begin_src stan :file models/ar_1.stan
data {
  int<lower=0> T;
  real y[T];
}

parameters {
  real alpha;
  real beta;
  // 強制的に定常過程を推定する場合の制約
  // real<lower=-1, upper=1> beta;
  real<lower=0> sigma;
}

model {
  for (t in 2:T) {
    y[t] ~ normal(alpha + beta * y[t-1], sigma);
  }
}
#+end_src

#+RESULTS:
[[file:models/ar_1.stan]]

**** Vectorized

#+begin_src stan :file models/ar_1_vec.stan
data {
  int<lower=0> T;
  vector[T] y;
}

parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}

model {
  y[2:T] ~ normal(alpha + beta * y[1:(T-1)], sigma);
}
#+end_src

#+RESULTS:
[[file:models/ar_1_vec.stan]]

*** サンプリング実行

- このデータくらいだと、vectorize しても速度は変わらない
#+begin_src R
## ar1_fit <- stan(file = "models/ar_1.stan", data = list(T = length(ar1), y = ar1))
ar1_fit <- stan(file = "models/ar_1_vec.stan", data = list(T = length(ar1), y = ar1))
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'ar_1_vec' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000167 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.67 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ar_1_vec' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000208 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.08 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ar_1_vec' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000163 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.63 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ar_1_vec' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.000203 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.03 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.221437 seconds (Warm-up)
Chain 2:                0.23616 seconds (Sampling)
Chain 2:                0.457597 seconds (Total)
Chain 2: 
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.231599 seconds (Warm-up)
Chain 3:                0.394478 seconds (Sampling)
Chain 3:                0.626077 seconds (Total)
Chain 3: 
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.328499 seconds (Warm-up)
Chain 4:                0.332053 seconds (Sampling)
Chain 4:                0.660552 seconds (Total)
Chain 4: 
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.296878 seconds (Warm-up)
Chain 1:                0.447341 seconds (Sampling)
Chain 1:                0.744219 seconds (Total)
Chain 1:
#+end_example

*** 結果の確認
**** fit object

- AR 係数 0.5 なので、シミュレーションデータ通り

#+begin_src R
ar1_fit
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ar_1.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
alpha   -0.01    0.00 0.03   -0.08   -0.03   -0.01    0.01    0.05  4166    1
beta     0.49    0.00 0.03    0.44    0.48    0.49    0.51    0.55  3934    1
sigma    0.97    0.00 0.02    0.93    0.96    0.97    0.99    1.02  3780    1
lp__  -469.84    0.03 1.29 -473.16 -470.44 -469.49 -468.87 -468.36  2044    1

Samples were drawn using NUTS(diag_e) at Wed Nov 13 22:49:30 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R
get_posterior_mean(ar1_fit)
#+end_src

#+RESULTS:
:        mean-chain:1  mean-chain:2 mean-chain:3  mean-chain:4 mean-all chains
: alpha   -0.01229227   -0.01228584   -0.0136302   -0.01271205     -0.01273009
: beta     0.49548081    0.49471484    0.4948913    0.49476827      0.49496380
: sigma    0.97170088    0.97238199    0.9706657    0.97278024      0.97188221
: lp__  -469.88705713 -469.86119775 -469.7977479 -469.80177191   -469.83694367

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(ar1_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-1tx0ls.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_trace(ar1_fit, nrow = 3)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-pDPCMG.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(ar1_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-qaIz6o.png]]

*** Shiny Stan

#+begin_src R :results silent
launch_shinystan(ar1_fit)
#+end_src

** AR(p) Model
*** データ

- AR(2) に従うモデル
#+begin_src R :results silent
ar2 <- arima.sim(n = 1000, model = list(order = c(2, 0, 0), ar = c(0.5, -0.2)))
#+end_src

*** モデル

_AR(p) モデル_


$y_t = \alpha + \Sigma_{i=1}^p \beta_i y_{t-i} + \epsilon, \epsilon \sim N(0, \sigma)$


Stan 向けに書き直すと

$y_t \sim Normal(\mu, \sigma)$

$\mu = \alpha + \Sigma_{i=1}^p \beta_i y_{t-i}$

#+begin_src stan :file models/ar_p.stan
data {
  int<lower=0> T;
  int<lower=0> P;
  real y[T];
}

parameters {
  real alpha;
  real beta[P];
  real<lower=0> sigma;
}

model {
  for (t in (P+1):T) {
    real mu;
    mu = alpha;
    for (p in 1:P) {
      mu = mu + beta[p] * y[t-p];
    }
    y[t] ~ normal(mu, sigma);
  }
}
#+end_src

#+RESULTS:
[[file:models/ar_p.stan]]

*** サンプリング

- AR(2) を推定してみる
#+begin_src R
ar2_fit <- stan(file = "models/ar_p.stan", data = list(T = length(ar2), P = 2, y = ar2))
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'ar_p' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000201 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.01 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ar_p' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000196 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.96 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ar_p' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000283 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.83 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ar_p' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.00027 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.7 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.472854 seconds (Warm-up)
Chain 3:                0.487944 seconds (Sampling)
Chain 3:                0.960798 seconds (Total)
Chain 3: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.4551 seconds (Warm-up)
Chain 2:                0.513612 seconds (Sampling)
Chain 2:                0.968712 seconds (Total)
Chain 2: 
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.709853 seconds (Warm-up)
Chain 4:                0.597551 seconds (Sampling)
Chain 4:                1.3074 seconds (Total)
Chain 4: 
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.758781 seconds (Warm-up)
Chain 1:                0.623962 seconds (Sampling)
Chain 1:                1.38274 seconds (Total)
Chain 1: 
Warning message:
In readLines(file, warn = TRUE) :
  incomplete final line found on '/home/shun/Dropbox/repos/github/five-dots/notes/lang/stan/time_series/models/ar_p.stan'
#+end_example

*** 結果の確認
**** fit object

- AR1=0.5, AR2=-0.2 なので、シミュレーションデータ通り

#+begin_src R
ar2_fit
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: ar_p.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

           mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
alpha     -0.02    0.00 0.03   -0.09   -0.04   -0.02    0.00    0.04  3661    1
beta[1]    0.52    0.00 0.03    0.46    0.50    0.52    0.54    0.58  4138    1
beta[2]   -0.22    0.00 0.03   -0.28   -0.24   -0.22   -0.20   -0.16  3947    1
sigma      0.96    0.00 0.02    0.92    0.95    0.96    0.98    1.01  4049    1
lp__    -462.33    0.03 1.42 -465.81 -463.02 -462.02 -461.29 -460.60  1821    1

Samples were drawn using NUTS(diag_e) at Wed Nov 13 23:21:37 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R
get_posterior_mean(ar2_fit)
#+end_src

#+RESULTS:
:          mean-chain:1  mean-chain:2  mean-chain:3  mean-chain:4 mean-all chains
: alpha     -0.02326868   -0.02259838   -0.02328339   -0.02540334     -0.02363845
: beta[1]    0.52203868    0.52300815    0.52183754    0.52288914      0.52244338
: beta[2]   -0.22044615   -0.22257311   -0.22059710   -0.22044190     -0.22101457
: sigma      0.96460063    0.96513673    0.96415581    0.96537668      0.96481747
: lp__    -462.34232213 -462.32031441 -462.42669593 -462.22277194   -462.32802610

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(ar2_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-HheyoU.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file) :height 640
stan_trace(ar2_fit, nrow = 4)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Yr4YU4.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(ar2_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-QIRs6u.png]]

*** Shiny Stan

#+begin_src R :results silent
launch_shinystan(ar2_fit)
#+end_src

** ARCH(1) Model
*** データ

- ARCH(1) に従うデータ
#+begin_src R :results silent
arch_spec <- garchSpec(model = list(alpha = c(0.4), beta = 0))
arch1 <- garchSim(arch_spec, 1000)
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(arch1)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-uD04a8.png]]

*** モデル

_ARCH(m) モデルの一般系_

$y_t = \mu + u_t$

$u_t = \sqrt{h_t \epsilon_t}, \epsilon_t \sim N(0, 1)$

$h_t = \omega + \Sigma_{i=1}^m \alpha_i u_{t-i}$


- $\mu$ は ARMA などの条件付き期待値のモデル
- $u_t$ はノイズ (期待値モデルの残差)
- $h_t$ は条件付き期待値
- ノイズのスケールが時変のモデル


_Stan User Guide に記載されている ARCH(1) モデル_

$r_t = \mu + a_t$

$a_t = \sigma_t \epsilon_t$

$\epsilon \sim Normal(0, 1)$

$\sigma_t^2 = \alpha_0 + \alpha_1 a_{t-1}^2$

#+begin_src stan :file models/arch_1.stan
data {
  int<lower=0> T;
  real r[T];
}

parameters {
  real mu;
  // 条件付き分散が正であることを保証する制約
  real<lower=0> alpha0;
  // 条件付き分散が正であることを保証する制約 + 定常性を保証するための制約
  real<lower=0, upper=1> alpha1;
}

model {
  for (t in 2:T) {
    // 前期の r から mu を引くと、前期の条件付き分散になる
    r[t] ~ normal(mu, sqrt(alpha0 + alpha1 * pow(r[t-1] - mu, 2)));
  }
}

#+end_src

#+RESULTS:
[[file:models/arch_1.stan]]

*** サンプリング

- ARCH(1) を推定してみる
#+begin_src R
arch1_fit <- stan(file = "models/arch_1.stan", data = list(T = length(arch1), r = as.numeric(arch1)))
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'arch_1' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000387 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 3.87 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'arch_1' NOW (CHAIN 3).

SAMPLING FOR MODEL 'arch_1' NOW (CHAIN 1).
Chain 3: 
Chain 3: Gradient evaluation took 0.000277 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.77 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 1: 
Chain 1: Gradient evaluation took 0.000365 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.65 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'arch_1' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.000347 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 3.47 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: 
Chain 4:  Elapsed Time: 15.6005 seconds (Warm-up)
Chain 4:                15.1196 seconds (Sampling)
Chain 4:                30.7201 seconds (Total)
Chain 4: 
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 20.5589 seconds (Warm-up)
Chain 3:                14.3381 seconds (Sampling)
Chain 3:                34.897 seconds (Total)
Chain 3: 
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 23.4007 seconds (Warm-up)
Chain 1:                16.6856 seconds (Sampling)
Chain 1:                40.0863 seconds (Total)
Chain 1: 
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 30.4354 seconds (Warm-up)
Chain 2:                12.7901 seconds (Sampling)
Chain 2:                43.2255 seconds (Total)
Chain 2:
#+end_example

*** 結果の確認
**** fit object

- alpha0=0, alpha1=0.4 なので、概ねシミュレーションデータ通り

#+begin_src R
arch1_fit
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: arch_1.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

          mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
mu        0.00    0.00 0.00    0.00    0.00    0.00    0.00    0.00  3804    1
alpha0    0.00    0.00 0.00    0.00    0.00    0.00    0.00    0.00  1093    1
alpha1    0.48    0.00 0.06    0.36    0.44    0.48    0.52    0.61  1318    1
lp__   6178.43    0.04 1.26 6175.07 6177.87 6178.74 6179.34 6179.86  1048    1

Samples were drawn using NUTS(diag_e) at Wed Nov 13 23:55:15 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R
get_posterior_mean(arch1_fit)
#+end_src

#+RESULTS:
:        mean-chain:1 mean-chain:2 mean-chain:3 mean-chain:4 mean-all chains
: mu     4.422492e-05 4.427369e-05 4.623516e-05 4.310227e-05    4.445901e-05
: alpha0 9.651202e-07 9.673814e-07 9.657120e-07 9.601781e-07    9.645979e-07
: alpha1 4.771255e-01 4.801884e-01 4.746211e-01 4.814268e-01    4.783405e-01
: lp__   6.178534e+03 6.178386e+03 6.178461e+03 6.178340e+03    6.178430e+03

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(arch1_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Y6wWFh.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_trace(arch1_fit, nrow = 3)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-4QJvdn.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(arch1_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-YJekPQ.png]]

*** Shiny Stan

#+begin_src R :results silent
launch_shinystan(arch1_fit)
#+end_src

** GARCH(1,1) Model
*** データ

- GARCH(1, 1) に従うデータ
#+begin_src R :results silent
garch_spec <- garchSpec(model = list(alpha = 0.4, beta = 0.1))
garch11 <- garchSim(garch_spec, 1000)
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(garch11)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-1FHFfr.png]]

*** モデル

_GARCH(1, 1)_

$\sigma_t^2 = \alpha_0 + \alpha_1^2 a_{t-1} + \beta \sigma_{t-1}^2$

#+begin_src stan :file models/garch_1-1.stan
data {
  int<lower=0> T;
  real r[T];
  real<lower=0> sigma1;
}

parameters {
  real mu;
  real<lower=0> alpha0;
  real<lower=0, upper=1> alpha1;
  real<lower=0, upper=(1-alpha1)> beta1;
}

transformed parameters {
  real<lower=0> sigma[T];
  sigma[1] = sigma1;

  for (t in 2:T) {
    sigma[t] = sqrt(alpha0 + alpha1 * pow(r[t-1] - mu, 2) + beta1 * pow(sigma[t-1], 2));
  }
}

model {
  r ~ normal(mu, sigma);
}
#+end_src

#+RESULTS:
[[file:models/garch_1-1.stan]]

*** サンプリング

- GARCH(1, 1) を推定してみる
#+begin_src R
garch11_fit <- stan(file = "models/garch_1-1.stan",
                    data = list(T = length(garch11), 
                                r = as.numeric(garch11),
                                ## 初期値として標本標準偏差を利用
                                sigma1 = sd(garch11)),
                    iter = 3000, warmup = 500)
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'garch_1-1' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.000522 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 5.22 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'garch_1-1' NOW (CHAIN 2).
Chain 2: 

SAMPLING FOR MODEL 'garch_1-1' NOW (CHAIN 1).
Chain 2: Gradient evaluation took 0.000581 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.81 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: 
Chain 1: Gradient evaluation took 0.000527 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.27 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'garch_1-1' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000454 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 4.54 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 60.3604 seconds (Warm-up)
Chain 1:                40.3321 seconds (Sampling)
Chain 1:                100.692 seconds (Total)
Chain 1: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 69.13 seconds (Warm-up)
Chain 2:                34.9943 seconds (Sampling)
Chain 2:                104.124 seconds (Total)
Chain 2: 
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 61.8459 seconds (Warm-up)
Chain 4:                41.1183 seconds (Sampling)
Chain 4:                102.964 seconds (Total)
Chain 4: 
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 69.2799 seconds (Warm-up)
Chain 3:                42.403 seconds (Sampling)
Chain 3:                111.683 seconds (Total)
Chain 3:
#+end_example

*** 結果の確認
**** fit object

- alpha0=0, alpha1=0.4, beta=0.1 なので、概ねシミュレーションデータ通り
#+begin_src R
get_posterior_mean(garch11_fit) %>% head()
#+end_src

#+RESULTS:
:          mean-chain:1 mean-chain:2 mean-chain:3 mean-chain:4 mean-all chains
: mu       1.640800e-05 1.469060e-05 1.530533e-05 1.276106e-05    1.479125e-05
: alpha0   1.063486e-06 1.078572e-06 1.058168e-06 1.069906e-06    1.067533e-06
: alpha1   4.520519e-01 4.561498e-01 4.583035e-01 4.527352e-01    4.548101e-01
: beta1    1.257995e-01 1.205674e-01 1.265363e-01 1.230166e-01    1.239799e-01
: sigma[1] 1.583214e-03 1.583214e-03 1.583214e-03 1.583214e-03    1.583214e-03
: sigma[2] 1.206449e-03 1.207200e-03 1.205317e-03 1.205674e-03    1.206160e-03

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(garch11_fit)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-ohH2gF.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_trace(garch11_fit, pars=c("alpha0", "alpha1", "beta1"), nrow = 3, inc_warmup = TRUE)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-2tBwLA.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(garch11_fit, pars=c("alpha0", "alpha1", "beta1"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-HjAxGp.png]]

** MA(2) Model
*** データ

- MA(2) のシミュレーションデータ
#+begin_src R :results silent
ma2 <- arima.sim(n = 1000, model = list(order = c(0, 0, 2), ma = c(0.4, -0.2)))
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(ma2)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-RjpGDQ.png]]

*** モデル

_MA(q) モデル_

$y_t = \mu + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q} + \epsilon$

$\epsilon_t \sim Normal(0, \sigma)$

#+begin_src stan :file models/ma_2.stan
data {
  int<lower=3> T;
  vector[T] y;
}

parameters {
  real mu;
  real<lower=0> sigma;
  vector[2] theta;
}

transformed parameters {
  vector[T] epsilon;
  epsilon[1] = y[1] - mu;
  epsilon[2] = y[2] - mu - (theta[1] * epsilon[1]);

  // 観測値 - 平均 - MA 項を引いたものが当期の誤差項
  for (t in 3:T)
    epsilon[t] = (y[t] - mu - theta[1] * epsilon[t - 1] - theta[2] * epsilon[t - 2]);
}

model {
  // 事前分布にコーシー分布を指定
  mu    ~ cauchy(0, 2.5);
  theta ~ cauchy(0, 2.5);
  // σ は下限が設定されているので、半コーシー分布になる
  sigma ~ cauchy(0, 2.5);

  for (t in 3:T)
    y[t] ~ normal(mu
                  + theta[1] * epsilon[t - 1]
                  + theta[2] * epsilon[t - 2],
                  sigma);
}
#+end_src

#+RESULTS:
[[file:models/ma_2.stan]]

*** サンプリング

- MA(2) を推定してみる
#+begin_src R
ma2_fit <- stan(file = "models/ma_2.stan",
                data = list(T = length(ma2), 
                            y = as.numeric(ma2)))
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'ma_2' NOW (CHAIN 1).

SAMPLING FOR MODEL 'ma_2' NOW (CHAIN 2).
Chain 1: Rejecting initial value:
Chain 1:   Log probability evaluates to log(0), i.e. negative infinity.
Chain 1:   Stan can't start sampling from this initial value.
Chain 1: 
Chain 1: Gradient evaluation took 0.000926 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 9.26 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 
SAMPLING FOR MODEL 'ma_2' NOW (CHAIN 3).
2: 
Chain 2: Gradient evaluation took 0.000817 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 8.17 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.000903 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 9.03 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ma_2' NOW (CHAIN 4).
Chain 4: Rejecting initial value:
Chain 4:   Error evaluating the log probability at the initial value.
Chain 4: Exception: normal_lpdf: Location parameter is inf, but must be finite!  (in 'model78c9126e56d9_ma_2' at line 31)

Chain 4: 
Chain 4: Gradient evaluation took 0.000888 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 8.88 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 2.4224 seconds (Warm-up)
Chain 1:                2.79645 seconds (Sampling)
Chain 1:                5.21885 seconds (Total)
Chain 1: 
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 2.51093 seconds (Warm-up)
Chain 4:                2.8292 seconds (Sampling)
Chain 4:                5.34013 seconds (Total)
Chain 4: 
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 2.46612 seconds (Warm-up)
Chain 3:                2.67711 seconds (Sampling)
Chain 3:                5.14323 seconds (Total)
Chain 3: 
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 4.74703 seconds (Warm-up)
Chain 2:                2.14986 seconds (Sampling)
Chain 2:                6.89688 seconds (Total)
Chain 2:
#+end_example

*** 結果の確認
**** fit object

- theta1=0.4, theta2=-0.2 なので、概ねシミュレーションデータ通り
#+begin_src R
get_posterior_mean(ma2_fit) %>% head()
#+end_src

#+RESULTS:
:            mean-chain:1 mean-chain:2 mean-chain:3 mean-chain:4 mean-all chains
: mu           -0.9320681   0.02053506   0.02182659   -0.9709893      -0.4651739
: sigma         0.3462466   1.02518664   1.02433258    6.3678958       2.1909154
: theta[1]     -1.3302193   0.37852507   0.37567124   -0.4103644      -0.2465968
: theta[2]      1.0245575  -0.21694353  -0.21805414    1.0125661       0.4005315
: epsilon[1]    0.2227271  -0.72987609  -0.73116761    0.2616483      -0.2441671
: epsilon[2]   -0.3226092  -1.29517523  -1.29806731   -0.4725927      -0.8471111

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(ma2_fit, pars = c("theta[1]", "theta[2]"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-cVpDIA.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_trace(ma2_fit, pars=c("theta[1]", "theta[2]"), nrow = 2, inc_warmup = TRUE)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-pSGIDV.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(ma2_fit, pars=c("theta[1]", "theta[2]"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-rsmBpv.png]]

** MA(q) Model
*** モデル
 
#+begin_src stan :file models/ma_q.stan
data {
  int<lower=0> Q;
  int<lower=3> T;
  vector[T] y;
}

parameters {
  real mu;
  real<lower=0> sigma;
  vector[Q] theta;
}

transformed parameters {
  vector[T] epsilon;
  for (t in 1:T) {
    epsilon[t] = y[t] - mu;
    for (q in 1:min(t - 1, Q))
      epsilon[t] = epsilon[t] - (theta[q] * epsilon[t - q]);
  }
}

model {
  vector[T] eta;
  mu ~ cauchy(0, 2.5);
  theta ~ cauchy(0, 2.5);
  sigma ~ cauchy(0, 2.5);

  for (t in 1:T) {
    eta[t] = mu;
    for (q in 1:min(t - 1, Q))
      eta[t] = eta[t] + theta[q] * epsilon[t - q];
  }
  y ~ normal(eta, sigma);
}
#+end_src

#+RESULTS:
[[file:models/ma_q.stan]]

*** サンプリング

- 同じ MA(2) のデータを推定してみる
#+begin_src R
ma_q_fit <- stan(file = "models/ma_q.stan",
                 data = list(Q = 2,
                             T = length(ma2), 
                             y = ma2))
#+end_src

#+RESULTS:
#+begin_example


SAMPLING FOR MODEL 'ma_q' NOW (CHAIN 1).

SAMPLING FOR MODEL 'ma_q' NOW (CHAIN 2).
Chain 1: Rejecting initial value:
Chain 1:   Log probability evaluates to log(0), i.e. negative infinity.
Chain 1:   Stan can't start sampling from this initial value.
Chain 1: 
Chain 1: Gradient evaluation took 0.000827 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.27 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 

SAMPLING FOR MODEL 'ma_q' NOW (CHAIN 4).
Chain 2: 
Chain 2: Gradient evaluation took 0.000867 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 8.67 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'ma_q' NOW (CHAIN 3).
Chain 4: Rejecting initial value:
Chain 4:   Log probability evaluates to log(0), i.e. negative infinity.
Chain 4:   Stan can't start sampling from this initial value.
Chain 4: 
Chain 4: Gradient evaluation took 0.000925 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 9.25 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.000739 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 7.39 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 1.96963 seconds (Warm-up)
Chain 3:                1.8742 seconds (Sampling)
Chain 3:                3.84383 seconds (Total)
Chain 3: 
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 2.08686 seconds (Warm-up)
Chain 1:                1.96831 seconds (Sampling)
Chain 1:                4.05517 seconds (Total)
Chain 1: 
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 1.95994 seconds (Warm-up)
Chain 4:                1.97899 seconds (Sampling)
Chain 4:                3.93893 seconds (Total)
Chain 4: 
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 3.7558 seconds (Warm-up)
Chain 2:                1.80827 seconds (Sampling)
Chain 2:                5.56407 seconds (Total)
Chain 2:
#+end_example

*** 結果の確認
**** fit object

- theta1=0.4, theta2=-0.2 なので、概ねシミュレーションデータ通り
#+begin_src R
get_posterior_mean(ma_q_fit) %>% head()
#+end_src

#+RESULTS:
:            mean-chain:1 mean-chain:2 mean-chain:3 mean-chain:4 mean-all chains
: mu           0.02207814   0.01816935   0.02106709   0.02204582       0.0208401
: sigma        1.02488148   1.02587768   1.02506716   1.02503914       1.0252164
: theta[1]     0.37558107   0.37389999   0.37439689   0.37423963       0.3745294
: theta[2]    -0.21794270  -0.21745970  -0.21861493  -0.21734296      -0.2178401
: epsilon[1]  -0.73141917  -0.72751037  -0.73040811  -0.73138684      -0.7301811
: epsilon[2]  -1.29831537  -1.29710549  -1.29852049  -1.29924917      -1.2982976

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(ma_q_fit, pars = c("theta[1]", "theta[2]"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Urbcax.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_trace(ma_q_fit, pars=c("theta[1]", "theta[2]"), nrow = 2, inc_warmup = TRUE)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-cm1aOu.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(ma_q_fit, pars=c("theta[1]", "theta[2]"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-AXGv9A.png]]

** ARMA(1,1)
*** データ

- ARMA(1, 1) のシミュレーションデータ
#+begin_src R :results silent
arma11 <- arima.sim(n = 1000, model = list(order = c(1, 0, 1), ar = 0.4, ma = 0.2))
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(arma11)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Q88ltM.png]]

*** モデル

$y_t = \mu + \phi y_{t-1} + \theta \epsilon_{t-1} + \epsilon_t$

$\epsilon \sim Normal(0, \sigma)$

#+begin_src stan :file models/arma_1-1.stan
data {
  int<lower=1> T;
  real y[T];
}

parameters {
  real mu;
  real<lower=-1, upper=1> phi;
  real<lower=-1, upper=1> theta;
  real<lower=0> sigma;
}

model {
  // 時点 t での予測値
  vector[T] nu;
  // 時点 t での誤差 (あとで残差チェックに使える)
  vector[T] err;
  
  // err[0] == 0と仮定
  nu[1] = mu + (phi * mu);
  err[1] = y[1] - nu[1];

  for (t in 2:T) {
    nu[t] = mu + (phi * y[t-1]) + (theta * err[t-1]);
    err[t] = y[t] - nu[t];
  }

  mu    ~ normal(0, 10);
  phi   ~ normal(0, 2);
  theta ~ normal(0, 2);
  sigma ~ cauchy(0, 5);
  err   ~ normal(0, sigma);    // 尤度
}

// 局所変数のベクトルを利用しない例
// model {
//   real err;
  
//   mu    ~ normal(0, 10);
//   phi   ~ normal(0, 2);
//   theta ~ normal(0, 2);
//   sigma ~ cauchy(0, 5);

//   err = y[1] - mu + (phi * mu);
//   err ~ normal(0, sigma);

//   for (t in 2:T) {
//     err = y[t] - (mu + phi * y[t-1] + theta * err);
//     err ~ normal(0, sigma);
//   }
// }
#+end_src

#+RESULTS:
[[file:models/arma_1-1.stan]]

*** サンプリング

- 同じ MA(2) のデータを推定してみる
#+begin_src R
arma_11_fit <- stan(file = "models/arma_1-1.stan",
                    data = list(T = length(arma11), 
                                y = arma11),
                    init = c(mu = 0, phi = 0, theta = 0, sigma = 0))
#+end_src

#+RESULTS:
#+begin_example
hash mismatch so recompiling; make sure Stan code ends with a blank line

SAMPLING FOR MODEL 'arma_1-1' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.00055 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.5 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'arma_1-1' NOW (CHAIN 2).

SAMPLING FOR MODEL 'arma_1-1' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000532 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 5.32 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: 
Chain 2: Gradient evaluation took 0.000558 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.58 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'arma_1-1' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.000668 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 6.68 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 1.66212 seconds (Warm-up)
Chain 2:                1.64508 seconds (Sampling)
Chain 2:                3.3072 seconds (Total)
Chain 2: 
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 1.57612 seconds (Warm-up)
Chain 1:                1.67499 seconds (Sampling)
Chain 1:                3.25111 seconds (Total)
Chain 1: 
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 1.63911 seconds (Warm-up)
Chain 4:                1.70669 seconds (Sampling)
Chain 4:                3.34579 seconds (Total)
Chain 4: 
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 1.6325 seconds (Warm-up)
Chain 3:                1.94105 seconds (Sampling)
Chain 3:                3.57355 seconds (Total)
Chain 3: 
Warning message:
In readLines(file, warn = TRUE) :
  incomplete final line found on '/home/shun/Dropbox/repos/github/five-dots/notes/lang/stan/time_series/models/arma_1-1.stan'
#+end_example

*** 結果の確認
**** fit object

- phi=0.4, theta=0.2 のシミュレーションデータ
#+begin_src R
get_posterior_mean(arma_11_fit) %>% head()
#+end_src

#+RESULTS:
:        mean-chain:1  mean-chain:2  mean-chain:3 mean-chain:4 mean-all chains
: mu      -0.05087493   -0.05173105   -0.04809092   -0.0497944     -0.05012282
: phi      0.42746426    0.42511304    0.42724025    0.4275963      0.42685346
: theta    0.15501841    0.15790889    0.15532634    0.1548704      0.15578101
: sigma    1.00049832    1.00164468    1.00293623    1.0012285      1.00157693
: lp__  -502.55587986 -502.60339896 -502.59011170 -502.4664546   -502.55396129

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(arma_11_fit, pars = c("phi", "theta"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-9D6iv5.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_trace(arma_11_fit, pars=c("phi", "theta"), nrow = 2, inc_warmup = TRUE)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-0bw0UQ.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(ma_q_fit, pars=c("theta[1]", "theta[2]"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Pz15ke.png]]

** ARMA(p, q)
*** データ

- ARMA(2, 2) のシミュレーションデータ
#+begin_src R :results silent
set.seed(1983)
arma22 <- arima.sim(n = 1000,
                    model = list(order = c(2, 0, 2), ar = c(0.4, 0.1), ma = c(0.3, -0.1)))
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(arma22)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-VlDjP6.png]]

*** モデル

_ARMA(p, q) Model_

$y_t = \mu + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t$

$\epsilon \sim N(0, \sigma)$

#+begin_src stan :file models/arma_p-q.stan
data {
	int<lower=1> T;
	int<lower=0> P;
	int<lower=0> Q;
	vector[T] y;
	int<lower=0> T_forecast;
}

parameters {
	real mu;
	vector[P] phi;
	vector<lower=-1, upper=1>[Q] theta;
	real<lower=0> sigma;
}

transformed parameters {
  // error term
  vector[T] eps;
  eps[1] = y[1] - mu;

  for (t in 2:T) {
    eps[t] = y[t] - mu;

    for(p in 1:min(t-1, P))
      eps[t] -= phi[p] * y[t-p];

    for(q in 1:min(t-1, Q))
      eps[t] -= theta[q] * eps[t-q];
  }
}

model {
  vector[T] eta;

  // priors
	mu ~ normal(0, 5);
  phi ~ normal(0, 2);
  theta ~ normal(0, 2);
  sigma ~ cauchy(0, 5); // TODO half-t or half-norm

	// log-likelihood
	for (t in 1:T) {
	  eta[t] = mu;

    // AR terms
	  for (p in 1:min(t-1, P))
	    eta[t] += phi[p] * y[t-p];

    // MA terms
	  for (q in 1:min(t-1, Q))
	    eta[t] += theta[q] * eps[t-q];

	  y[t] ~ normal(eta[t], sigma);
	}
}

generated quantities {
  vector[T+T_forecast] y_pred;
  vector[T+T_forecast] eps_pred;
  
  vector[T] log_lik;
  vector[T] eta;

  // prediction
  eps_pred[1:T] = eps;
  y_pred[1:T] = y;

  for (t in (T+1):(T+T_forecast)) {
    eps_pred[t] = normal_rng(0, sigma);
    y_pred[t] = mu + eps_pred[t];

    // AR terms
    for (p in 1:P)
	    y_pred[t] += phi[p] * y_pred[t-p];

    // MA terms
	  for (q in 1:Q)
	    y_pred[t] += theta[q] * eps_pred[t-q];
  }

  // log-likelihood for WAIC calculation by {loo}
  for (t in 1:T) {
    eta[t] = mu;

    // AR terms
	  for (p in 1:min(t-1, P))
	    eta[t] += phi[p] * y[t-p];

    // MA terms
	  for (q in 1:min(t-1, Q))
	    eta[t] += theta[q] * eps[t-q];

    log_lik[t] = normal_lpdf(y[t] | eta[t], sigma);
  }
}
#+end_src

#+RESULTS:
[[file:models/arma_p-q.stan]]

*** TODO 収束対策・パラメタ制約・事前分布
*** AR の定常性・ MA の反転可能性

- [[https://github.com/stan-dev/math/issues/309][Functions to ensure stationarity in autoregression and invertibility in moving average parameters #309@stan-dev]]

#+begin_src stan :file models/arma_funs.stan
functions {
  /*
    Partial Autocorrelations to Autocorrelations

    @param vector of coefficients of a partial autocorrelation function
    @return vector of coefficients of an Autocorrelation function

  */
  row_vector pacf_to_acf(vector x) {
    int n = num_elements(x);
    matrix[n, n] y = diag_matrix(x);

    for (k in 2:n) {
      for (i in 1:(k - 1)) {
        y[k, i] = y[k - 1, i] - x[k] * y[k - 1, k - i];
      }
    }
    return y[n];
  }

  /*
    Constrain vector of coefficients to the stationary and intertible region for AR or MA functions.

    @param vector x An unconstrained vector in (-Inf, Inf)
    @returns vector y A vector of coefficients for a stationary AR or inverible MA process.

  */
  row_vector constrain_stationary(vector x) {
    vector[num_elements(x)] r;
    int n = num_elements(x);

    // transform (-Inf, Inf) to (-1, 1)
    for (i in 1:n) {
      r[i] = x[i] / (sqrt(1.0 + pow(x[i], 2)));
    }
    // Transform PACF to ACF
    return pacf_to_acf(r);
  }

  /*
  Convert coefficients of an autocorrelation function to partial autocorrelations.

  @param vector x Coeffcients of an autocorrelation function.
  @returns vector y Coefficients of the corresponding partial autocorrelation function.

  */
  vector acf_to_pacf(vector x) {
    matrix[num_elements(x), num_elements(x)] y;
    vector[num_elements(x)] r;
    int n;
    n = num_elements(x);
    y = rep_matrix(0.0, n, n);
    y[n] = -x ';
    for (j in 0:(n - 1)) {
      int k;
      k = n - j;
      for (i in 1:(k - 1)) {
        y[k - 1, i] = (y[k, i] - y[k, k] * y[k, k - i]) / (1 - pow(y[k, k], 2));
      }
    }
    r = diagonal(y);
    return r;
  }

  /*
  Transform from stationary and invertible space to (-Inf, Inf)

  @param vector x Coeffcients of an autocorrelation function.
  @returns vector y Coefficients of the corresponding partial autocorrelation function.

  */
  vector unconstrain_stationary(vector x) {
    matrix[num_elements(x), num_elements(x)] y;
    vector[num_elements(x)] r;
    vector[num_elements(x)] z;
    int n;
    n = num_elements(x);
    // Transform ACF to PACF
    r = acf_to_pacf(x);
    // Transform (-1, 1) to (-Inf, Inf)
    for (i in 1:n) {
      z[i] = r[i] / (sqrt(1.0 - pow(r[i], 2)));
    }
    return z;
  }
}
#+end_src

#+RESULTS:
[[file:models/arma_funs.stan]]

*** サンプリング

- ARMA(2, 2) のデータを推定してみる

- エラー対応
  - There were 5 divergent transitions after warmup.
    Increasing adapt_delta above 0.8 may help. See
    http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
    Examine the pairs() plot to diagnose sampling problems

  - The largest R-hat is 1.09, indicating chains have not mixed.
    Running the chains for more iterations may help. See
    http://mc-stan.org/misc/warnings.html#r-hat

  - Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
    Running the chains for more iterations may help. See
    http://mc-stan.org/misc/warnings.html#bulk-ess 
  
  - Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
    Running the chains for more iterations may help. See
    http://mc-stan.org/misc/warnings.html#tail-ess 

#+begin_src R
arma_22_fit <- stan(
  file = "models/arma_p-q.stan",
  data = list(P = 2, Q = 2, T = length(arma22), y = arma22, T_forecast = 10),
  init = c(mu = 0, phi = 0, theta = 0, sigma = 0),
  ## warmup = 500, iter = 3000
  ## control = list(adapt_delta = 0.99),
  seed = 1234)
#+end_src

#+RESULTS:
#+begin_example
hash mismatch so recompiling; make sure Stan code ends with a blank line

SAMPLING FOR MODEL 'arma_p-q' NOW (CHAIN 1).

SAMPLING FOR MODEL 'arma_p-q' NOW (CHAIN 2).

SAMPLING FOR MODEL 'arma_p-q' NOW (CHAIN 3).
Chain 1: 
Chain 1: Gradient evaluation took 0.001131 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 11.31 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'arma_p-q' NOW (CHAIN 4).
Chain 2: 
Chain 2: Gradient evaluation took 0.000857 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 8.57 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.000786 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 7.86 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.001044 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 10.44 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 16.7943 seconds (Warm-up)
Chain 3:                15.4723 seconds (Sampling)
Chain 3:                32.2665 seconds (Total)
Chain 3: 
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 15.1849 seconds (Warm-up)
Chain 2:                17.3768 seconds (Sampling)
Chain 2:                32.5617 seconds (Total)
Chain 2: 
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 16.2068 seconds (Warm-up)
Chain 1:                21.0358 seconds (Sampling)
Chain 1:                37.2426 seconds (Total)
Chain 1: 
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 16.0809 seconds (Warm-up)
Chain 4:                24.2027 seconds (Sampling)
Chain 4:                40.2837 seconds (Total)
Chain 4: 
Warning messages:
1: In readLines(file, warn = TRUE) :
  incomplete final line found on '/home/shun/Dropbox/repos/github/five-dots/notes/lang/stan/time_series/models/arma_p-q.stan'
2: There were 31 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
3: Examine the pairs() plot to diagnose sampling problems
 
4: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess
#+end_example

*** 結果の確認
**** fit object

- phi[1]=0.4, phi[2]=0.1, theta[1]=0.3, theta[2]=-0.1 のシミュレーションデータ
#+begin_src R
print(arma_22_fit, pars = c("mu", "phi", "theta", "sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: arma_p-q.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

         mean se_mean   sd  2.5%   25%  50%  75% 97.5% n_eff Rhat
mu       0.06    0.00 0.05 -0.04  0.02 0.05 0.09  0.18   803 1.00
phi[1]   0.25    0.03 0.42 -0.50 -0.07 0.24 0.53  1.09   142 1.03
phi[2]   0.07    0.01 0.20 -0.32 -0.08 0.07 0.22  0.40   186 1.03
theta[1] 0.49    0.04 0.42 -0.34  0.20 0.50 0.80  1.27   139 1.03
theta[2] 0.08    0.01 0.12 -0.18  0.00 0.09 0.16  0.30   126 1.04
sigma    0.99    0.00 0.02  0.94  0.97 0.99 1.00  1.03   310 1.02

Samples were drawn using NUTS(diag_e) at Fri Nov 15 15:03:43 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

#+begin_src R
get_posterior_mean(arma_22_fit) %>% head()
#+end_src

#+RESULTS:
:          mean-chain:1 mean-chain:2 mean-chain:3 mean-chain:4 mean-all chains
: mu        -0.05823425  -0.06081557  -0.05669686  -0.05842023     -0.05854173
: phi[1]    -0.61470149  -0.59944260  -0.59208190  -0.60978138     -0.60400184
: phi[2]     0.04162722   0.05173815   0.05898878   0.04280198      0.04878903
: theta[1]   0.39962978   0.38470488   0.37851441   0.39450566      0.38933868
: theta[2]   0.09714038   0.08846080   0.08647682   0.09804781      0.09253145
: sigma      1.03715091   1.03767539   1.03759311   1.03766159      1.03752025

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(arma_22_fit, pars = c("mu", "phi", "theta", "sigma"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-Mynbpw.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file) :height 640
stan_trace(arma_22_fit, nrow = 4, inc_warmup = TRUE, pars = c("mu", "phi", "theta", "sigma"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-A7Pkka.png]]

**** stan_hist()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_hist(arma_22_fit, pars = c("mu", "phi", "theta", "sigma"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-VePHuj.png]]

*** WAIC

#+begin_src R
arma_22_llk <- extract_log_lik(arma_22_fit)
waic <- waic(arma_22_llk)
waic$estimates["waic", "Estimate"]
#+end_src

#+RESULTS:
: [1] 2815.958

- =Forecast::Arima()= での AIC と比較してみる
#+begin_src R
Arima(arma22, order = c(2,0,2)) %>% AIC
#+end_src

#+RESULTS:
: [1] 2817.905

*** WAIC による次数選択

- P=3, Q=3 までの 9 通りのサンプリング
#+begin_src R :results silent
arma_orders <- cross(list(P = c(1, 2, 3), Q = c(1, 2, 3)))
arma_fits <- map(arma_orders, function(order) {
  stan(
    file = "models/arma_p-q.stan",
    data = list(P = order$P, Q = order$Q, T = length(arma22), y = arma22, T_forecast = 10),
    init = c(mu = 0, phi = 0, theta = 0, sigma = 0),
    seed = 1234)
})
#+end_src

- 全てについて WAIC を計算
- ARMA(1,1) が選択された
#+begin_src R :results value :colnames yes
arma_llks <- map2_dfr(arma_orders, arma_fits, function(order, fit) {
  llk <- extract_log_lik(fit)
  waic <- waic(llk)
  data.frame(p = order$P, q = order$Q, waic = waic$estimates["waic", "Estimate"])
})
arma_llks %>% arrange(waic)
#+end_src

#+RESULTS:
| p | q |             waic |
|---+---+------------------|
| 1 | 1 | 2813.68783337513 |
| 3 | 1 | 2814.84588750635 |
| 2 | 1 | 2815.40639900447 |
| 1 | 2 | 2815.51976758479 |
| 1 | 3 | 2815.71014210666 |
| 2 | 2 | 2815.95823552817 |
| 3 | 3 | 2816.08310844953 |
| 2 | 3 | 2816.17007338453 |
| 3 | 2 | 2816.46486329802 |

** ARMA(p, q) by ={brms}=
*** brms fit

- ={brms}= パッケージを利用した場合
#+begin_src R :results silent
arma_22_brm <- brm(Y ~ 1, data = data.frame(Y = arma22), autocor = cor_arma(~1, p = 2, q = 2))
#+end_src

- ar[1]=0.4, ar[2]=0.1, ma[1]=0.3, ma[2]=-0.1 のシミュレーションデータ
#+begin_src R
arma_22_brm
#+end_src

#+RESULTS:
#+begin_example
 Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: Y ~ 1 
   Data: data.frame(Y = arma22) (Number of observations: 1000) 
Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup samples = 4000

Correlation Structures:
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
ar[1]     0.44      0.52    -0.44     1.36 1.09       36      102
ar[2]    -0.02      0.25    -0.44     0.40 1.08       41      117
ma[1]     0.30      0.52    -0.63     1.16 1.09       35       68
ma[2]     0.02      0.15    -0.26     0.26 1.07       42       72

Population-Level Effects: 
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept     0.08      0.07    -0.06     0.22 1.01      935      442

Family Specific Parameters: 
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     0.99      0.02     0.95     1.03 1.01      628     1617

Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
is a crude measure of effective sample size, and Rhat is the potential 
scale reduction factor on split chains (at convergence, Rhat = 1).
Warning messages:
1: The model has not converged (some Rhats are
1.1). Do not analyse the results! 
We recommend running more iterations and/or setting stronger priors. 
2: There were 530 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help.
See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
#+end_example

*** stan code by =make_stancode()=

#+begin_src stan
// generated with brms 2.10.0
functions {
}

data {
  int<lower=1> N; // number of observations
  vector[N] Y;    // response variable

  // data needed for ARMA correlations
  int<lower=0> Kar;  // AR order
  int<lower=0> Kma;  // MA order

  // number of lags per observation
  int<lower=0> J_lag[N];
  int prior_only;  // should the likelihood be ignored?
}

transformed data {
  int max_lag = max(Kar, Kma);
}

parameters {
  // temporary intercept for centered predictors
  real Intercept;
  real<lower=0> sigma; // residual SD
  vector[Kar] ar;      // autoregressive effects
  vector[Kma] ma;      // moving-average effects
}

transformed parameters {
}

model {
  // initialize linear predictor term
  vector[N] mu = Intercept + rep_vector(0, N);

  // objects storing residuals
  matrix[N, max_lag] Err = rep_matrix(0, N, max_lag);
  vector[N] err;

  // include ARMA terms
  for (n in 1:N) {
    mu[n] += head(Err[n], Kma) * ma;
    err[n] = Y[n] - mu[n];
    for (i in 1:J_lag[n]) {
      Err[n + 1, i] = err[n + 1 - i];
    }
    mu[n] += head(Err[n], Kar) * ar;
  }

  // priors including all constants
  target += student_t_lpdf(Intercept | 3, 0, 10);
  target += student_t_lpdf(sigma | 3, 0, 10) - 1 * student_t_lccdf(0 | 3, 0, 10);

  // likelihood including all constants
  if (!prior_only) {
    target += normal_lpdf(Y | mu, sigma);
  }
}

generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept;
}
#+end_src

*** =get_prior()=

#+begin_src R :results value :colnames yes
brms::get_prior(Y ~ 1, data = data.frame(Y = arma22), autocor = cor_arma(~1, p = 2, q = 2))
#+end_src

#+RESULTS:
| prior               | class     | coef | group | resp | dpar | nlpar | bound              |
|---------------------+-----------+------+-------+------+------+-------+--------------------|
|                     | ar        |      |       |      |      |       | <lower=-1,upper=1> |
| student_t(3, 0, 10) | Intercept |      |       |      |      |       |                    |
|                     | ma        |      |       |      |      |       | <lower=-1,upper=1> |
| student_t(3, 0, 10) | sigma     |      |       |      |      |       |                    |

*** WAIC & LOO

#+begin_src R
brms::waic(arma_22_brm)
#+end_src

#+RESULTS:
: 
: Computed from 4000 by 1000 log-likelihood matrix
: 
:           Estimate   SE
: elpd_waic  -1408.0 22.2
: p_waic         5.6  0.3
: waic        2815.9 44.3

#+begin_src R
brms::loo(arma_22_brm)
#+end_src

#+RESULTS:
#+begin_example

Computed from 4000 by 1000 log-likelihood matrix

         Estimate   SE
elpd_loo  -1408.0 22.2
p_loo         5.6  0.3
looic      2816.0 44.3
------
Monte Carlo SE of elpd_loo is 0.1.

All Pareto k estimates are good (k < 0.5).
See help('pareto-k-diagnostic') for details.
#+end_example

*** =plot()=

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(arma_22_brm, pars = c("ar", "ma"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-9u86Uw.png]]

** ARMA(p, q) + GARCH(r, m) Model
*** モデル

_ARMA(p, q) + GARCH(r, m) Model_
  - 参考: [[https://stats.stackexchange.com/questions/41509/what-is-the-difference-between-garch-and-arma][What is the difference between GARCH and ARMA?@Stackoverflow]]

$y_t \sim Normal(\mu_t, \sigma_t^2)$

$\mu_t = \phi_1 \mu_{y-1} + \dots + \phi_p \mu_{y-p} + (\phi1 + \theta_1) u_{t-1} + \dots + (\phi_m + \theta_m) u_{t-m}$

$\sigma_t^2 = \omega + \alpha_1 u_{t-1}^2 + \dots + \alpha_s u_{t-s}^2 + \dots + \beta_1 \sigma_{t-1}^2 + \dots + \beta_r \sigma_{t-r}^2$

$\frac{u_t}{\sigma_t} \sim Normal(0, 1)$

Where
$u_t := y_t - \mu_t$
$\phi_i = 0$ for $i > p$
$\theta_j = 0$ for $j > q$

--------
別の書き方?

$y_t = \mu_t + u_t$

$\mu_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \theta_1 \epsilon_{t-1} + \dots + \theta_q \epsilon_{t-q}$

$u_t = \sigma_t \epsilon_t$

$\sigma_t^2 = \omega + \alpha_1^2 u_{t-1} + \dots + \alpha_r u_{t-r} + \beta_1 \sigma_{t-1}^2 + \dots + \beta_m \sigma_{t-m}$

$\epsilon \sim N(0, 1)$

#+begin_src stan :file models/arma_p-q.stan
data {
	int<lower=1> T;
	int<lower=0> P;
	int<lower=0> Q;
	vector[T] y;
	int<lower=0> T_forecast;
}

parameters {
	real mu;
	vector[P] phi;
	vector[Q] theta;
	real<lower=0> sigma;
}

transformed parameters {
  // error term
  vector[T] eps;
  eps[1] = y[1] - mu;

  for (t in 2:T) {
    eps[t] = y[t] - mu;

    for(p in 1:min(t-1, P))
      eps[t] -= phi[p] * y[t-p];

    for(q in 1:min(t-1, Q))
      eps[t] -= theta[q] * eps[t-q];
  }
}

model {
  vector[T] eta;

  // priors
	mu ~ normal(0, 5);
  phi ~ normal(0, 1);
  theta ~ normal(0, 1);
  sigma ~ cauchy(0, 5); // TODO half-t or half-norm

	// log-likelihood
	for (t in 1:T) {
	  eta[t] = mu;

    // AR terms
	  for (p in 1:min(t-1, P))
	    eta[t] += phi[p] * y[t-p];

    // MA terms
	  for (q in 1:min(t-1, Q))
	    eta[t] += theta[q] * eps[t-q];

	  y[t] ~ normal(eta[t], sigma);
	}
}

generated quantities {
  vector[T+T_forecast] y_pred;
  vector[T+T_forecast] eps_pred;
  
  vector[T] log_lik;
  vector[T] eta;

  // prediction
  eps_pred[1:T] = eps;
  y_pred[1:T] = y;

  for (t in (T+1):(T+T_forecast)) {
    eps_pred[t] = normal_rng(0, sigma);
    y_pred[t] = mu + eps_pred[t];

    // AR terms
    for (p in 1:P)
	    y_pred[t] += phi[p] * y_pred[t-p];

    // MA terms
	  for (q in 1:Q)
	    y_pred[t] += theta[q] * eps_pred[t-q];
  }

  // log-likelihood for WAIC calculation by {loo}
  for (t in 1:T) {
    eta[t] = mu;

    // AR terms
	  for (p in 1:min(t-1, P))
	    eta[t] += phi[p] * y[t-p];

    // MA terms
	  for (q in 1:min(t-1, Q))
	    eta[t] += theta[q] * eps[t-q];

    log_lik[t] = normal_lpdf(y[t] | eta[t], sigma);
  }
}
#+end_src

#+RESULTS:
[[file:models/arma_p-q.stan]]

** ARMA(p, q) + eGARCH(r, m) Model
** VARMA(1, 1)
*** プロット用の関数

#+begin_src R :results silent
# plot 80 % & 90 % predition interval of y
plot.stan.pred <- function(data,stanout, Time, Time.forecast, N, span=NULL){
  if (is.null(span) ) span <- 1:(Time + Time.forecast) 
  y_pred <- data.frame()
  for ( i in 1:N){
    m.pred <- rstan::extract(stanout, "y_pred")$y_pred[,,i]
    # 90 % pred interval
    temp <- data.frame(t = Time+1:Time.forecast,
                       series = i,
                       y90_l  = apply(m.pred, 2, quantile, probs=.05),
                       y90_u = apply(m.pred, 2, quantile, probs=.95),
                       y80_l  = apply(m.pred, 2, quantile, probs=.1),
                       y80_u = apply(m.pred, 2, quantile, probs=.9),
                       y_med  = apply(m.pred, 2, median),
                       y_mean = apply(m.pred, 2, mean, rm.na=T)
    )
    y_pred <- rbind(y_pred, temp)
  }
  temp <- data.frame(t=1:Time, data)
  colnames(temp)[1+1:N] <- seq(1:N)
  y_pred <- temp %>% gather(key=series, value=y_mean, -t) %>% mutate(series=as.integer(series)) %>% bind_rows(y_pred)
  y_pred <- y_pred %>% filter(t %in% span)
  ggplot(y_pred) + geom_line(aes(x=t, y=y_mean)) + geom_line(aes(x=t, y_med), linetype=2) +
    geom_ribbon(aes(x=t, ymin=y90_l, ymax=y90_u), alpha=.2, fill="blue") +  geom_ribbon(aes(x=t, ymin=y80_l, ymax=y80_u), alpha=.5, fill="grey") +
    xlim(c(min(span),max(span))) + labs(title="ARMA (1,1) Forecasts by MCMC", y="y") + facet_wrap(~series, nrow = N)
}
#+end_src

*** データ

#+begin_src R :results silent
varma11 <- arima.sim(n = 400, list(ar = c(0.8897), ma = c(-0.2279)),
                     sd = sqrt(0.1796))
#+end_src

#+begin_src R :results output graphics file :file (my/get-babel-file)
ggtsdisplay(varma11)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-QUnv7j.png]]

*** モデル

- [[http://ill-identified.hatenablog.com/entry/2016/02/14/205311][R + Stan で ベクトル ARMA (VARMA) を推定@ill-identified diary]]

#+begin_src stan :file models/varma_1-1.stan
data {
	int<lower=1> T ; // num observations
	int<lower=1> N ; // num series
	vector[N] y[T] ; // observed outputs
	int<lower=0> T_forecast ; // forecasting span
}

parameters {
	vector[N] mu ;        // mean coeffs
	matrix[N,N] Psi ;     // autoregression coeff matrix
	matrix[N,N] Theta ;   // moving avg coeff matrix
	cov_matrix[N] Sigma ; // noise scale matrix
}

transformed parameters{
  vector[N] eps[T] ; // error terms 
  
  eps[1] <- y[1] -mu ;
  for ( t in 2:T){
    eps[t] <- y[t] - (mu + Psi* y[t-1] + Theta * eps[t-1]) ;
  }
}

model {
	/* priors  */
	mu ~ normal(0,10) ;
	to_vector(Psi)   ~ normal(0,2) ;
	to_vector(Theta) ~ normal(0,2) ;
	Sigma ~ inv_wishart(N, N*diag_matrix(rep_vector(1,N))) ;

	/* likelihood */
	for (t in 2:T){
	  y[t] ~ multi_normal(mu + Psi * y[t-1] + Theta * eps[t-1], Sigma) ;
	}
	
}

/* prediction */
generated quantities{
  vector[N] y_pred[T_forecast] ; 
  vector[N] eps_pred[T_forecast] ;
  eps_pred[1] <- multi_normal_rng(rep_vector(0,N), Sigma) ;
  y_pred[1] <- mu + Psi * y[T] + Theta * eps[T] + eps_pred[1] ;
  for( t in 2:T_forecast) {
    eps_pred[t]    <- multi_normal_rng(rep_vector(0,N), Sigma) ;
    y_pred[t] <- mu + Psi*y_pred[t-1] + Theta * eps_pred[t-1] + eps_pred[t] ;
  }
}
#+end_src

#+RESULTS:
[[file:models/varma_1-1.stan]]

*** サンプリング

#+begin_src R
varma_11_fit <- stan(file = "models/varma_1-1.stan",
                     data = list(T = 400, N = 1, y = matrix(varma11), T_forecast = 50),
                     chain = 1)
#+end_src

#+RESULTS:
#+begin_example

Error in (function (x)  : attempt to apply non-function
recompiling to avoid crashing R session

SAMPLING FOR MODEL 'varma_1-1' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.001179 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 11.79 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 6.18923 seconds (Warm-up)
Chain 1:                5.70018 seconds (Sampling)
Chain 1:                11.8894 seconds (Total)
Chain 1:
#+end_example

*** auto.arima() (比較用)

#+begin_src R :results silent
auto_arima_fit <- auto.arima(varma11)
arima_fit <- arima(varma11, order = c(1,0,1))
#+end_src

*** 結果の確認
**** fit object

- Psi[1,1]=0.8897, Theta[1,1]=-0.2279, Sigma[1,1]=0.1796
#+begin_src R
print(varma_11_fit, pars = c("mu", "Psi", "Theta", "Sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: varma_1-1.
1 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=1000.

            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
mu[1]      -0.01       0 0.02 -0.05 -0.02 -0.01  0.00  0.02   895    1
Psi[1,1]    0.86       0 0.03  0.80  0.84  0.86  0.88  0.92   830    1
Theta[1,1] -0.25       0 0.05 -0.35 -0.29 -0.25 -0.21 -0.14   712    1
Sigma[1,1]  0.19       0 0.01  0.16  0.18  0.19  0.20  0.22   875    1

Samples were drawn using NUTS(diag_e) at Thu Nov 14 22:20:28 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

- =auto.arima()=
#+begin_src R
auto_arima_fit
#+end_src

#+RESULTS:
#+begin_example
Series: varma11 
ARIMA(2,0,0) with zero mean 

Coefficients:
         ar1     ar2
      0.5866  0.2344
s.e.  0.0485  0.0485

sigma^2 estimated as 0.1838:  log likelihood=-228.29
AIC=462.57   AICc=462.63   BIC=474.55
#+end_example

- =arima()=
#+begin_src R
arima_fit
#+end_src

#+RESULTS:
#+begin_example

Call:
arima(x = varma11, order = c(1, 0, 1))

Coefficients:
         ar1      ma1  intercept
      0.8691  -0.2602    -0.0695
s.e.  0.0302   0.0565     0.1194

sigma^2 estimated as 0.1842:  log likelihood = -229.66,  aic = 467.33
#+end_example

**** stan_plot()

#+begin_src R :results output graphics file :file (my/get-babel-file)
stan_plot(varma_11_fit, pars = c("mu", "Psi", "Theta", "Sigma"))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-NjUKjw.png]]

**** stan_trace()

#+begin_src R :results output graphics file :file (my/get-babel-file) :height 640
stan_trace(varma_11_fit, pars = c("mu", "Psi", "Theta", "Sigma"),nrow = 4, inc_warmup = TRUE)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-2g9aCk.png]]

** VARMA(p, q)
*** モデル
 
- [[http://ill-identified.hatenablog.com/entry/2016/02/14/205311][R + Stan で ベクトル ARMA (VARMA) を推定@ill-identified diary]]

#+begin_src stan :file models/varma_p-q.stan
data {
	int<lower=1> T; // num observations
	int<lower=1> N; // num series
	int<lower=0> p; // AR(p)
	int<lower=0> q; // MA(q)
	vector[N] y[T]; // observed outputs
	int<lower=0> T_forecast; // forecasting span
}

parameters {
	vector[N] mu;         // mean coeffs
	matrix[N,N] Psi[p];   // autoregression coeff matrix
	matrix[N,N] Theta[q]; // moving avg coeff matrix
	cov_matrix[N] Sigma;  // noise scale matrix
}

transformed parameters{
  vector[N] eps[T]; // error terms 
  
  eps[1] <- y[1] -mu;
  for (t in 2:T) `{
    eps[t] <- y[t] - mu;
    for(i in 1:min(t-1,p)) {
      eps[t] <- eps[t] - Psi[i] * y[t-i];
    }
    for(i in 1:min(t-1,q)) {
      eps[t] <- eps[t] - Theta[i] * eps[t-i];
    }
  }
}

model {
  vector[N] eta[T];
	/* priors  */
	mu ~ normal(0,10);
	for(i in 1:p)
	  to_vector(Psi[i]) ~ normal(0,2);
	for(i in 1:q)
	  to_vector(Theta[i]) ~ normal(0,2);
	Sigma ~ inv_wishart(N, N * diag_matrix(rep_vector(1,N)));

	/* likelihood */
	for (t in 1:T) {
	  eta[t] <- mu 
	  for(i in 1:min(t-1,p))
	    eta[t] <- eta[t] + Psi[i] * y[t-i];
	  for(i in 1:min(t-1,q))
	    eta[t] <- eta[t] + Theta[i] * eps[t-i];
	  y[t] ~ multi_normal(eta[t], Sigma);
	}
}

/* prediction */
generated quantities{
  vector[N] y_pred[T+T_forecast]; 
  vector[N] eps_pred[T+T_forecast];

  eps_pred[1:T] <- eps;
  y_pred[1:T] <- y;

  for(t in (T+1):(T+T_forecast)) {
    eps_pred[t] <- multi_normal_rng(rep_vector(0,N), Sigma);
    y_pred[t]   <- mu + eps_pred[t];
    for(i in 1:p)
	    y_pred[t] <- y_pred[t] + Psi[i] * y_pred[t-i];
	  for(i in 1:q)
	    y_pred[t] <- y_pred[t] + Theta[i] * eps_pred[t-i]
  }
}
#+end_src

#+RESULTS:
[[file:models/varma_p-q.stan]]

** Hidden Markov
*** 参考

- Blog: 機械学習・自然言語処理の勉強メモ
  - [[http://kento1109.hatenablog.com/entry/2017/12/15/160315][隠れマルコフモデル（HMM）について]]
  - [[http://kento1109.hatenablog.com/entry/2018/06/21/121441][Stan：隠れマルコフモデル1]]
  - [[http://kento1109.hatenablog.com/entry/2018/06/23/124927][Stan：隠れマルコフモデル2]]

*** User Guide の例
**** Model

#+begin_src stan :file models/hmm1.stan
data {
  int<lower=1> K;  // カテゴリーの数
  int<lower=1> V;  // 単語(word)の数
  int<lower=0> T;  // 時点の数
  int<lower=1,upper=V> w[T]; // 単語(word)
  int<lower=1,upper=K> z[T]; // カテゴリー
  vector<lower=0>[K] alpha;  // 推移(transit)確率の事前確率
  vector<lower=0>[V] beta;   // 単語 vを出力(emit)する確率の事前確率
}

parameters {
  // 推移(transit)確率
  // simplex[K] = 長さK で合計1のベクトル
  // つまり K x K の推移確率行列 P
  simplex[K] theta[K];
  
  // 単語 vを出力(emit)する確率
  simplex[V] phi[K];
}

model {
  // simplex 型のパラメタの事前分布
  // alpha, beta は ディリクレ分布のパラメタ
  for (k in 1:K)
    theta[k] ~ dirichlet(alpha);
  for (k in 1:K)
    phi[k] ~ dirichlet(beta);
  
  // カテゴリカル分布: Simplex 型の確率を受取り、K カテゴリーのいずれかを返す
  for (t in 1:T)
    w[t] ~ categorical(phi[z[t]]);
  for (t in 2:T)
    z[t] ~ categorical(theta[z[t - 1]]);
}
#+end_src

*** [[https://khakieconomics.github.io/2018/02/24/Regime-switching-models.html][Regime-switching models in Stan]] の例
**** データ (GOOGL)

#+begin_src R :results output graphics file :file (my/get-babel-file)
library(tidyverse)
library(rstan)
library(Quandl)

googl <- Quandl("WIKI/GOOGL", collapse = "weekly")
googl <- googl %>%
  mutate(Date = as.Date(Date)) %>%
  arrange(Date) %>%
  mutate(l_ac = log(`Adj. Close`),
         dl_ac = c(NA, diff(l_ac))) %>%
  dplyr::filter(Date > "2010-01-01")

plot.ts(googl$dl_ac, main = "Weekly changes in Google's adjusted close price")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-fqKCzJ.png]]

**** モデル

- 2 状態モデル

$\eta$ eta
$\xi$ xi

#+begin_src stan :file models/hmm2.stan
data {
  int T;
  vector[T] y;
}

parameters {
  vector<lower=0, upper=1>[2] p;    // 推移確率 (2状態なので、一方は 1-p で定義)
  real<lower=0> rho;                // AR(1)係数
  vector[2] alpha;
  vector<lower=0>[2] sigma;
  real<lower=0, upper=1> xi1_init;  // 初期値
  real y_tm1_init;                  // 初期値
}

transformed parameters {
  matrix[T, 2] eta; // 観測方程式の対数尤度の指数関数
  matrix[T, 2] xi;  // フィルター化確率
  vector[T] f;      // 時点Tの尤度 (各状態の加重平均)
  
  // fill in etas
  // 各観測方程式の尤度
  for(t in 1:T) {
    // ランダムウォーク (状態1)
    eta[t, 1] = exp(normal_lpdf(y[t]| alpha[1], sigma[1]));
    
    // AR(1)
    if(t == 1) {
      eta[t, 2] = exp(normal_lpdf(y[t]| alpha[2] + rho * y_tm1_init, sigma[2]));
    } else {
      eta[t, 2] = exp(normal_lpdf(y[t]| alpha[2] + rho * y[t-1], sigma[2]));
    }
  }
  
  // work out likelihood contributions
  for(t in 1:T) {
    // for the first observation
    if(t == 1) {
      f[t] = p[1]       * xi1_init       * eta[t, 1] + // stay in state 1
             (1 - p[1]) * xi1_init       * eta[t, 2] + // transition from 1 to 2
             p[2]       * (1 - xi1_init) * eta[t, 2] + // stay in state 2 
             (1 - p[2]) * (1 - xi1_init) * eta[t, 1];  // transition from 2 to 1
      
      // 時点1のフィルター化確率を更新
      xi[t, 1] = (p[1] * xi1_init * eta[t, 1] + (1 - p[2]) * (1 - xi1_init) * eta[t, 1]) / f[t];
      xi[t, 2] = 1.0 - xi[t, 1];
    
    } else {
      // and for the rest
      f[t] = p[1]       * xi[t-1, 1] * eta[t, 1] + // stay in state 1
             (1 - p[1]) * xi[t-1, 1] * eta[t, 2] + // transition from 1 to 2
             p[2]       * xi[t-1, 2] * eta[t, 2] + // stay in state 2 
             (1 - p[2]) * xi[t-1, 2] * eta[t, 1];  // transition from 2 to 1
      
      // work out xi
      // t-1 のフィルター化確率を使う
      xi[t, 1] = (p[1] * xi[t-1, 1] * eta[t, 1] + (1 - p[2]) * xi[t-1, 2] * eta[t, 1]) / f[t];
      
      // there are only two states so the probability of the other state is 1 - prob of the first
      xi[t, 2] = 1.0 - xi[t, 1];
    }
  }
  
}
model {
  // priors
  p ~ beta(10, 2);
  rho ~ normal(1, .1);
  alpha ~ normal(0, .1);
  sigma ~ cauchy(0, 1);
  xi1_init ~ beta(2, 2);
  y_tm1_init ~ normal(0, .1);
  
  // likelihood is really easy here!
  // 一度 exp() したのを再度 log() で元に戻す
  // log_sum_exp を使った方がいい?
  target += sum(log(f));
}
#+end_src

#+RESULTS:
[[file:models/hmm2.stan]]

**** 当てはめ

#+begin_src R
compiled_model <- stan_model("models/hmm2.stan")
googl_mod <- sampling(compiled_model,
                      data= list(T = nrow(googl),
                                 y = googl$dl_ac * 100), # リターン x 100 の方が収束しやすい?
                      iter = 1000, chains = 4)
#+end_src

#+RESULTS:
#+begin_example
recompiling to avoid crashing R session
Warning message:
In readLines(file, warn = TRUE) :
  incomplete final line found on '/home/shun/Dropbox/repos/github/five-dots/notes/lang/stan/time_series/models/hmm2.stan'


SAMPLING FOR MODEL 'hmm2' NOW (CHAIN 1).

SAMPLING FOR MODEL 'hmm2' NOW (CHAIN 2).

SAMPLING FOR MODEL 'hmm2' NOW (CHAIN 3).
Chain 1: 
Chain 1: Gradient evaluation took 0.000917 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 9.17 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 2: 
Chain 2: Gradient evaluation took 0.000989 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 9.89 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.00082 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 8.2 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 3.43312 seconds (Warm-up)
Chain 1:                1.66614 seconds (Sampling)
Chain 1:                5.09926 seconds (Total)
Chain 1: 
Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 4.30065 seconds (Warm-up)
Chain 2:                2.13856 seconds (Sampling)
Chain 2:                6.4392 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'hmm2' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.002263 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 22.63 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 4.1207 seconds (Warm-up)
Chain 3:                2.25941 seconds (Sampling)
Chain 3:                6.38011 seconds (Total)
Chain 3: 
Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 3.91466 seconds (Warm-up)
Chain 4:                1.19022 seconds (Sampling)
Chain 4:                5.10488 seconds (Total)
Chain 4:
#+end_example

**** 収束確認

#+begin_src R
print(googl_mod, pars = c("alpha", "rho", "p", "sigma"))
#+end_src

#+RESULTS:
#+begin_example
Inference for Stan model: hmm2.
4 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=2000.

         mean se_mean   sd  2.5%   25%  50%   75% 97.5% n_eff Rhat
alpha[1] 0.06    0.00 0.08 -0.11  0.00 0.06  0.11  0.21  3930    1
alpha[2] 0.01    0.00 0.10 -0.18 -0.06 0.01  0.08  0.19  3710    1
rho      0.96    0.00 0.10  0.75  0.89 0.96  1.03  1.16  3458    1
p[1]     0.96    0.00 0.02  0.91  0.95 0.96  0.97  0.98  1769    1
p[2]     0.53    0.00 0.12  0.31  0.45 0.53  0.61  0.77  2544    1
sigma[1] 2.87    0.00 0.13  2.61  2.78 2.87  2.96  3.13  2067    1
sigma[2] 9.48    0.05 2.13  6.61  8.10 9.08 10.39 15.30  1551    1

Samples were drawn using NUTS(diag_e) at Tue Dec  3 11:53:22 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

**** プロット確認

#+begin_src R :results output graphics file :file (my/get-babel-file)
goog <- as.data.frame(googl_mod, pars = "xi") %>% 
  gather(par, value) %>% 
  dplyr::filter(grepl(",1", par)) %>%
  mutate(time = readr::parse_number(stringr::str_extract(par, "[0-9]{1,3}[,]"))) %>% 
  group_by(par) %>% 
  summarise(time = first(time), 
            mean = mean(value),
            lower = quantile(value, .025),
            upper = quantile(value, .975)) %>% 
  ## mutate(date = readr::parse_date(googl$Date)) %>% 
  mutate(date = googl$Date) %>% 
  ggplot(aes(x = date, y = mean)) +
  #geom_point(alpha = 0.3) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3,  fill = "green") +
  labs(title ="Probability of being in random walk state\n vsv momentum state",
       y = "Probability",
       subtitle = "GOOGL")

close <- ggplot(data = googl,aes(x = Date, y = `Adj. Close`)) +
  geom_line(colour = "green")

gridExtra::grid.arrange(goog, close)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-RaptiS.png]]

*** [[https://discourse.mc-stan.org/t/regime-switching-model/4098][Regime Switching Model@Stan Forum]] の例

#+begin_src stan :file models/hmm3.stan
data {
  int<lower=1> T; // number of observations (length) - time series of returns
  real r[T];      // weekly return
  
  // 1-2 bear regime
  // 3-4 bull regime
  real m[4];  // mu prior mean m = c(-0.7, 0.2, -0.2, 0.3)
  real n[4];  // mu prior sd n = c(1, 1, 1, 1)
  real v[4];  // sigma2 prior v = c(1, 1, 1, 1)
  real s[4];  // sigma2 prior s = c(1, 1, 1, 1)
  
  vector<lower=0>[3] alpha[4];  // transit prior rows
  // {p11, p12, p14} ~ Dir(8, 1.5, 0.5), 
  // {p21, p22, p24} ~ Dir(1.5, 8, 0.5), 
  // {p31, p33, p34} ~ Dir(0.5, 8, 1.5), 
  // {p41, p43, p44} ~ Dir(0.5, 1.5, 8).
  
  simplex[4] s0;             // inital state
}

parameters {
  simplex[3] reppp[4];     // transit probs row i 
  real<upper=0> mu1;       // bear market
  real<lower=0> mu2;       // bear rally
  real<upper=0> mu3;       // bull correction
  real<lower=0> mu4;       // bull market
  real<lower=0> sigma2[4]; // sd
  // simplex[4] s0;        // inital state
}

transformed parameters {
  // First, let’s define the likelihood contribution of an observation r[t] under each state
  vector[4] eta[T]; 
  simplex[4] si[T]; // probability of being in each state
  vector[T] f;      // likelihood
  vector[4] PI;     // unconditional state probs
  matrix[4,4] P;    // transit probs
  real mu_bull;
  real mu_bear;

  // put together the transition matrix
  for (j in 1:4){
    P[j, 1] = reppp[j,1];
     if ( j == 1 || j==2 ){
       P[j, 2] = reppp[j,2];
       P[j, 3] = 0;
     } else {
       P[j, 2] = 0;
       P[j, 3] = reppp[j,2];
     }
     P[j, 4] = reppp[j,3];
   }
  
  // fill in etas (likelihood contribution of an observation r[t] under each state)
  {
    real mu[4];                // expected returns
    
    mu[1] = mu1;
    mu[2] = mu2;
    mu[3] = mu3;
    mu[4] = mu4;
   
    for(t in 1:T) {
      for(j in 1:4 ){
        eta[t,j] = exp(normal_lpdf(r[t] | mu[j], sigma2[j]));
      }
    }
  }
  
   // work out likelihood contributions
   f[1] = dot_product(P * s0, eta[1]);
   si[1] = ((P * s0) .* eta[1]) / f[1];
   
   // and for the rest
   for(t in 2:T) {
     f[t] = dot_product(P * si[t-1], eta[t]);
     si[t] =  ((P * si[t-1]) .* eta[t]) / f[t];
   }
  
  // unconditional probabilities PI
  {
    matrix[4,5] A;
    vector[5] ee;

    matrix[4,4] AAprime;
    vector[4] AE;

    A = append_col(P - diag_matrix(rep_vector(1., 4)),rep_vector(1., 4));
    ee = rep_vector(0., 5);
    ee[5] = 1.0;
    // PI = (A' * A) \ (A' * ee);
    AAprime = tcrossprod(A);
    AE = A * ee;
    PI = AAprime \ AE;

    mu_bear = PI[1]/(PI[1]+PI[2])*mu1 + PI[2]/(PI[1]+PI[2])*mu2;
    mu_bull = PI[3]/(PI[3]+PI[4])*mu3 + PI[4]/(PI[3]+PI[4])*mu4;
  }
}


// T observations (length)
model {
  // Priors
  for (kk in 1:4) {
    reppp[kk] ~ dirichlet(alpha[kk]); // Sample the transition probability
    sigma2[kk] ~ cauchy(v[kk], s[kk]);
  }
  mu1 ~ normal(m[1], n[1]);
  mu2 ~ normal(m[2], n[2]);
  mu3 ~ normal(m[3], n[3]);
  mu4 ~ normal(m[4], n[4]);
  
  // likelihood is really easy here!
  target += sum(log(f));

  if (mu_bull < 0)
    target += negative_infinity();
  if(mu_bear > 0)
    target += negative_infinity();
}
#+end_src

*** [[https://zenodo.org/record/1284341/files/main_pdf.pdf?download=1][A Tutorial on Hidden Markov Models using Stan - Zenodo]]
**** Gaussian の例
***** モデル

#+begin_src stan :file models/hmm_gaussian.stan
functions {
  vector normalize(vector x) {
    return x / sum(x);
  }
}

data {
  int<lower=1> T; // number of observations (length)
  int<lower=1> K; // number of hidden states
  real y[T];      // observations
}

parameters {
  // Discrete state model
  simplex[K] pi1;  // initial state probabilities
  simplex[K] A[K]; // transition probabilities
                   // A[i][j] = p(z_t = j | z_{t-1} = i)

  // Continuous observation model
  ordered[K] mu;          // observation means
  real<lower=0> sigma[K]; // observation standard deviations
}

transformed parameters {
  vector[K] logalpha[T];

  { // log p(z_t = j | x_{1:t})
    real accumulator[K];

    // t=1 の対数尤度
    // 確率 x 尤度の積ではなく、対数の和にしている
    logalpha[1] = log(pi1) + normal_lpdf(y[1] | mu, sigma);

    // t>1 の対数尤度
    for (t in 2:T) {
      for (j in 1:K) { // j = current (t)
        for (i in 1:K) { // i = previous (t-1)
         // Murphy (2012) Eq. 17.48
         // belief state + transition prob + local evidence at t
          accumulator[i] = logalpha[t-1, i] + log(A[i, j]) + normal_lpdf(y[t] | mu[j], sigma[j]);
        }
        logalpha[t, j] = log_sum_exp(accumulator);
      }
    }
  }
}

model {
  // Note: update based only on last logalpha
  target += log_sum_exp(logalpha[T]);
}

generated quantities {
  vector[K] logbeta[T];
  vector[K] loggamma[T];

  vector[K] alpha[T];
  vector[K] beta[T];
  vector[K] gamma[T];

  int<lower=1, upper=K> zstar[T];
  real logp_zstar;

  { // Forward algortihm
    // 人間が理解しやすいように、合計 1 の確率ベクトルに変換する
    for (t in 1:T)
      alpha[t] = softmax(logalpha[t]);
  }

  { // Backward algorithm log p(x_{t+1:T} | z_t = j)
    real accumulator[K];

    // 初期値 (=最新のデータ)
    for (j in 1:K)
      logbeta[T, j] = 1;

    // 最後から decrement していく
    // T=100 であれば、100, 99, 98, ..., 2 まで
    for (tforward in 0:(T-2)) {
      int t;
      t = T - tforward;

      // フィルタ化確率の逆のアルゴリズム
      // t-1 の状態から t の状態への遷移確率ではなく、
      // t の状態で t-1 を更新していく
      for (j in 1:K) { // j = previous (t-1)
        for (i in 1:K) { // i = next (t)
          // Murphy (2012) Eq. 17.58
          // backwards t + transition prob + local evidence at t
          accumulator[i] = logbeta[t, i] + log(A[j, i]) + normal_lpdf(y[t] | mu[i], sigma[i]);
        }
        logbeta[t-1, j] = log_sum_exp(accumulator);
      }
    }

    for (t in 1:T)
      beta[t] = softmax(logbeta[t]);
  }

  { // Forwards-backwards algorithm log p(z_t = j | x_{1:T})
    for(t in 1:T)
      loggamma[t] = alpha[t] .* beta[t];

    for(t in 1:T)
      gamma[t] = normalize(loggamma[t]);
  }

  { // Viterbi algorithm

    // backpointer to the most likely previous state on the most probable path
    int bpointer[T, K];

    // max prob for the seq up to t with final output from state k for time t
    real delta[T, K];

    // 初期値
    for (j in 1:K)
      delta[1, K] = normal_lpdf(y[1] | mu[j], sigma[j]);

    for (t in 2:T) {
      for (j in 1:K) { // j = current (t)
        delta[t, j] = negative_infinity();
        for (i in 1:K) { // i = previous (t-1)
          real logp;
          logp = delta[t-1, i] + log(A[i, j]) + normal_lpdf(y[t] | mu[j], sigma[j]);
          if (logp > delta[t, j]) {
            bpointer[t, j] = i;
            delta[t, j] = logp;
          }
        }
      }
    }

    logp_zstar = max(delta[T]);

    for (j in 1:K)
      if (delta[T, j] == logp_zstar)
        zstar[T] = j;

    for (t in 1:(T - 1)) {
      zstar[T - t] = bpointer[T - t + 1, zstar[T - t + 1]];
    }
  }
}
#+end_src

#+RESULTS:
[[file:models/hmm_gaussian.stan]]

***** データ

- シミュレーションデータ
  - 状態 = マルコフ過程
  - 観測 = 正規分布

#+begin_src R
runif_simplex <- function(T) {
  x <- -log(runif(T))
  x / sum(x) # 合計1に規格化
}

hmm_generate <- function(K, T) {
  ## 1. Parameters
  pi1   <- runif_simplex(K)                  # 一様分布から初期確率を生成
  A     <- t(replicate(K, runif_simplex(K))) # K x K 推移確率
  mu    <- sort(rnorm(K, 10 * 1:K, 1))       # 弱情報事前分布
  sigma <- abs(rnorm(K))                     # 弱情報事前分布 (半正規分布)

  ## 2. Hidden path
  z <- vector("numeric", T)

  z[1] <- sample(1:K, size = 1, prob = pi1) # 初期確率pi1 に合わせて初期状態を生成
  for (t in 2:T) {
    z[t] <- sample(1:K, size = 1, prob = A[z[t-1], ]) # 1期前の状態の推移確率から生成
  }

  ## 3. Observations
  y <- vector("numeric", T)
  for (t in 1:T) {
    y[t] <- rnorm(1, mu[z[t]], sigma[z[t]]) # 時点 t の状態毎の平均 + SD
  }

  list(y = y, z = z,
       theta = list(pi1 = pi1, A = A, mu = mu, sigma = sigma))
}
K <- 3
T <- 100
hmm_generate(K, T)
#+end_src

#+RESULTS:
#+begin_example

$y
  [1] 21.268295 29.977281 21.553754 30.091668 21.793068 30.436615 21.630538
  [8] 29.851415 21.406993 29.582339 21.033803 29.599152 20.550432 30.199135
 [15] 30.562921  9.778520 30.381841 30.421998 21.760930 30.874117 21.178774
 [22] 29.571076 21.446960 30.237837 21.265799 29.623402 21.691275 30.096701
 [29] 21.824414 29.863263 21.126033 30.191233 21.059094 30.351043 21.178656
 [36] 29.889396 21.562872 29.804812 20.986227 30.740604 29.878973 29.812671
 [43] 11.959417 30.573676  8.455726 21.021542 30.028140 21.620053 30.135953
 [50] 21.346760 29.761529 30.055181 20.969060 30.095072 21.036770 30.207374
 [57] 20.658925 30.071083 20.998896 30.215695 21.036372 30.922591 21.784041
 [64] 30.022398 13.635601 29.745553 10.984886 21.462227 30.046907 21.643819
 [71] 29.950412 21.372514 30.378084 21.647405 30.136669 20.931928 30.321928
 [78]  9.588104 30.709045 21.230994 30.009067 21.241585 30.025377 20.326908
 [85] 29.860756 21.017206 30.174947 21.543332 30.114428 20.566399 30.360670
 [92] 21.102488 28.733215 20.782250 30.002217 21.701332 29.823083 21.569586
 [99] 29.700245 30.121535

$z
  [1] 2 3 2 3 2 3 2 3 2 3 2 3 2 3 3 1 3 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2
 [38] 3 2 3 3 3 1 3 1 2 3 2 3 2 3 3 2 3 2 3 2 3 2 3 2 3 2 3 1 3 1 2 3 2 3 2 3 2
 [75] 3 2 3 1 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 3

$theta
$theta$pi1
[1] 0.01356010 0.95684187 0.02959803

$theta$A
           [,1]      [,2]      [,3]
[1,] 0.21027981 0.2888930 0.5008272
[2,] 0.01402289 0.0192630 0.9667141
[3,] 0.10689710 0.8333865 0.0597164

$theta$mu
[1] 10.41895 21.15231 30.05361

$theta$sigma
[1] 1.0943002 0.4693637 0.3329071
#+end_example

***** 当てはめ

#+begin_src R :results silent
hmm_init <- function(K, y) {
  clasif <- kmeans(y, K)
  init.mu <- by(y, clasif$cluster, mean)
  init.sigma <- by(y, clasif$cluster, sd)
  init.order <- order(init.mu)

  list(
    mu = init.mu[init.order],
    sigma = init.sigma[init.order]
  )
}

hmm_fit <- function(K, y) {
  rstan_options(auto_write = TRUE)
  options(mc.cores = parallel::detectCores())
  stan.model = "models/hmm_gaussian.stan"
  stan.data = list(
    T = length(y),
    K = K,
    y = y
  )

  stan(file = stan.model,
       data = stan.data,
       verbose = T,
       iter = 400,
       warmup = 200,
       thin = 1,
       chains = 1,
       cores = 1,
       seed = 900,
       ## 初期値をリストで返す関数
       init = function() { hmm_init(K, y) })
}
#+end_src

#+begin_src R
set.seed(900)
K <- 3
T_length <- 500
dataset <- hmm_generate(K, T_length)
fit <- hmm_fit(K, dataset$y)
#+end_src

#+RESULTS:
#+begin_example

TRANSLATING MODEL 'hmm_gaussian' FROM Stan CODE TO C+
CODE NOW.
successful in parsing the Stan model 'hmm_gaussian'.
COMPILING THE C+
CODE FOR MODEL 'hmm_gaussian' NOW.
OS: x86_64, linux-gnu; rstan: 2.19.2; Rcpp: 1.0.2; inline: 0.3.15 
g+
-std=gnu++14 -I"/usr/share/R/include" -DNDEBUG   -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/Rcpp/include/"  -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/RcppEigen/include/"  -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/RcppEigen/include/unsupported"  -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/BH/include" -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/StanHeaders/include/src/"  -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/StanHeaders/include/"  -I"/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/rstan/include" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS    -fpic  -g -O2 -fdebug-prefix-map=/build/r-base-uuRxut/r-base-3.6.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c file347c6f84380a.cpp -o file347c6f84380a.o
g+
-std=gnu++14 -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -Wl,-z,relro -o file347c6f84380a.so file347c6f84380a.o -L/home/shun/Dropbox/R/x86_64-pc-linux-gnu-library/3.6/StanHeaders/lib/ -lStanHeaders -L/usr/lib/R/lib -lR

CHECKING DATA AND PREPROCESSING FOR MODEL 'hmm_gaussian' NOW.

COMPILING MODEL 'hmm_gaussian' NOW.

STARTING SAMPLER FOR MODEL 'hmm_gaussian' NOW.

SAMPLING FOR MODEL 'hmm_gaussian' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.001119 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 11.19 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 400 [  0%]  (Warmup)
Chain 1: Iteration:  40 / 400 [ 10%]  (Warmup)
Chain 1: Iteration:  80 / 400 [ 20%]  (Warmup)
Chain 1: Iteration: 120 / 400 [ 30%]  (Warmup)
Chain 1: Iteration: 160 / 400 [ 40%]  (Warmup)
Chain 1: Iteration: 200 / 400 [ 50%]  (Warmup)
Chain 1: Iteration: 201 / 400 [ 50%]  (Sampling)
Chain 1: Iteration: 240 / 400 [ 60%]  (Sampling)
Chain 1: Iteration: 280 / 400 [ 70%]  (Sampling)
Chain 1: Iteration: 320 / 400 [ 80%]  (Sampling)
Chain 1: Iteration: 360 / 400 [ 90%]  (Sampling)
Chain 1: Iteration: 400 / 400 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 24.3301 seconds (Warm-up)
Chain 1:                2.71976 seconds (Sampling)
Chain 1:                27.0498 seconds (Total)
Chain 1: 
Warning messages:
1: In readLines(file, warn = TRUE) :
  incomplete final line found on '/home/shun/Dropbox/repos/github/five-dots/notes/lang/stan/time_series/models/hmm_gaussian.stan'
2: The largest R-hat is 1.08, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat 
3: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
4: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess
#+end_example

***** フィルタ化確率

- かなり良い正答率
#+begin_src R
alpha <- extract(fit, pars = "alpha")[[1]]
gamma <- extract(fit, pars = "gamma")[[1]]

alpha_med <- apply(alpha, c(2, 3), function(x) { quantile(x, c(0.50)) })
alpha_hard <- apply(alpha_med, 1, which.max)
table(true = dataset$z, estimated = alpha_hard)
#+end_src

#+RESULTS:
:     estimated
: true   1   2   3
:    1 155   0   0
:    2   6 226   1
:    3   0   5 107

***** zstar のプロット

#+begin_src R :results output graphics file :file (my/get-babel-file)
zstar <- extract(fit, pars = 'zstar')[[1]]
plot_statepath(zstar, dataset$z)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-LTbcWJ.png]]

#+begin_src R
table(true = dataset$z, estimated = apply(zstar, 2, median))
#+end_src

#+RESULTS:
:     estimated
: true   1   2   3
:    1 154   1   0
:    2   4 227   2
:    3   0   5 107

***** Most probable path

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot_outputvit(x = dataset$y,
               z = dataset$z,
               zstar = zstar,
               main = "Most probable path")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-9BcTJd.png]]

**** MS-GARCH の例
***** モデル

#+begin_src stan :file models/hmm_garch.stan
data {
  int<lower=0> T; // Length of series (number of observations)
  real y[T];      // Return series
}

parameters {
  // GARCH Parameters
  positive_ordered[2] alpha0; // Ordering prevents label switching

  real<lower=0, upper=1> alpha1[2];
  real<lower=0, upper=1-alpha1[1]> beta1_1;
  real<lower=0, upper=1-alpha1[2]> beta1_2;

  // HMM Transition Probabilities =>
  // Parameterize by probability of staying in state
  real<lower=0, upper=1> p_remain[2];
}

transformed parameters {
  // GARCH Parameters
  real<lower=0> beta1[2];

  // Vector of instantaneous GARCH volatilities
  vector[2] sigma_t[T];

  // HMM Parameters
  vector[2] log_alpha[T]; // Accumulated (unnormalized) state probabilities

  // Transition probabilities
  matrix[2, 2] A;
  A[1, 1] =  p_remain[1];
  A[1, 2] = 1 - p_remain[1];
  A[2, 1] = 1 - p_remain[2];
  A[2, 2] = p_remain[2];

  // GARCH Component
  // ------------------

  // Load beta1 from parameters
  beta1[1] = beta1_1;
  beta1[2] = beta1_2;

  // Initialize at unconditional variances
  sigma_t[1, 1] = alpha0[1] / (1 - alpha1[1] - beta1[1]); // Low-vol
  sigma_t[1, 2] = alpha0[2] / (1 - alpha1[2] - beta1[2]); // High-vol

  // GARCH dynamics rolling forward
  for(t in 2:T){
    for(i in 1:2){
      sigma_t[t, i] = sqrt(alpha0[i] +
                           alpha1[i] * pow(y[t-1], 2) +
                           beta1[i] * pow(sigma_t[t-1, i], 2));
    }
  }

  // HMM Component
  // ------------------

  { // Calculate log p(state at t = j | history up to t) recursively
    // Markov property allows us to do one-step updates

    real accumulator[2];

    // Assume initial equal distribution among two states
    // Better model would be to weight by HMM stationary distribution
    log_alpha[1, 1] = log(0.5) + normal_lpdf(y[1] | 0, sigma_t[1, 1]);
    log_alpha[1, 2] = log(0.5) + normal_lpdf(y[1] | 0, sigma_t[1, 2]);

    for(t in 2:T){
      for(j in 1:2) { // Current state
        for(i in 1:2) { // Previous state
          accumulator[i] = log_alpha[t-1, i] + // Probability from previous obs
                           log(A[i, j]) + // Transition probability
                           // (Local) likelihood / evidence for given state
                           normal_lpdf(y[t] | 0, sigma_t[t-1, i]);
        }
        log_alpha[t, j] = log_sum_exp(accumulator);
      }
    }
  }
}

model {
  // Priors

  // GARCH components (weakly informative)
  alpha0 ~ normal(0, 0.5); // Baseline vol is ~ 0.05
  alpha1 ~ normal(0, 1);
  beta1 ~ normal(1, 1); // Most volatility persistance from MA term of GARCH models

  // HMM components
  p_remain ~ beta(3, 1); // Weakly informative way to say that states are sort-of sticky

  // Likelihood
  target += log_sum_exp(log_alpha[T]); // Note: update based only on last log_alpha
}

generated quantities{
  vector[2] alpha[T];

  for(t in 1:T){
    alpha[t] = softmax(log_alpha[t]);
  }
}
#+end_src

#+RESULTS:
[[file:models/hmm_garch.stan]]

*** MSAR(1)
**** 注意点

- MCMC Chain がレジームを混同してしまう問題。(混合モデルのラベルスイッチング問題)
- Chain 毎に別のレジームを捉えてしまうため、係数が Mix されてしまう
- 対策
  1. パラメタに =ordered= や =positive_ordered= を使う
  2. =chains = 1= でサンプリングする
  3. 初期値をレジーム毎に変更する

- [[https://stan-ja.github.io/gh-pages-html/#%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%83%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%AE%E3%83%A9%E3%83%99%E3%83%AB%E3%82%B9%E3%82%A4%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0][混合分布モデルでのラベルスイッチング@Stan Manual]]
- [[http://kento1109.hatenablog.com/entry/2018/07/01/134603][Stan：混合モデルとラベルスイッチング@機械学習・自然言語処理の勉強メモ]]

**** モデル 1 (モデル 2 の方が完成度が高い)

- [[https://khakieconomics.github.io/2018/02/24/Regime-switching-models.html][Regime-switching models in Stan]] を参考

#+begin_src stan :file models/msar_1_1.stan
#include arma_funs.stan

data {
  int<lower=1> T;
  real r[T];
}

parameters {
  vector<lower=0, upper=1>[2] p;
  vector[2] alpha;
  // vector<lower=-1, upper=1>[2] beta;
  real<lower=0> beta1;
  real<upper=0> beta2;
  vector<lower=0>[2] sigma;

  real<lower=0, upper=1> xi1_init; // 状態 1 のフィルター確率の初期値
  real r_tm1_init;
}

transformed parameters {
  matrix[T, 2] eta; // 2 状態分の尤度
  matrix[T, 2] xi;  // 状態毎のフィルター化確率
  vector[T] f;
  real p11;
  real p12;
  real p21;
  real p22;

  // 観測方程式 (状態毎の尤度)
  for (t in 1:T) {
    if (t == 1) {
      eta[t, 1] = exp(normal_lpdf(r[t]| alpha[1] + beta1 * r_tm1_init, sigma[1]));
      eta[t, 2] = exp(normal_lpdf(r[t]| alpha[2] + beta2 * r_tm1_init, sigma[2]));
    } else {
      eta[t, 1] = exp(normal_lpdf(r[t]| alpha[1] + beta1 * r[t-1], sigma[1]));
      eta[t, 2] = exp(normal_lpdf(r[t]| alpha[2] + beta2 * r[t-1], sigma[2]));
    }
  }

  // 観測方程式を元に、状態の確率毎に重み付け
  for (t in 1:T) {
    if (t == 1) {
      p11 = p[1]       * xi1_init       * eta[t, 1];
      p12 = (1 - p[1]) * xi1_init       * eta[t, 2];
      p22 = p[2]       * (1 - xi1_init) * eta[t, 2];
      p21 = (1 - p[2]) * (1 - xi1_init) * eta[t, 1];
    } else {
      p11 = p[1]       * xi[t-1, 1] * eta[t, 1];
      p12 = (1 - p[1]) * xi[t-1, 1] * eta[t, 2];
      p22 = p[2]       * xi[t-1, 2] * eta[t, 2];
      p21 = (1 - p[2]) * xi[t-1, 2] * eta[t, 1];
    }

    f[t] = p11 + p12 + p22 + p21;
    xi[t, 1] = (p11 + p12) / f[t];
    xi[t, 2] = 1.0 - xi[t, 1];
  }
}

model {
  // Priors
  p ~ beta(10, 2);
  // beta ~ normal(0, 2);
  beta1 ~ normal(0, 2);
  beta2 ~ normal(0, 2);
  sigma ~ cauchy(0, 5);
  xi1_init ~ beta(2, 2);
  r_tm1_init ~ normal(0, 0.1);

  target += sum(log(f));
  // for (t in 2:T) {
  //   r[t] ~ normal(alpha + constrain_stationary(beta) * r[t-1], sigma);
  // }
}
#+end_src

#+RESULTS:
[[file:models/msar_1_1.stan]]

**** モデル 2

- [[https://zenodo.org/record/1284341/files/main_pdf.pdf?download=1][A Tutorial on Hidden Markov Models using Stan - Zenodo]] を参考

#+begin_src stan :file models/msar_1_2.stan
// TODO constrain_stationary() の real 対応

#include arma_funs.stan

data {
  int<lower=1> K;
  int<lower=1> T;
  real y[T];
}

parameters {
  // State model
  simplex[K] pi1;  // レジーム確率の初期値
  simplex[K] P[K]; // K x K の推移確率行列

  // Observation model
  vector[K] mu;
  vector<lower=-1, upper=1>[K] beta; // AR 係数

  // ボラティリティ順にレジームを並べる
  positive_ordered[K] sigma;
}

transformed parameters {
  // 対数レジーム確率 (フィルタ化確率) の時系列
  vector[K] log_pi[T];
  vector[K] accumulator;

  log_pi[1] = log(pi1) + normal_lpdf(y[1] | mu, sigma);

  for (t in 2:T) {
    // j = current (t)
    for (j in 1:K) {
      // i = previous (t-1)
      for (i in 1:K) {
        accumulator[i] =
          log_pi[t-1, i] +
          log(P[i, j]) +
          normal_lpdf(y[t] | mu[j] + beta[j] * y[t-1], sigma[j]);
      }
      // 対数尤度を一旦、指数関数で元に戻した上で合算し、再度対数化
      log_pi[t, j] = log_sum_exp(accumulator);
    }
  }
}

model {
  mu ~ normal(0, 0.05); // リターンの切片が 0 より大きく異なることは稀
  beta ~ normal(0, 2);
  sigma ~ cauchy(0, 5);

  // 対数尤度を累積しているので、最終時点 T のみを評価すればよい
  target += log_sum_exp(log_pi[T]);
}

generated quantities {
  vector[K] log_xi[T];
  vector[K] log_gamma[T];

  vector[K] pi[T];
  vector[K] xi[T];
  vector[K] gamma[T];

  // レジーム確率
  { // Forward algorithm
    for (t in 1:T)
      // 対数尤度を合計 1 の確率ベクトルに変換する
      pi[t] = softmax(log_pi[t]);
  }

  // 平滑化確率
  { // Backward algorithm
    real accumulator2[K];

    // 初期値 (=最新のデータ)
    for (j in 1:K)
      log_xi[T, j] = 1;

    // 新->旧の順に計算していく
    for (tforward in 0:(T-2)) {
      int t = T - tforward;

      /* レジーム確率の逆のアルゴリズム
         t-1 の状態から t の状態への遷移確率ではなく、
         t の状態で t-1 を更新していく */

      // j = previous (t-1)
      for (j in 1:K) {
        // i = next (t)
        for (i in 1:K) {
          // backwards t + transition prob + local evidence at t
          accumulator2[i] = log_xi[t, i] + log(P[j, i]) + normal_lpdf(y[t] | mu[i], sigma[i]);
        }
        log_xi[t-1, j] = log_sum_exp(accumulator2);
      }
    }
    for (t in 1:T)
      xi[t] = softmax(log_xi[t]);
  }

  { // Forwards-backwards algorithm
    for(t in 1:T)
      log_gamma[t] = pi[t] .* xi[t];

    for(t in 1:T)
      gamma[t] = normalize(log_gamma[t]);
  }
}
#+end_src

**** データ

#+begin_src R :results output graphics file :file (my/get-babel-file)
ar1 <- arima.sim(model = list(order = c(1,0,0), ar = 0.5), mean = 0, sd = 1, n = 500)
ar2 <- arima.sim(model = list(order = c(1,0,0), ar = -0.5), mean = 0, sd = 3, n = 200)
msar1 <- c(ar1[1:200], ar2[1:100], ar1[201:500], ar2[101:200])
ggtsdisplay(msar1)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-AhZBwV.png]]

**** 当てはめ

#+begin_src R
msar1_2_fit <- stan(file = "models/msar_1_2.stan",
                    data = list(K = 2, T = length(msar1), y = msar1),
                    iter = 2000,
                    chains = 1)
msar1_2_fit
#+end_src

#+RESULTS:
#+begin_example
SYNTAX ERROR, MESSAGE(S) FROM PARSER:
Variable "p" does not exist.
 error in 'model89e54efd31_msar_1' at line 44, column 13
  -------------------------------------------------
    42:   for (t in 1:T) {
    43:     if (t == 1) {
    44:       p11 = p[1]       * xi1_init       * eta[t, 1];
                    ^
    45:       p12 = (1 - p[1]) * xi1_init       * eta[t, 2];
  -------------------------------------------------

Error in stanc(file = file, model_code = model_code, model_name = model_name,  : 
  failed to parse Stan model 'msar_1' due to the above error.
In addition: Warning message:
In readLines(file, warn = TRUE) :
  incomplete final line found on '/home/shun/Dropbox/repos/github/five-dots/notes/lang/stan/time_series/models/msar_1.stan'
Inference for Stan model: msar_1.
4 chains, each with iter=3000; warmup=1500; thin=1; 
post-warmup draws per chain=1500, total post-warmup draws=6000.

              mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff
p[1]          0.74    0.01 0.17    0.44    0.57    0.79    0.89    0.97   360
p[2]          0.66    0.01 0.14    0.47    0.56    0.62    0.75    0.95   352
alpha[1]     -0.13    0.00 0.12   -0.36   -0.21   -0.14   -0.06    0.09  3799
alpha[2]     -0.03    0.00 0.21   -0.47   -0.17   -0.03    0.11    0.37  4319
beta1         0.41    0.00 0.17    0.16    0.29    0.39    0.50    0.87  1289
beta2        -0.65    0.00 0.09   -0.82   -0.71   -0.65   -0.60   -0.50  2604
sigma[1]      0.84    0.00 0.12    0.61    0.76    0.84    0.91    1.08  1648
sigma[2]      2.33    0.00 0.20    2.00    2.19    2.31    2.45    2.76  1680
xi1_init      0.45    0.01 0.22    0.09    0.29    0.44    0.62    0.87  1301
r_tm1_init    0.01    0.00 0.10   -0.19   -0.06    0.01    0.07    0.21  5971
eta[1,1]      0.26    0.00 0.05    0.16    0.24    0.27    0.29    0.35  3717
eta[1,2]      0.16    0.00 0.01    0.14    0.15    0.16    0.17    0.19  1520
eta[2,1]      0.38    0.00 0.06    0.29    0.34    0.37    0.40    0.56  1525
eta[2,2]      0.15    0.00 0.01    0.12    0.14    0.15    0.16    0.17  1469
eta[3,1]      0.37    0.00 0.06    0.28    0.33    0.36    0.39    0.54  1573
eta[3,2]      0.15    0.00 0.01    0.12    0.14    0.15    0.16    0.17  1479
eta[4,1]      0.44    0.00 0.07    0.34    0.40    0.43    0.48    0.60  1495
eta[4,2]      0.16    0.00 0.01    0.13    0.15    0.16    0.16    0.18  1409
eta[5,1]      0.43    0.00 0.06    0.33    0.39    0.42    0.46    0.56  3842
eta[5,2]      0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1628
eta[6,1]      0.04    0.00 0.03    0.00    0.02    0.04    0.06    0.10  1384
eta[6,2]      0.14    0.00 0.01    0.12    0.13    0.14    0.15    0.16  1934
eta[7,1]      0.30    0.00 0.09    0.17    0.24    0.29    0.35    0.58  1540
eta[7,2]      0.10    0.00 0.01    0.08    0.09    0.10    0.11    0.12  3050
eta[8,1]      0.06    0.00 0.04    0.00    0.04    0.06    0.09    0.14  1499
eta[8,2]      0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.19  2246
eta[9,1]      0.32    0.00 0.09    0.08    0.27    0.32    0.38    0.47  1602
eta[9,2]      0.16    0.00 0.01    0.14    0.15    0.16    0.17    0.19  2070
eta[10,1]     0.44    0.00 0.06    0.34    0.40    0.44    0.48    0.58  1731
eta[10,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1962
eta[11,1]     0.24    0.00 0.05    0.14    0.21    0.24    0.27    0.34  2517
eta[11,2]     0.14    0.00 0.01    0.12    0.13    0.14    0.14    0.16  5737
eta[12,1]     0.45    0.00 0.07    0.32    0.40    0.45    0.49    0.59  1647
eta[12,2]     0.13    0.00 0.01    0.11    0.12    0.13    0.14    0.15  4732
eta[13,1]     0.25    0.00 0.07    0.12    0.20    0.24    0.28    0.42  1667
eta[13,2]     0.12    0.00 0.01    0.10    0.11    0.12    0.12    0.14  5855
eta[14,1]     0.33    0.00 0.10    0.16    0.25    0.32    0.38    0.59  1271
eta[14,2]     0.10    0.00 0.01    0.08    0.09    0.10    0.11    0.12  4651
eta[15,1]     0.36    0.00 0.10    0.19    0.29    0.35    0.41    0.59  1227
eta[15,2]     0.11    0.00 0.01    0.08    0.10    0.11    0.11    0.13  4872
eta[16,1]     0.07    0.00 0.05    0.01    0.04    0.07    0.10    0.18  2198
eta[16,2]     0.07    0.00 0.01    0.04    0.06    0.07    0.07    0.09  3127
eta[17,1]     0.30    0.00 0.12    0.01    0.23    0.32    0.40    0.50  1578
eta[17,2]     0.12    0.00 0.01    0.10    0.12    0.12    0.13    0.15  3790
eta[18,1]     0.45    0.00 0.06    0.36    0.41    0.45    0.49    0.58  3073
eta[18,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1770
eta[19,1]     0.23    0.00 0.04    0.14    0.20    0.23    0.25    0.31  4139
eta[19,2]     0.15    0.00 0.01    0.13    0.14    0.15    0.16    0.17  3949
eta[20,1]     0.15    0.00 0.07    0.01    0.10    0.15    0.20    0.28  1156
eta[20,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1671
eta[21,1]     0.36    0.00 0.06    0.27    0.32    0.35    0.38    0.52  1652
eta[21,2]     0.15    0.00 0.01    0.12    0.14    0.15    0.16    0.17  1473
eta[22,1]     0.47    0.00 0.07    0.36    0.43    0.47    0.51    0.61  2755
eta[22,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.19  1415
eta[23,1]     0.21    0.00 0.04    0.12    0.19    0.22    0.24    0.29  3014
eta[23,2]     0.16    0.00 0.01    0.13    0.15    0.16    0.16    0.18  1531
eta[24,1]     0.20    0.00 0.06    0.10    0.16    0.20    0.23    0.35  2408
eta[24,2]     0.12    0.00 0.01    0.10    0.11    0.12    0.13    0.14  2274
eta[25,1]     0.21    0.00 0.09    0.09    0.16    0.20    0.24    0.45  1715
eta[25,2]     0.10    0.00 0.01    0.08    0.09    0.10    0.11    0.12  3207
eta[26,1]     0.25    0.00 0.09    0.12    0.19    0.23    0.28    0.53  1562
eta[26,2]     0.10    0.00 0.01    0.07    0.09    0.10    0.11    0.12  3227
eta[27,1]     0.29    0.00 0.09    0.17    0.24    0.28    0.33    0.56  1518
eta[27,2]     0.11    0.00 0.01    0.08    0.10    0.11    0.11    0.13  2703
eta[28,1]     0.45    0.00 0.06    0.35    0.41    0.45    0.49    0.59  2086
eta[28,2]     0.14    0.00 0.01    0.12    0.13    0.14    0.15    0.17  1479
eta[29,1]     0.47    0.00 0.07    0.36    0.43    0.47    0.51    0.62  2348
eta[29,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1485
eta[30,1]     0.46    0.00 0.07    0.35    0.41    0.45    0.50    0.61  1571
eta[30,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1934
eta[31,1]     0.37    0.00 0.06    0.27    0.33    0.36    0.40    0.52  1462
eta[31,2]     0.15    0.00 0.01    0.13    0.15    0.15    0.16    0.17  3523
eta[32,1]     0.46    0.00 0.07    0.34    0.41    0.46    0.51    0.61  1498
eta[32,2]     0.15    0.00 0.01    0.13    0.14    0.15    0.16    0.17  3381
eta[33,1]     0.17    0.00 0.05    0.08    0.14    0.17    0.21    0.27  2613
eta[33,2]     0.12    0.00 0.01    0.10    0.11    0.12    0.12    0.14  5925
eta[34,1]     0.03    0.00 0.03    0.00    0.01    0.02    0.04    0.10  1204
eta[34,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1701
eta[35,1]     0.11    0.00 0.06    0.03    0.07    0.10    0.13    0.27  2230
eta[35,2]     0.08    0.00 0.01    0.06    0.07    0.08    0.09    0.10  4726
eta[36,1]     0.29    0.00 0.10    0.02    0.24    0.31    0.36    0.45  1785
eta[36,2]     0.15    0.00 0.01    0.13    0.14    0.15    0.16    0.18  1318
eta[37,1]     0.45    0.00 0.07    0.34    0.41    0.44    0.49    0.61  1434
eta[37,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.17    0.19  2151
eta[38,1]     0.43    0.00 0.07    0.32    0.38    0.42    0.47    0.61  1253
eta[38,2]     0.16    0.00 0.01    0.13    0.15    0.16    0.16    0.18  3055
eta[39,1]     0.28    0.00 0.07    0.11    0.24    0.28    0.32    0.39  1594
eta[39,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1664
eta[40,1]     0.48    0.00 0.07    0.36    0.43    0.47    0.52    0.63  1680
eta[40,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1485
eta[41,1]     0.42    0.00 0.05    0.34    0.39    0.42    0.45    0.54  2202
eta[41,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1513
eta[42,1]     0.47    0.00 0.07    0.36    0.43    0.47    0.51    0.63  1648
eta[42,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1510
eta[43,1]     0.48    0.00 0.07    0.37    0.43    0.47    0.51    0.63  1684
eta[43,2]     0.17    0.00 0.01    0.14    0.16    0.17    0.18    0.20  1591
eta[44,1]     0.09    0.00 0.04    0.02    0.07    0.09    0.12    0.17  1681
eta[44,2]     0.14    0.00 0.01    0.12    0.13    0.14    0.15    0.16  1747
eta[45,1]     0.46    0.00 0.07    0.34    0.42    0.46    0.50    0.60  2903
eta[45,2]     0.14    0.00 0.01    0.12    0.13    0.14    0.15    0.17  1448
           Rhat
p[1]       1.01
p[2]       1.00
alpha[1]   1.00
alpha[2]   1.00
beta1      1.00
beta2      1.00
sigma[1]   1.00
sigma[2]   1.00
xi1_init   1.00
r_tm1_init 1.00
eta[1,1]   1.00
eta[1,2]   1.00
eta[2,1]   1.00
eta[2,2]   1.00
eta[3,1]   1.00
eta[3,2]   1.00
eta[4,1]   1.00
eta[4,2]   1.00
eta[5,1]   1.00
eta[5,2]   1.00
eta[6,1]   1.00
eta[6,2]   1.00
eta[7,1]   1.00
eta[7,2]   1.00
eta[8,1]   1.00
eta[8,2]   1.00
eta[9,1]   1.00
eta[9,2]   1.00
eta[10,1]  1.00
eta[10,2]  1.00
eta[11,1]  1.00
eta[11,2]  1.00
eta[12,1]  1.00
eta[12,2]  1.00
eta[13,1]  1.00
eta[13,2]  1.00
eta[14,1]  1.00
eta[14,2]  1.00
eta[15,1]  1.00
eta[15,2]  1.00
eta[16,1]  1.00
eta[16,2]  1.00
eta[17,1]  1.00
eta[17,2]  1.00
eta[18,1]  1.00
eta[18,2]  1.00
eta[19,1]  1.00
eta[19,2]  1.00
eta[20,1]  1.00
eta[20,2]  1.00
eta[21,1]  1.00
eta[21,2]  1.00
eta[22,1]  1.00
eta[22,2]  1.00
eta[23,1]  1.00
eta[23,2]  1.00
eta[24,1]  1.00
eta[24,2]  1.00
eta[25,1]  1.00
eta[25,2]  1.00
eta[26,1]  1.00
eta[26,2]  1.00
eta[27,1]  1.00
eta[27,2]  1.00
eta[28,1]  1.00
eta[28,2]  1.00
eta[29,1]  1.00
eta[29,2]  1.00
eta[30,1]  1.00
eta[30,2]  1.00
eta[31,1]  1.00
eta[31,2]  1.00
eta[32,1]  1.00
eta[32,2]  1.00
eta[33,1]  1.00
eta[33,2]  1.00
eta[34,1]  1.00
eta[34,2]  1.00
eta[35,1]  1.00
eta[35,2]  1.00
eta[36,1]  1.00
eta[36,2]  1.00
eta[37,1]  1.00
eta[37,2]  1.00
eta[38,1]  1.00
eta[38,2]  1.00
eta[39,1]  1.00
eta[39,2]  1.00
eta[40,1]  1.00
eta[40,2]  1.00
eta[41,1]  1.00
eta[41,2]  1.00
eta[42,1]  1.00
eta[42,2]  1.00
eta[43,1]  1.00
eta[43,2]  1.00
eta[44,1]  1.00
eta[44,2]  1.00
eta[45,1]  1.00
eta[45,2]  1.00
 [ reached getOption("max.print") -- omitted 1415 rows ]

Samples were drawn using NUTS(diag_e) at Tue Dec  3 16:52:29 2019.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
#+end_example

***

**** フィルタ化確率

- 200-300, 600-700 がレジーム 2
#+begin_src R :results output graphics file :file (my/get-babel-file)
as.data.frame(msar1_2_fit, pars = "pi") %>%
  pivot_longer(everything()) %>%
  ## 状態 1 のみ抽出
  dplyr::filter(str_detect(name, ",1")) %>%
  group_by(name) %>%
  summarise(mean = mean(value),
            lower = quantile(value, .025),
            upper = quantile(value, .975)) %>%
  mutate(no = as.integer(str_extract(name, "(?<=^pi\\[).*(?=,1\\]$)"))) %>%
  arrange(no) %>%
  ggplot(aes(x = no, y = mean)) +
  geom_point(alpha = 0.3) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3,  fill = "green")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-yTHuR3.png]]

**** 平滑化確率

#+begin_src R :results output graphics file :file (my/get-babel-file)
as.data.frame(msar1_2_fit, pars = "gamma") %>%
  pivot_longer(everything()) %>%
  ## 状態 1 のみ抽出
  dplyr::filter(str_detect(name, ",1")) %>%
  group_by(name) %>%
  summarise(mean = mean(value),
            lower = quantile(value, .025),
            upper = quantile(value, .975)) %>%
  mutate(no = as.integer(str_extract(name, "(?<=^pi\\[).*(?=,1\\]$)"))) %>%
  arrange(no) %>%
  ggplot(aes(x = no, y = mean)) +
  geom_point(alpha = 0.3) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3,  fill = "green")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-yTHuR3.png]]

**** MAP 推定量

#+begin_src R :results output graphics file :file (my/get-babel-file)
as.data.frame(msar1_2_fit, pars = "zstar") %>%
  pivot_longer(everything()) %>%
  group_by(name) %>%
  summarise(mean = round(mean(value))) %>%
  mutate(no = readr::parse_number(name)) %>%
  arrange(no) %>%
  ggplot(aes(x = no, y = mean)) +
  geom_line()
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-vfJoxW.png]]

** 時変係数 AR モデル
*** データ
**** 薬の効果なしのデータ

- [[https://logics-of-blue.com/%e6%85%a3%e3%82%8c%e3%81%ae%e7%b5%b1%e8%a8%88%e3%83%a2%e3%83%87%e3%83%aa%e3%83%b3%e3%82%b0%ef%bc%9astan%e3%81%a7%e6%8e%a8%e5%ae%9a%e3%81%99%e3%82%8b%e6%99%82%e5%a4%89%e4%bf%82/][”慣れ”の統計モデリング：Stanで推定する時変係数モデル@Logics of Blue]] の例

- シミュレーションデータの作成
- 収縮期血圧をシミュレーション

#+begin_src R :results output graphics file :file (my/get-babel-file)
N_NotMedicine <- 10              # 薬を投与しなかった日数
N_Medicine <- 100                # 薬を投与した日数
N <- N_NotMedicine + N_Medicine  # 合計日数
muZero <- 160                    # 初期の血圧の「状態」

## 血圧の「状態」のシミュレーション
mu <- numeric()

set.seed(12)

mu[1] <- rnorm(1, mean=muZero, sd=3)

for(i in 2:N) {
  mu[i] <- rnorm(1, mean=mu[i-1], sd=3)
}

plot(mu, type="b")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-vJb5GE.png]]

**** 薬の効果トレンドのデータ

- 薬を使っても、徐々に血圧は下がらなくなっていく
#+begin_src R :results output graphics file :file (my/get-babel-file)
coefMedicineTrendZero <- 0.005

## 時間的に変化する薬の効果
coefMedicine <- numeric(N_Medicine)
coefMedicineTrend <- numeric(N_Medicine)

## 薬の効果をトレンドモデルで表す
set.seed(1)
coefMedicineTrend[1] <- rnorm(1, mean=coefMedicineTrendZero, sd=0.03)

## トレンドのシミュレーション
for(i in 2:N_Medicine) {
  coefMedicineTrend[i] <- rnorm(1, mean=coefMedicineTrend[i-1], sd=0.03)
}

plot(coefMedicineTrend, type = "b")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-jIiQvU.png]]

**** 薬の効果のシミュレーションデータ

#+begin_src R :results output graphics file :file (my/get-babel-file)
coefMedicineZero <- -25
coefMedicine[1] <- rnorm(1, mean=coefMedicineTrend[1] + coefMedicineZero, sd=0.5)

for(i in 2:N_Medicine) {
  coefMedicine[i] <- rnorm(1, mean=coefMedicineTrend[i] + coefMedicine[i-1], sd=0.5)
}

plot(coefMedicine, type="b")
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-DmhZZU.png]]

- 更にノイズを加える
#+begin_src R :results output graphics file :file (my/get-babel-file)
coefMedicineReal <- numeric(100)
set.seed(1)
for(i in 1:100) {
  coefMedicineReal[i] <- rnorm(1, mean=coefMedicine[i], sd=2)
}

plot(coefMedicineReal, type="b", ylab="薬の効果（マイナスだと血圧が下がる）")
lines(coefMedicine, col=2)
legend("topleft", legend=c("薬の効果", "薬の効果のトレンド"), col=c(1,2), lwd=1)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-WVMWOw.png]]

**** 血圧の観測データ

- 血圧の観測値のシミュレーション
  - 最初の 10 日は薬なし
  - 70 日後に薬を倍にした
  - 100 日後に薬を 3 倍にした
#+begin_src R :results output graphics file :file (my/get-babel-file)
medicine <- c(rep(0, N_NotMedicine), rep(1, 60), rep(2, 30), rep(3, 10))
medicine

bloodPressure <- numeric(N)
bloodPressureMean <- numeric(N)

set.seed(1)

                                        # 最初の10日は薬なし
for(i in 1:N_NotMedicine) {
  bloodPressureMean[i] <- mu[i]
  bloodPressure[i] <- rnorm(1, mean=bloodPressureMean[i], sd=10)
}

                                        # 薬を投与した後の血圧のシミュレーション
for(i in (N_NotMedicine+1):N) {
  bloodPressureMean[i] <- mu[i] + coefMedicineReal[i-10] * medicine[i]
  bloodPressure[i] <- rnorm(1, mean=bloodPressureMean[i], sd=10)
}

plot(bloodPressure, type="b")
abline(v=10)
abline(v=70)
abline(v=100)
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-4qbHQi.png]]

**** 状態と合わせたプロット

#+begin_src R :results output graphics file :file (my/get-babel-file)
plot(bloodPressure, type="b")
lines(mu, col=2, lwd=2)
lines(bloodPressureMean, col=3, lty=2, lwd=2)
legend("bottomright",
       legend=c("観測値", "薬がなかった時の「状態」", "薬が入った時の「状態」"),
       lwd=c(1,2,2), col=c(1,2,3), lty=c(1,1,2))
#+end_src

#+RESULTS:
[[file:/home/shun/Dropbox/memo/img/babel/fig-WuKXKW.png]]

**** モデル

#+begin_src stan :file models/variying_ar_1.stan
data {
  int N_Medicine;          // 薬を投与した日数
  int N;                   // 合計日数
  vector[N] bloodPressure; // 収縮期血圧値
  vector[N] medicine;      // 投与した薬の量
}

parameters {
  real muZero;                          // 血圧の「状態」の初期値
  vector[N] mu;                         // 血圧の「状態」
  real<lower=0> sigmaV;                 // 観測誤差の大きさ
  real<lower=0> sigmaW;                 // 過程誤差の大きさ
  real coefMedicineTrendZero;           // 薬の効果のトレンドの初期値
  real coefMedicineZero;                // 薬の効果の初期値
  vector[N_Medicine] coefMedicineTrend; // 薬の効果のトレンド
  vector[N_Medicine] coefMedicine;      // 薬の効果
  vector[N_Medicine] coefMedicineReal;  // ノイズの入った後の薬の効果
  real<lower=0> sigmaWcoef;             // 薬の係数の過程誤差の大きさ
  real<lower=0> sigmaWcoefTrend;        // 薬の係数のトレンドの過程誤差の大きさ
  real<lower=0> sigmaVcoef;             // 薬の係数の観測誤差の大きさ
}

model {
  /* 状態方程式の部分 */
  // 血圧の状態の推定
  // 左端から初年度の状態を推定する
  mu[1] ~ normal(muZero, sigmaW/100);  

  // 血圧の状態の遷移
  for(i in 2:N) {
    mu[i] ~ normal(mu[i-1], sigmaW/100);
  }

  // 薬の効果の変化をモデル化する部分
  // 初年度の薬の効果のトレンドを推定する
  coefMedicineTrend[1] ~ normal(coefMedicineTrendZero, sigmaWcoefTrend/100); 

  // 薬の効果のトレンドの遷移
  for(i in 2:N_Medicine) {
    coefMedicineTrend[i] ~ normal(coefMedicineTrend[i-1], sigmaWcoefTrend/100);
  }

  // 初年度の薬の効果を推定する
  coefMedicine[1] ~ normal(coefMedicineTrend[1] + coefMedicineZero, sigmaWcoef/100);  

  // 薬の効果の遷移
  for(i in 2:N_Medicine) {
    coefMedicine[i] ~ normal(coefMedicineTrend[i] + coefMedicine[i-1], sigmaWcoef/100);
  }

  // 薬の効果にノイズが入る
  for(i in 1:N_Medicine) {
    coefMedicineReal[i] ~ normal(coefMedicine[i], sigmaVcoef/100);
  }

  /* 観測方程式の部分(薬なし) */
  for(i in 1:10) {
    bloodPressure[i] ~ normal(mu[i], sigmaV/100);
  }

  /* 観測方程式の部分(薬あり) */
  for(i in 11:N) {
    bloodPressure[i] ~ normal(mu[i] + coefMedicineReal[i-10]*medicine[i], sigmaV/100);
  }
}
#+end_src

#+RESULTS:
[[file:models/variying_ar_1.stan]]

**** 当てはめ

#+begin_src R
stanData <- list(
  N_Medicine = N_Medicine, 
  N = N,
  bloodPressure = bloodPressure,
  medicine = medicine)

# 乱数の種
set.seed(1)

# ローカルレベルモデル
time_variant_coef_Model_1 <- stan(
  file="models/varying_ar_1.stan",
  data=stanData,
  iter=45000,
  warmup=35000,
  thin=10,
  chains=3,
  control=list(
    adapt_delta=0.9,
    max_treedepth=13
  )
)
#+end_src

#+RESULTS:
: Error in file(fname, "rt") : cannot open the connection
: In addition: Warning messages:
: 1: In normalizePath(file) :
:   path[1]="models/varying_ar_1.stan": No such file or directory
: 2: In file(fname, "rt") :
:   cannot open file 'models/varying_ar_1.stan': No such file or directory
: Error in get_model_strcode(file, model_code) : 
:   cannot open model file "models/varying_ar_1.stan"

* 参考

- Stan 公式ドキュメント一覧
  - [[https://mc-stan.org/docs/2_21/stan-users-guide/time-series-chapter.html][Stan User’s Guide: 2 Time-Series Models]] ([[https://stan-ja.github.io/gh-pages-html/#%E6%99%82%E7%B3%BB%E5%88%97%E3%83%A2%E3%83%87%E3%83%AB][日本語訳]])
  - Stan Forum Discussion
    - [[https://discourse.mc-stan.org/t/regime-switching-model/4098][Regime Switching Model]]
    - [[https://discourse.mc-stan.org/t/hidden-markov-model-with-constraints/1625][Hidden Markov Model with constraints]]
    - [[https://discourse.mc-stan.org/t/transversing-up-a-graph-hierarchical-hidden-markov-model/1304][Transversing up a graph (Hierarchical Hidden Markov Model)]]

- Blog
  - [[http://statmodeling.hatenablog.com/entry/calc-waic-wbic][WAICとWBICを事後分布から計算する@StatModeling Memorandum]]
  - [[http://ill-identified.hatenablog.com/entry/2016/02/14/205311][R + Stan で ベクトル ARMA (VARMA) を推定@ill-identified diary]]
  - [[http://ill-identified.hatenablog.com/entry/2016/04/02/225155][stan + R ベクトル ARIMA (VARIMA) で人口予測 (?)@ill-identified diary]]
  - [[https://logics-of-blue.com/%e6%85%a3%e3%82%8c%e3%81%ae%e7%b5%b1%e8%a8%88%e3%83%a2%e3%83%87%e3%83%aa%e3%83%b3%e3%82%b0%ef%bc%9astan%e3%81%a7%e6%8e%a8%e5%ae%9a%e3%81%99%e3%82%8b%e6%99%82%e5%a4%89%e4%bf%82/][”慣れ”の統計モデリング：Stanで推定する時変係数モデル@Logics of Blue]]
  - [[https://khakieconomics.github.io/2018/02/24/Regime-switching-models.html][Regime-switching models in Stan]]
  - [[https://statmodeling.stat.columbia.edu/2017/02/07/hmms-stan-absolutely/][HMMs in Stan? Absolutely!]]

- ={rugarch}=
  - [[https://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf][Introduction to the rugarch package (PDF)]] Stan モデルの数式の参考に

- Research paper
  - [[http://www.fs.hub.hit-u.ac.jp/inc/files/performance/masters-thesis/2015/saito2015.pdf][Carhart 4 ファクターモデルの 条件付きモデルへの拡張と 投資戦略への応用 (PDF)]]
  - [[https://zenodo.org/record/1284341/files/main_pdf.pdf?download=1][A Tutorial on Hidden Markov Models using Stan - Zenodo]]

- Github
  - [[https://github.com/luisdamiano/gsoc17-hhmm][Bayesian Hierarchical Hidden Markov Models applied to financial time series, a research replication project for Google Summer of Code 2017.]]
  - [[https://github.com/jeffalstott/pystan_time_series][pystan_time_series]]
  - [[https://github.com/stan-dev/math/issues/309][Functions to ensure stationarity in autoregression and invertibility in moving average parameters #309@stan-dev]]
